{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5155e6-54fa-48ce-92c8-535e9fbd02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# üîé Auto Label Comparison for Review Candidates (V32)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Macro Description\n",
    "# -----------------\n",
    "# This script automates label checking for images in:\n",
    "#     .../V32_YYYYMMDD_xxxxxx/review_candidates_by_predicted_class/<predicted_label>/*.jpg\n",
    "\n",
    "# It compares the *predicted label* implied by each review folder name against the\n",
    "# *true label* implied by your original dataset folder structure:\n",
    "#     .../img_datasets/ferckjalfaga_dataset_14_labels/<true_label>/*.jpg\n",
    "\n",
    "# Key features:\n",
    "#   1) Walks every nested class folder under `review_candidates_by_predicted_class/`\n",
    "#   2) Indexes the original dataset once (filename -> true_label + absolute path)\n",
    "#   3) For each review image, finds its true label (by filename) and flags mismatches\n",
    "#   4) Writes a detailed audit CSV:\n",
    "#         auto_label_comparison_V32.csv\n",
    "#         Columns: filename, predicted_label, true_label, source_path, review_path, status\n",
    "#   5) (Optional) Writes a curation patch CSV you can feed to V33 training:\n",
    "#         curated_additions_V32.csv\n",
    "#         Columns: filepath, correct_label, notes\n",
    "#      - Uses ORIGINAL dataset paths when available (preferred for pipeline ingestion)\n",
    "#      - Falls back to review folder paths if original not found\n",
    "\n",
    "# Assumptions / Notes:\n",
    "# - Filenames are unique across the dataset. If not, the script marks them as AMBIGUOUS.\n",
    "# - If you want absolute reproducibility, keep the original dataset stable while curating.\n",
    "# - This is metadata-driven (no visual validation). Use it to quickly capture obvious\n",
    "#   false positives / negatives; do manual spot checks where necessary.\n",
    "\n",
    "# Recommended Flow:\n",
    "#   1) Run the script\n",
    "#   2) Open `auto_label_comparison_V32.csv` ‚Üí filter rows where status == \"MISMATCH\"\n",
    "#   3) Review quickly; use the generated `curated_additions_V32.csv` as your patch\n",
    "#   4) Integrate the patch into V33 dataset-building & oversampling\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca4e5791-714a-4c89-9c80-bc5cdf16d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d5e247-0964-4ebe-8c20-f940c795563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# üîß CONFIGURATION ‚Äî EDIT THESE PATHS IF NEEDED\n",
    "# ---------------------------------------------------------\n",
    "# V32 run directory\n",
    "V32_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\"\n",
    "\n",
    "# Review candidates root (predicted-label folders live here)\n",
    "REVIEW_DIR = os.path.join(V32_DIR, \"review_candidates_by_predicted_class\")\n",
    "\n",
    "# Original dataset root (true-label folders live here)\n",
    "DATASET_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "\n",
    "# Output CSVs\n",
    "COMPARISON_CSV = os.path.join(V32_DIR, \"auto_label_comparison_V32.csv\")\n",
    "GENERATE_PATCH = True  # set False if you don‚Äôt want curated_additions_V32.csv created\n",
    "PATCH_CSV = os.path.join(V32_DIR, \"curated_additions_V32.csv\")\n",
    "\n",
    "# File extensions to consider as images\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".webp\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abfef09-76e3-4bb8-81f9-513e109a7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# üß∞ HELPERS\n",
    "# ---------------------------------------------------------\n",
    "def is_image(fname: str) -> bool:\n",
    "    return os.path.splitext(fname)[1].lower() in IMG_EXTS\n",
    "\n",
    "\n",
    "def index_original_dataset(dataset_root: str):\n",
    "    \"\"\"\n",
    "    Walk the original dataset once.\n",
    "    Returns:\n",
    "        true_label_by_filename : dict[str, str or 'AMBIGUOUS']\n",
    "        source_path_by_filename: dict[str, str or 'AMBIGUOUS']\n",
    "        duplicates             : set[str] filenames seen in >1 label folder\n",
    "    \"\"\"\n",
    "    true_label_by_filename = {}\n",
    "    source_path_by_filename = {}\n",
    "    occurrences = defaultdict(list)\n",
    "\n",
    "    for true_label in os.listdir(dataset_root):\n",
    "        class_dir = os.path.join(dataset_root, true_label)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        for fname in os.listdir(class_dir):\n",
    "            if not is_image(fname):\n",
    "                continue\n",
    "            abs_path = os.path.join(class_dir, fname)\n",
    "            occurrences[fname].append((true_label, abs_path))\n",
    "\n",
    "    duplicates = set()\n",
    "    for fname, items in occurrences.items():\n",
    "        if len(items) == 1:\n",
    "            lbl, pth = items[0]\n",
    "            true_label_by_filename[fname] = lbl\n",
    "            source_path_by_filename[fname] = pth\n",
    "        else:\n",
    "            # filename appears in multiple true-label folders ‚Üí ambiguous\n",
    "            duplicates.add(fname)\n",
    "            true_label_by_filename[fname] = \"AMBIGUOUS\"\n",
    "            source_path_by_filename[fname] = \"AMBIGUOUS\"\n",
    "\n",
    "    return true_label_by_filename, source_path_by_filename, duplicates\n",
    "\n",
    "\n",
    "def walk_review_candidates(review_root: str):\n",
    "    \"\"\"\n",
    "    Yield tuples of (predicted_label, review_abs_path, filename)\n",
    "    for every image found under review_candidates_by_predicted_class/<predicted_label>/...\n",
    "    \"\"\"\n",
    "    for predicted_label in os.listdir(review_root):\n",
    "        pred_dir = os.path.join(review_root, predicted_label)\n",
    "        if not os.path.isdir(pred_dir):\n",
    "            continue\n",
    "        for fname in os.listdir(pred_dir):\n",
    "            if not is_image(fname):\n",
    "                continue\n",
    "            yield predicted_label, os.path.join(pred_dir, fname), fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c635a929-ec17-41a9-8017-9ca7bdbfbdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Indexing original dataset (this is fast for moderate datasets)...\n",
      "   ‚Üí Indexed 6174 unique filenames (ambiguous filenames: 0)\n",
      "üîç Scanning review candidates and comparing labels...\n",
      "‚úÖ Wrote comparison: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/auto_label_comparison_V32.csv\n",
      "   ‚Üí Checked 11317 files; mismatches: 2016; ambiguous filenames: 0\n",
      "üìù Wrote curated patch: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/curated_additions_V32.csv (rows: 2016; only MISMATCH entries with resolvable true labels)\n",
      "\n",
      "Next steps:\n",
      "  1) Open the comparison CSV and filter `status == MISMATCH` to inspect disagreements.\n",
      "  2) If PATCH is enabled, review `curated_additions_V32.csv` quickly; append notes or adjust any edge cases.\n",
      "  3) Feed the patch CSV into your V33 dataset builder and oversample it.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# üöÄ MAIN\n",
    "# ---------------------------------------------------------\n",
    "def main():\n",
    "    if not os.path.isdir(REVIEW_DIR):\n",
    "        raise FileNotFoundError(f\"Review directory not found: {REVIEW_DIR}\")\n",
    "    if not os.path.isdir(DATASET_ROOT):\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {DATASET_ROOT}\")\n",
    "\n",
    "    print(\"üìã Indexing original dataset (this is fast for moderate datasets)...\")\n",
    "    true_label_by_filename, source_path_by_filename, duplicates = index_original_dataset(DATASET_ROOT)\n",
    "    print(f\"   ‚Üí Indexed {len(true_label_by_filename)} unique filenames \"\n",
    "          f\"(ambiguous filenames: {len(duplicates)})\")\n",
    "\n",
    "    rows = []\n",
    "    mismatches = 0\n",
    "    total = 0\n",
    "\n",
    "    print(\"üîç Scanning review candidates and comparing labels...\")\n",
    "    for predicted_label, review_abs_path, fname in walk_review_candidates(REVIEW_DIR):\n",
    "        total += 1\n",
    "        true_label = true_label_by_filename.get(fname, \"UNKNOWN_SOURCE\")\n",
    "        source_path = source_path_by_filename.get(fname, \"\")\n",
    "\n",
    "        if true_label == \"AMBIGUOUS\":\n",
    "            status = \"AMBIGUOUS_FILENAME\"\n",
    "        elif true_label == \"UNKNOWN_SOURCE\":\n",
    "            status = \"MISSING_IN_ORIGINAL\"\n",
    "        elif true_label == predicted_label:\n",
    "            status = \"MATCH\"\n",
    "        else:\n",
    "            status = \"MISMATCH\"\n",
    "            mismatches += 1\n",
    "\n",
    "        rows.append([\n",
    "            fname,                 # filename only\n",
    "            predicted_label,       # from review folder name\n",
    "            true_label,            # from original dataset index\n",
    "            source_path,           # absolute path in original dataset (or 'AMBIGUOUS')\n",
    "            review_abs_path,       # absolute path in review tree\n",
    "            status                 # MATCH / MISMATCH / AMBIGUOUS_FILENAME / MISSING_IN_ORIGINAL\n",
    "        ])\n",
    "\n",
    "    # Write comparison CSV (rich audit)\n",
    "    os.makedirs(os.path.dirname(COMPARISON_CSV), exist_ok=True)\n",
    "    with open(COMPARISON_CSV, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"filename\", \"predicted_label\", \"true_label\",\n",
    "                    \"source_path\", \"review_path\", \"status\"])\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"‚úÖ Wrote comparison: {COMPARISON_CSV}\")\n",
    "    print(f\"   ‚Üí Checked {total} files; mismatches: {mismatches}; \"\n",
    "          f\"ambiguous filenames: {len(duplicates)}\")\n",
    "\n",
    "    # Optionally create a ready-to-use curation patch for V33\n",
    "    if GENERATE_PATCH:\n",
    "        # Only include rows where we have a resolvable true label (not ambiguous/missing)\n",
    "        patch_rows = []\n",
    "        for fname, predicted_label, true_label, source_path, review_path, status in rows:\n",
    "            if status == \"MISMATCH\" and true_label not in {\"AMBIGUOUS\", \"UNKNOWN_SOURCE\"}:\n",
    "                # Prefer the ORIGINAL dataset path for ingestion; it‚Äôs stable for training\n",
    "                filepath_for_patch = source_path if source_path else review_path\n",
    "                patch_rows.append([filepath_for_patch, true_label, f\"auto from review vs. dataset ({predicted_label}‚Üí{true_label})\"])\n",
    "\n",
    "        # If nothing mismatched, still write an empty, well-formed file\n",
    "        with open(PATCH_CSV, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"filepath\", \"correct_label\", \"notes\"])\n",
    "            w.writerows(patch_rows)\n",
    "\n",
    "        print(f\"üìù Wrote curated patch: {PATCH_CSV} \"\n",
    "              f\"(rows: {len(patch_rows)}; only MISMATCH entries with resolvable true labels)\")\n",
    "\n",
    "    # Guidance summary\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1) Open the comparison CSV and filter `status == MISMATCH` to inspect disagreements.\")\n",
    "    print(\"  2) If PATCH is enabled, review `curated_additions_V32.csv` quickly; \"\n",
    "          \"append notes or adjust any edge cases.\")\n",
    "    print(\"  3) Feed the patch CSV into your V33 dataset builder and oversample it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdb6be-e416-47e7-a795-062f5fc18e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
