{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1aca954-7fa3-4157-8b00-43b0403caa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script performs a one-time analysis of ferckjalfaga dataset. \n",
    "    # It loads the final V29 model, removes its final classification layer to access\n",
    "    # the rich feature embeddings (the \"faceprints\"), and then calculates the average \n",
    "    # faceprint for each of the core emotion classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f43b28-0334-4158-8c32-d4d151568d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f060d1-7402-4610-8090-9adf39073bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- IMPORTANT: Update this path to point to your FINAL, CURATED training dataset ---\n",
    "# This should be the root folder containing the subfolders for each emotion class.\n",
    "TRAINING_DATA_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\" \n",
    "\n",
    "# Path to the final, production-ready model\n",
    "MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "\n",
    "# Output file path\n",
    "CENTROIDS_SAVE_PATH = \"emotion_centroids.pt\"\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cf87e76-82c4-4329-9ca3-09f746dcae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading model from /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 ---\n",
      "\n",
      "üñ•Ô∏è Using device: mps\n",
      "‚úÖ Model modified to output feature embeddings.\n",
      "\n",
      "--- Loading training data from /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c0fb37ca484fc19a028c5738f15ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded and prepared with custom collator.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Load Model and Processor ---\n",
    "print(f\"--- Loading model from {MODEL_PATH} ---\")\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_PATH)\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# --- Set up device ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# --- Modify the model to output feature embeddings ---\n",
    "embedding_size = model.config.hidden_size\n",
    "model.classifier = nn.Identity(embedding_size)\n",
    "model.to(device).eval()\n",
    "print(\"‚úÖ Model modified to output feature embeddings.\")\n",
    "\n",
    "# --- Load and process the dataset ---\n",
    "print(f\"\\n--- Loading training data from {TRAINING_DATA_DIR} ---\")\n",
    "if not os.path.exists(TRAINING_DATA_DIR):\n",
    "    raise FileNotFoundError(f\"CRITICAL: The specified training data directory does not exist. Please update the TRAINING_DATA_DIR path.\")\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=TRAINING_DATA_DIR, split=\"train\")\n",
    "\n",
    "def transform(examples):\n",
    "    # Process images on-the-fly\n",
    "    examples[\"pixel_values\"] = processor([img.convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")['pixel_values']\n",
    "    return examples\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "\n",
    "# --- THIS IS THE NEW/CORRECTED PART ---\n",
    "# Define a custom collate function to handle batching\n",
    "def custom_collate(batch):\n",
    "    \"\"\"\n",
    "    Manually creates a batch by stacking 'pixel_values' and creating a 'labels' tensor,\n",
    "    while ignoring the raw 'image' objects.\n",
    "    \"\"\"\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "    return {'pixel_values': pixel_values, 'label': labels}\n",
    "\n",
    "# Update the DataLoader to use our custom collate function\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=custom_collate)\n",
    "print(\"‚úÖ Data loaded and prepared with custom collator.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6669d6b0-36a4-42e0-bc46-0b7cb19f474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating embeddings for all training images ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 546/546 [05:46<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All embeddings generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. CALCULATE EMBEDDINGS PER CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "# Dictionary to hold all embeddings, separated by class label ID\n",
    "class_embeddings = {i: [] for i in range(model.config.num_labels)}\n",
    "\n",
    "print(\"\\n--- Generating embeddings for all training images ---\")\n",
    "for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    labels = batch['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the feature embeddings from the modified model\n",
    "        embeddings = model(pixel_values=pixel_values).logits\n",
    "    \n",
    "    # Move embeddings to CPU and store them\n",
    "    cpu_embeddings = embeddings.cpu().numpy()\n",
    "    for i in range(len(labels)):\n",
    "        label_id = labels[i].item()\n",
    "        class_embeddings[label_id].append(cpu_embeddings[i])\n",
    "\n",
    "print(\"‚úÖ All embeddings generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4500b7e8-cd6e-4701-b486-8890c8c59e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating average embedding (centroid) for each class ---\n",
      "  -> Centroid calculated for 'anger'\n",
      "  -> Centroid calculated for 'disgust'\n",
      "  -> Centroid calculated for 'fear'\n",
      "  -> Centroid calculated for 'happiness'\n",
      "  -> Centroid calculated for 'neutral'\n",
      "  -> Centroid calculated for 'questioning'\n",
      "  -> Centroid calculated for 'sadness'\n",
      "  -> Centroid calculated for 'surprise'\n",
      "  -> Centroid calculated for 'contempt'\n",
      "  -> Centroid calculated for 'unknown'\n",
      "\n",
      "‚úÖ Centroids successfully calculated and saved to: emotion_centroids.pt\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. CALCULATE AND SAVE CENTROIDS\n",
    "# ==============================================================================\n",
    "\n",
    "emotion_centroids = {}\n",
    "id2label = model.config.id2label\n",
    "\n",
    "print(\"\\n--- Calculating average embedding (centroid) for each class ---\")\n",
    "for label_id, embeddings_list in class_embeddings.items():\n",
    "    if not embeddings_list:\n",
    "        print(f\"‚ö†Ô∏è Warning: No images found for class {id2label[label_id]}. Skipping centroid calculation.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate the mean of all embeddings for this class\n",
    "    centroid = np.mean(np.array(embeddings_list), axis=0)\n",
    "    emotion_centroids[label_id] = torch.from_numpy(centroid)\n",
    "    print(f\"  -> Centroid calculated for '{id2label[label_id]}'\")\n",
    "\n",
    "# Save the final dictionary of centroids to a file\n",
    "torch.save(emotion_centroids, CENTROIDS_SAVE_PATH)\n",
    "print(f\"\\n‚úÖ Centroids successfully calculated and saved to: {CENTROIDS_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6986617-2e80-44ef-b7ea-6546d119573a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
