{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9afec7-74f2-4d33-9529-ad6a6bd97d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import hashlib\n",
    "import imagehash\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdf2c1-9c79-4306-91e2-6c72810eabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- Main Configuration ---\n",
    "# ==============================================================================\n",
    "INPUT_FOLDER = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/pexels_dataset_archive/pexels_scraped\"\n",
    "OUTPUT_FOLDER = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/pexels_dataset_archive/pexels_curated\"\n",
    "YUNET_MODEL_PATH = \"face_detection_yunet_2023mar.onnx\"\n",
    "\n",
    "# 1. MINIMUM SIZE: Lowered to 72 to capture smaller faces in group shots.\n",
    "MIN_WIDTH = 72\n",
    "MIN_HEIGHT = 72\n",
    "\n",
    "# 2. BLUR THRESHOLD (Sharpness): Measures edge clarity. Higher is sharper.\n",
    "BLUR_THRESHOLD = 100.0\n",
    "\n",
    "# 3. CONTRAST THRESHOLD: Measures the dynamic range of the image.\n",
    "CONTRAST_THRESHOLD = 15.0\n",
    "\n",
    "# --- Landmark and Pose Filtering ---\n",
    "REQUIRE_ALL_LANDMARKS = True\n",
    "MAX_EYE_ANGLE = 10.0\n",
    "\n",
    "# How similar two images can be to be considered duplicates.\n",
    "# A value of 0-1 is very strict (nearly identical). Higher values are more lenient.\n",
    "PERCEPTUAL_HASH_THRESHOLD = 1\n",
    "\n",
    "# <--- NEW: PADDING FACTOR ---\n",
    "# Increases the crop area around the face. 0.35 means 35% padding on each side.\n",
    "PADDING_FACTOR = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee443fd-e927-4a52-8386-d5301bc3e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: Detect, Filter, and Crop High-Quality Faces (with Padding)\n",
    "# ==============================================================================\n",
    "def crop_all_faces(input_folder, output_folder, model_path):\n",
    "    print(\"--- Starting Step 1: Face Detection and Cropping ---\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"FATAL ERROR: Model file not found at '{model_path}'\")\n",
    "        sys.exit()\n",
    "\n",
    "    try:\n",
    "        face_detector = cv2.FaceDetectorYN.create(\n",
    "            model=model_path, config=\"\", input_size=(320, 320),\n",
    "            score_threshold=0.7\n",
    "        )\n",
    "        print(\"  Successfully initialized OpenCV FaceDetectorYN.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not initialize the face detector. Error: {e}\")\n",
    "        sys.exit()\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    total_faces_saved = 0\n",
    "    \n",
    "    all_image_paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(input_folder)) for f in fn if f.lower().endswith(('.png', '.jpg', 'jpeg', '.bmp', '.tiff'))]\n",
    "    print(f\"  Found {len(all_image_paths)} total images to process.\")\n",
    "\n",
    "    for file_path in tqdm(all_image_paths, desc=\"Step 1: Cropping faces\"):\n",
    "        try:\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None: continue\n",
    "            \n",
    "            img_height, img_width, _ = image.shape\n",
    "            face_detector.setInputSize((img_width, img_height))\n",
    "            _, faces = face_detector.detect(image)\n",
    "            \n",
    "            if faces is not None:\n",
    "                for i, face_data in enumerate(faces):\n",
    "                    box = list(map(int, face_data[:4]))\n",
    "                    landmarks = list(map(int, face_data[4:14]))\n",
    "                    \n",
    "                    if REQUIRE_ALL_LANDMARKS and len(landmarks) < 10: continue\n",
    "                    \n",
    "                    right_eye, left_eye = (landmarks[0], landmarks[1]), (landmarks[2], landmarks[3])\n",
    "                    angle = np.degrees(np.arctan2(left_eye[1] - right_eye[1], left_eye[0] - right_eye[0]))\n",
    "                    if abs(angle) > MAX_EYE_ANGLE: continue\n",
    "\n",
    "                    x, y, w, h = box\n",
    "                    if not (x >= 0 and y >= 0 and x + w <= img_width and y + h <= img_height): continue\n",
    "                    \n",
    "                    if w < MIN_WIDTH or h < MIN_HEIGHT: continue\n",
    "                    \n",
    "                    # <--- MODIFIED SECTION: Apply padding to the bounding box ---\n",
    "                    pad_w = int(w * PADDING_FACTOR)\n",
    "                    pad_h = int(h * PADDING_FACTOR)\n",
    "                    \n",
    "                    # Calculate new coordinates with padding\n",
    "                    x1 = x - pad_w\n",
    "                    y1 = y - pad_h\n",
    "                    x2 = x + w + pad_w\n",
    "                    y2 = y + h + pad_h\n",
    "                    \n",
    "                    # Ensure coordinates are within image bounds\n",
    "                    x1 = max(0, x1)\n",
    "                    y1 = max(0, y1)\n",
    "                    x2 = min(img_width, x2)\n",
    "                    y2 = min(img_height, y2)\n",
    "                    # <--- END MODIFIED SECTION ---\n",
    "\n",
    "                    # <--- MODIFIED LINE: Crop using padded coordinates ---\n",
    "                    cropped_face = image[y1:y2, x1:x2]\n",
    "\n",
    "                    # Skip if the final crop is somehow invalid\n",
    "                    if cropped_face.size == 0: continue\n",
    "                    \n",
    "                    gray_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    sharpness_score = cv2.Laplacian(gray_face, cv2.CV_64F).var()\n",
    "                    if sharpness_score < BLUR_THRESHOLD: continue\n",
    "                        \n",
    "                    contrast_score = gray_face.std()\n",
    "                    if contrast_score < CONTRAST_THRESHOLD: continue\n",
    "\n",
    "                    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                    output_filename = f\"{base_filename}_face_{i+1}.jpg\"\n",
    "                    output_path = os.path.join(output_folder, output_filename)\n",
    "                    cv2.imwrite(output_path, cropped_face)\n",
    "                    total_faces_saved += 1\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  WARNING: An error occurred on {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"--- Step 1 Complete ---\")\n",
    "    print(f\"  Total high-quality faces found and saved: {total_faces_saved}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a9a72-cb23-4a20-9375-5ee365ecd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 2: Remove Perceptual Duplicates\n",
    "# ==============================================================================\n",
    "def remove_perceptual_duplicates(target_folder, hash_threshold):\n",
    "    print(f\"--- Starting Step 2: Removing Perceptual Duplicates (Threshold: {hash_threshold}) ---\")\n",
    "    hashes = {}\n",
    "    duplicates_to_remove = []\n",
    "    \n",
    "    image_paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(target_folder)) for f in fn if f.lower().endswith(('.png', '.jpg', 'jpeg'))]\n",
    "\n",
    "    for file_path in tqdm(image_paths, desc=\"Step 2: Hashing images\"):\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            h = imagehash.phash(img)\n",
    "            \n",
    "            found_match = False\n",
    "            for seen_hash in hashes:\n",
    "                if (h - seen_hash) <= hash_threshold:\n",
    "                    duplicates_to_remove.append(file_path)\n",
    "                    found_match = True\n",
    "                    break\n",
    "            \n",
    "            if not found_match:\n",
    "                hashes[h] = file_path\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  WARNING: Could not process {file_path} for duplicate check: {e}\")\n",
    "            \n",
    "    if duplicates_to_remove:\n",
    "        print(f\"  Found {len(duplicates_to_remove)} perceptual duplicate images. Removing...\")\n",
    "        for dup_path in duplicates_to_remove:\n",
    "            try: os.remove(dup_path)\n",
    "            except Exception as e: print(f\"  Failed to remove duplicate {dup_path}: {e}\")\n",
    "    else:\n",
    "        print(\"  No duplicate files were found.\")\n",
    "    print(f\"--- Step 2 Complete ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44041c87-3d77-4a18-a1ec-1eb8548ba8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Main Execution Block\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Tuned Image Curation Pipeline...\")\n",
    "    print(\"=========================================\")\n",
    "    crop_all_faces(INPUT_FOLDER, OUTPUT_FOLDER, YUNET_MODEL_PATH)\n",
    "    remove_perceptual_duplicates(OUTPUT_FOLDER, hash_threshold=PERCEPTUAL_HASH_THRESHOLD)\n",
    "    print(\"=========================================\")\n",
    "    print(\"Pipeline finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
