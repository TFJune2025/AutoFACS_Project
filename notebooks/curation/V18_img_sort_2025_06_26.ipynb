{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab1aa-4eca-4d03-92c7-730c210b16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 0. IMPORTS & DEPENDENCIES\n",
    "# =============================\n",
    "\n",
    "# ---- STANDARD IMPORTS ----\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# ---- FACE DETECTION (RetinaFace or fallback to MTCNN) ----\n",
    "try:\n",
    "    from retinaface import RetinaFace\n",
    "    USE_RETINAFACE = True\n",
    "except ImportError:\n",
    "    from facenet_pytorch import MTCNN\n",
    "    USE_RETINAFACE = False\n",
    "\n",
    "# ---- FACE DEDUPLICATION (ArcFace) ----\n",
    "try:\n",
    "    from insightface.app import FaceAnalysis\n",
    "    USE_ARCFACE = True\n",
    "except ImportError:\n",
    "    from imagehash import phash\n",
    "    USE_ARCFACE = False\n",
    "\n",
    "# ---- LOAD YOUR V18 MODEL ----\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00380506-ab01-4506-a19a-acf73c4438da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. CONFIGURATION & PARAMETERS\n",
    "# =============================\n",
    "INPUT_DIR = \"/path/to/your/input_images\"    # Folder of images to process\n",
    "SAVE_DIR = \"/path/to/output/sorted_faces\"   # Output root folder\n",
    "MODEL_PATH = \"/path/to/your/V18_model\"      # Your V18 supervised model checkpoint\n",
    "BATCH_SIZE = 32                             # Adjust for speed/memory\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Thresholds\n",
    "CONFIDENCE_THRESHOLDS = {    # Per-class thresholds; fallback to global if class not in dict\n",
    "    \"disgust\": 0.90,\n",
    "    \"contempt\": 0.90,\n",
    "    \"fear\": 0.90,\n",
    "    \"questioning\": 0.90,\n",
    "    \"surprise\": 0.90,\n",
    "    \"anger\": 0.90,\n",
    "    \"happiness\": 0.92,\n",
    "    \"sadness\": 0.90,\n",
    "    \"neutral\": 0.92,\n",
    "    \"unknown\": 0.98,\n",
    "}\n",
    "GLOBAL_CONF_THRESH = 0.90\n",
    "GLOBAL_ENTROPY_THRESH = 0.45\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744aa5c-7894-4715-a960-e42e1eaa879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 2. UTILITY FUNCTIONS\n",
    "# =============================\n",
    "\n",
    "# FACE DETECTION & CROPPING\n",
    "def detect_and_crop(image_path):\n",
    "    \"\"\"Detect and crop the largest face in the image.\"\"\"\n",
    "    img = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    if USE_RETINAFACE:\n",
    "        faces = RetinaFace.detect_faces(image_path)\n",
    "        if not faces:\n",
    "            return None\n",
    "        largest = max(faces.values(), key=lambda f: f['facial_area'][2]*f['facial_area'][3])\n",
    "        x1, y1, x2, y2 = largest['facial_area']\n",
    "        face = img[y1:y2, x1:x2]\n",
    "    else:\n",
    "        mtcnn = MTCNN(margin=10, keep_all=True, device=DEVICE)\n",
    "        faces, _ = mtcnn.detect(Image.fromarray(img))\n",
    "        if faces is None or len(faces) == 0:\n",
    "            return None\n",
    "        x1, y1, x2, y2 = [int(v) for v in faces[0]]\n",
    "        face = img[y1:y2, x1:x2]\n",
    "    pil_face = Image.fromarray(face)\n",
    "    # QUALITY CHECK HERE\n",
    "    if not is_quality_ok(pil_face):\n",
    "        return None\n",
    "    return pil_face\n",
    "\n",
    "\n",
    "# FACE DEDUPLICATION (ArcFace/phash)\n",
    "if USE_ARCFACE:\n",
    "    arcface_app = FaceAnalysis(name=\"antelopev2\", providers=['CPUExecutionProvider'])\n",
    "    arcface_app.prepare(ctx_id=0)\n",
    "else:\n",
    "    arcface_app = None\n",
    "\n",
    "def compute_face_embedding(face_img):\n",
    "    \"\"\"Return a 512-dim ArcFace embedding (InsightFace).\"\"\"\n",
    "    if not USE_ARCFACE or arcface_app is None:\n",
    "        raise RuntimeError(\"InsightFace/ArcFace not installed\")\n",
    "    face_np = np.array(face_img)\n",
    "    res = arcface_app.get(face_np)\n",
    "    if not res or len(res) == 0:\n",
    "        return None\n",
    "    return res[0].embedding\n",
    "\n",
    "def compute_phash(face_img):\n",
    "    return str(phash(face_img))\n",
    "\n",
    "def deduplicate(images, embeddings=None, hashes=None, threshold=0.7):\n",
    "    \"\"\"Remove duplicate or near-duplicate faces.\"\"\"\n",
    "    if embeddings is not None:\n",
    "        # Deduplicate by cosine similarity\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        keep = []\n",
    "        used = set()\n",
    "        sims = cosine_similarity(embeddings)\n",
    "        for i in range(len(images)):\n",
    "            if i in used: continue\n",
    "            keep.append(i)\n",
    "            for j in range(i+1, len(images)):\n",
    "                if sims[i, j] > threshold:\n",
    "                    used.add(j)\n",
    "        return [images[i] for i in keep]\n",
    "    elif hashes is not None:\n",
    "        seen = set()\n",
    "        keep = []\n",
    "        for i, h in enumerate(hashes):\n",
    "            if h not in seen:\n",
    "                seen.add(h)\n",
    "                keep.append(i)\n",
    "        return [images[i] for i in keep]\n",
    "    else:\n",
    "        return images\n",
    "\n",
    "\n",
    "# IMAGE QUALITY FILTERING\n",
    "def is_quality_ok(face_img, min_size=48, blur_thresh=80):\n",
    "    \"\"\"\n",
    "    Check if image is of sufficient quality: size and sharpness.\n",
    "    \"\"\"\n",
    "    if min(face_img.size) < min_size:\n",
    "        return False\n",
    "    # Quick blur check using variance of Laplacian\n",
    "    face_np = np.array(face_img.convert(\"L\"))\n",
    "    lap_var = cv2.Laplacian(face_np, cv2.CV_64F).var()\n",
    "    if lap_var < blur_thresh:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# MODEL LOADING (V18 & PROCESSOR)\n",
    "def get_v18_model():\n",
    "    model = AutoModelForImageClassification.from_pretrained(MODEL_PATH)\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "    model.to(DEVICE).eval()\n",
    "    return model, processor\n",
    "    \n",
    "\n",
    "# BATCH INFERENCE\n",
    "def batch_infer(model, processor, pil_imgs):\n",
    "    # Preprocess\n",
    "    inputs = processor(pil_imgs, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        entropies = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1)\n",
    "        confs, preds = probs.max(dim=-1)\n",
    "    return preds.cpu().numpy(), confs.cpu().numpy(), entropies.cpu().numpy(), probs.cpu().numpy()\n",
    "\n",
    "\n",
    "# SOFTMAX, ENTROPY, AND ASSIGNMENT LOGIC\n",
    "def calculate_entropy(probs):\n",
    "    \"\"\"\n",
    "    Calculate prediction entropy for a vector of softmax probabilities.\n",
    "    \"\"\"\n",
    "    return -np.sum(probs * np.log(probs + 1e-12))\n",
    "    \n",
    "\n",
    "# EXPORT & AUDIT LOGGING\n",
    "def assign_class(pred_idx, conf, entropy, id2label):\n",
    "    pred_class = id2label[pred_idx]\n",
    "    thresh = CONFIDENCE_THRESHOLDS.get(pred_class, GLOBAL_CONF_THRESH)\n",
    "    if conf < thresh or entropy > GLOBAL_ENTROPY_THRESH:\n",
    "        return \"unknown\"\n",
    "    return pred_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f744c-d6b8-4403-b62d-9ccfaeabe9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 3. MAIN PIPELINE EXECUTION\n",
    "# =============================\n",
    "\n",
    "def main():\n",
    "    # Load model/processor\n",
    "    model, processor = get_v18_model()\n",
    "    id2label = model.config.id2label\n",
    "    print(\"Loaded V18 model with labels:\", id2label)\n",
    "\n",
    "    # Discover images\n",
    "    all_img_paths = list(Path(INPUT_DIR).rglob(\"*.[jp][pn]g\"))\n",
    "    print(f\"Found {len(all_img_paths)} images in {INPUT_DIR}\")\n",
    "\n",
    "    # Step 1: Detect/crop faces\n",
    "    cropped_imgs, cropped_paths = [], []\n",
    "    for img_path in tqdm(all_img_paths, desc=\"Detect/crop faces\"):\n",
    "        try:\n",
    "            face = detect_and_crop(str(img_path))\n",
    "            if face is not None:\n",
    "                cropped_imgs.append(face)\n",
    "                cropped_paths.append(str(img_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error cropping {img_path}: {e}\")\n",
    "\n",
    "    print(f\"Detected {len(cropped_imgs)} faces.\")\n",
    "\n",
    "    # Step 2: Deduplicate\n",
    "    if USE_ARCFACE:\n",
    "        embeddings = [compute_face_embedding(img) for img in cropped_imgs]\n",
    "        dedup_indices = deduplicate(list(range(len(cropped_imgs))), embeddings=embeddings)\n",
    "    else:\n",
    "        hashes = [compute_phash(img) for img in cropped_imgs]\n",
    "        dedup_indices = deduplicate(list(range(len(cropped_imgs))), hashes=hashes)\n",
    "    cropped_imgs = [cropped_imgs[i] for i in dedup_indices]\n",
    "    cropped_paths = [cropped_paths[i] for i in dedup_indices]\n",
    "    print(f\"Deduplicated to {len(cropped_imgs)} unique faces.\")\n",
    "\n",
    "    # Step 3: Batch inference and sort\n",
    "    audit_rows = []\n",
    "    os.makedirs(os.path.join(SAVE_DIR, \"unknown\"), exist_ok=True)\n",
    "    for i in tqdm(range(0, len(cropped_imgs), BATCH_SIZE), desc=\"Batch inference\"):\n",
    "        batch_imgs = cropped_imgs[i:i+BATCH_SIZE]\n",
    "        batch_paths = cropped_paths[i:i+BATCH_SIZE]\n",
    "        preds, confs, ents, probs = batch_infer(model, processor, batch_imgs)\n",
    "        for pidx, conf, ent, path, probs_vec in zip(preds, confs, ents, batch_paths, probs):\n",
    "            assigned = assign_class(pidx, conf, ent, id2label)\n",
    "            out_dir = os.path.join(SAVE_DIR, assigned)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            img_name = os.path.basename(path)\n",
    "            shutil.copy(path, os.path.join(out_dir, img_name))\n",
    "            audit_rows.append({\n",
    "                \"image_path\": path,\n",
    "                \"assigned_class\": assigned,\n",
    "                \"predicted_label\": id2label[pidx],\n",
    "                \"confidence\": conf,\n",
    "                \"entropy\": ent,\n",
    "                **{f\"prob_{id2label[i]}\": p for i, p in enumerate(probs_vec)}\n",
    "            })\n",
    "\n",
    "    # Step 4: Save audit CSV and print stats\n",
    "    audit_df = pd.DataFrame(audit_rows)\n",
    "    audit_csv = os.path.join(SAVE_DIR, \"sort_audit.csv\")\n",
    "    audit_df.to_csv(audit_csv, index=False)\n",
    "    print(f\"Audit CSV saved to {audit_csv}\")\n",
    "\n",
    "    print(\"Final sorted class counts:\", Counter(audit_df[\"assigned_class\"]))\n",
    "    print(\"Mean/median confidence per class:\")\n",
    "    for c in sorted(audit_df[\"assigned_class\"].unique()):\n",
    "        subset = audit_df[audit_df[\"assigned_class\"] == c]\n",
    "        print(f\"  {c:12} | n={len(subset)} | mean_conf={subset['confidence'].mean():.3f} | median_conf={subset['confidence'].median():.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63812af1-76cb-44a5-bfe8-4b331410575f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
