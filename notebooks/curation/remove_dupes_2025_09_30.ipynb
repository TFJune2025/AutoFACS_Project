{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb44a5e-7d4d-4ced-9666-7cf0b9f1599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda03edf-4b01-402d-bbe5-6a833157c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- Configuration ---\n",
    "# ==============================================================================\n",
    "\n",
    "# The folder you want to scan for duplicates. This script will search all subfolders within it.\n",
    "TARGET_FOLDER = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/flick_dataset_archive/flickr_curated\"\n",
    "\n",
    "# The strictness of the duplicate check. Lower is stricter.\n",
    "# 0: Only files that are pixel-for-pixel identical will be considered duplicates.\n",
    "# 1: Catches duplicates with extremely minor, imperceptible differences.\n",
    "# 2-3: A good balance for catching visually identical images with different compression/color profiles.\n",
    "HASH_THRESHOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad62c5f-2497-481c-9541-60a5b6bfcb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- Main Duplicate Removal Function ---\n",
    "# ==============================================================================\n",
    "def remove_perceptual_duplicates(target_folder, hash_threshold):\n",
    "    \"\"\"\n",
    "    Finds and deletes visually similar images in a folder and its subfolders.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Perceptual Duplicate Removal ---\")\n",
    "    print(f\"  Target Folder: {target_folder}\")\n",
    "    print(f\"  Strictness Threshold: {hash_threshold}\")\n",
    "\n",
    "    hashes = {}\n",
    "    duplicates_to_remove = []\n",
    "\n",
    "    # Find all image files in the target folder and all subfolders\n",
    "    image_paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(target_folder)) for f in fn if f.lower().endswith(('.png', '.jpg', 'jpeg'))]\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(\"  No images found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"  Found {len(image_paths)} images to analyze.\")\n",
    "\n",
    "    # Iterate through all images to calculate and compare hashes\n",
    "    for file_path in tqdm(image_paths, desc=\"Hashing images\"):\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                h = imagehash.phash(img)\n",
    "            \n",
    "            found_match = False\n",
    "            for seen_hash in hashes:\n",
    "                if (h - seen_hash) <= hash_threshold:\n",
    "                    duplicates_to_remove.append(file_path)\n",
    "                    found_match = True\n",
    "                    break\n",
    "            \n",
    "            if not found_match:\n",
    "                hashes[h] = file_path\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  WARNING: Could not process {file_path}. Error: {e}\")\n",
    "            \n",
    "    # Delete all the identified duplicates\n",
    "    if duplicates_to_remove:\n",
    "        print(f\"\\n  Found {len(duplicates_to_remove)} duplicate images. Removing...\")\n",
    "        \n",
    "        # <--- CHANGE: Removed tqdm and added a print statement inside the loop ---\n",
    "        for dup_path in duplicates_to_remove:\n",
    "            try:\n",
    "                # Print the name of the file being removed\n",
    "                print(f\"    - Removing: {os.path.basename(dup_path)}\")\n",
    "                os.remove(dup_path)\n",
    "            except Exception as e:\n",
    "                print(f\"  Failed to remove duplicate {dup_path}: {e}\")\n",
    "    else:\n",
    "        print(\"\\n  No duplicate files were found.\")\n",
    "        \n",
    "    print(f\"--- Duplicate Removal Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1040ea9-e9e1-459f-8f2c-bfb5becfb196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- Main Execution Block ---\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.isdir(TARGET_FOLDER):\n",
    "        print(f\"ERROR: The specified TARGET_FOLDER does not exist: {TARGET_FOLDER}\")\n",
    "    else:\n",
    "        remove_perceptual_duplicates(TARGET_FOLDER, hash_threshold=HASH_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172b946-0e76-4357-93fa-5a4ad8a1119f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions)",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
