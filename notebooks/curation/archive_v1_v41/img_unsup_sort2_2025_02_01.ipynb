{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fad698-dfef-42f3-901d-2fa0b6c2bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 14:04:59.733855: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from hdbscan import HDBSCAN  # If using HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb9bf07-e21d-4679-b3d7-633e2234706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 213 images in dataset\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATASET_ROOT = \"/Volumes/JavaAOT/Documents/AI/ml_expressions/img_datasets/jaffe_dataset\"\n",
    "IMAGE_EXTENSIONS = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']\n",
    "\n",
    "def get_image_paths(root_dir, extensions):\n",
    "    \"\"\"Recursively get all image paths from nested directories\"\"\"\n",
    "    image_paths = []\n",
    "    for ext in extensions:\n",
    "        image_paths.extend(glob(os.path.join(root_dir, '**', ext), recursive=True))\n",
    "    return image_paths\n",
    "\n",
    "# 1. Initialize Paths (Run This Next)\n",
    "all_image_paths = get_image_paths(DATASET_ROOT, IMAGE_EXTENSIONS)\n",
    "print(f\"Found {len(all_image_paths)} images in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6ed4b1-d659-4d5e-8e48-01825cf63dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Emotion-Specific Feature Extraction\n",
    "# Custom feature extractor focusing on facial expression regions\n",
    "def create_emotion_feature_extractor():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Focus on mid-level convolutional layers that capture facial expressions\n",
    "    emotion_features = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=[\n",
    "            base_model.get_layer('block4_conv3').output,\n",
    "            base_model.get_layer('block5_conv3').output\n",
    "        ]\n",
    "    )\n",
    "    return emotion_features\n",
    "\n",
    "emotion_extractor = create_emotion_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ba1678-5679-42fe-a11b-ca17416bbebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Facial Region Masking\n",
    "# Add facial region masking before feature extraction\n",
    "def apply_emotion_mask(img_array):\n",
    "    \"\"\"Focus on eyes, mouth, and eyebrow regions\"\"\"\n",
    "    mask = np.zeros_like(img_array)\n",
    "    # Define facial regions (coordinates need adjustment for your data)\n",
    "    mask[75:150, 50:175] = 1  # Eyes and eyebrows\n",
    "    mask[150:200, 75:150] = 1  # Mouth\n",
    "    return img_array * mask\n",
    "\n",
    "# Modify preprocessing\n",
    "def preprocess_image(path):\n",
    "    img = tf.keras.preprocessing.image.load_img(path, target_size=(224, 224))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    return apply_emotion_mask(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd83408-4699-43ad-96b1-5686d446bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Expression-Specific Clustering\n",
    "def emotion_clustering(features, n_components=20):\n",
    "    # Combine features\n",
    "    spatial_features = features[0].mean(axis=(1, 2))\n",
    "    channel_features = features[1].mean(axis=(1, 2))\n",
    "    combined_features = np.concatenate([spatial_features, channel_features], axis=1)\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced = pca.fit_transform(combined_features)\n",
    "    \n",
    "    # Clustering\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=50,\n",
    "        cluster_selection_epsilon=0.5,\n",
    "        metric='manhattan'\n",
    "    )\n",
    "    return clusterer.fit_predict(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16012384-aa1f-41d2-ad1d-dac8f3f218d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Cluster Validation\n",
    "def validate_emotion_clusters(clusters, image_paths):\n",
    "    from collections import defaultdict\n",
    "    cluster_samples = defaultdict(list)\n",
    "    \n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        if cluster != -1:  # Ignore noise\n",
    "            cluster_samples[cluster].append(image_paths[idx])\n",
    "    \n",
    "    # Display cluster samples with facial landmarks\n",
    "    for cluster_id, samples in cluster_samples.items():\n",
    "        print(f\"\\nCluster {cluster_id} Samples:\")\n",
    "        display_samples(samples[:5])  # Show first 5 samples\n",
    "        \n",
    "def display_samples(samples):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    for img_path, ax in zip(samples, axes):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Add facial landmark visualization\n",
    "        # (Implement actual landmark detection here)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "381bf92f-594d-4e8d-aa7f-0957fe80b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'interact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m validate_emotion_clusters(clusters, all_image_paths)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 5. Interactive labeling (modified to show emotion landmarks)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;129m@interact\u001b[39m(cluster\u001b[38;5;241m=\u001b[39mIntSlider(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(clusters), step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_emotion_cluster\u001b[39m(cluster):\n\u001b[1;32m     20\u001b[0m     review_cluster(all_image_paths, clusters, cluster)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interact' is not defined"
     ]
    }
   ],
   "source": [
    "#5. Execution Pipeline\n",
    "# Modified workflow\n",
    "all_image_paths = get_image_paths(DATASET_ROOT, IMAGE_EXTENSIONS)\n",
    "\n",
    "# 1. Preprocess with emotion masks\n",
    "batch_images = [preprocess_image(p) for p in all_image_paths]\n",
    "\n",
    "# 2. Extract emotion-specific features\n",
    "features = emotion_extractor.predict(np.array(batch_images))\n",
    "\n",
    "# 3. Emotion-aware clustering\n",
    "clusters = emotion_clustering(features)\n",
    "\n",
    "# 4. Validate clusters\n",
    "validate_emotion_clusters(clusters, all_image_paths)\n",
    "\n",
    "# 5. Interactive labeling (modified to show emotion landmarks)\n",
    "@interact(cluster=IntSlider(min=0, max=np.max(clusters), step=1))\n",
    "def show_emotion_cluster(cluster):\n",
    "    review_cluster(all_image_paths, clusters, cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996b63d-3c49-48c6-9299-e2dcbfa1e079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions)",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
