{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eb129f-202d-484c-abad-e91a7f9e31f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/img_curation_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import vertexai\n",
    "from vertexai.preview.vision_models import Image, ImageGenerationModel\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c08560-054f-4ca8-a15c-f685716028d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AUTHENTICATION SETUP ---\n",
    "# Set the environment variable for Google Cloud authentication directly in the script.\n",
    "# This line replaces the need to run the 'export' command in your terminal.\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/natalyagrokh/AI/gemini-env/curation_pipeline/key.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc11d746-dc99-479e-907d-f7c59ee41225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# All user-configurable settings are in this class for easy management.\n",
    "class Config:\n",
    "    # 1. Your Google Cloud Project Details\n",
    "    PROJECT_ID = \"MLexpressImgSorting\"\n",
    "    LOCATION = \"us-central1\"\n",
    "\n",
    "    # 2. Folder Paths\n",
    "    # This should point to a folder of your clean, pure emotion face crops\n",
    "    INPUT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_adult\"\n",
    "    # This is where the newly generated images will be saved\n",
    "    OUTPUT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/generated_neutral_speech_faces\"\n",
    "\n",
    "    # 3. Generation Settings\n",
    "    # How many different variations to create for EACH input image\n",
    "    VARIATIONS_PER_IMAGE = 3\n",
    "    \n",
    "    # 4. Model Settings\n",
    "    # The Vertex AI model to use for image editing\n",
    "    MODEL_NAME = \"imagegeneration@006\"\n",
    "\n",
    "# --- DYNAMIC PROMPT COMPONENTS ---\n",
    "# These lists allow you to systematically create a diverse and balanced dataset.\n",
    "# Feel free to add or remove items to suit your specific needs.\n",
    "DEMOGRAPHICS = [\n",
    "    \"a 25-year-old Caucasian woman\", \"a 30-year-old Black man\", \"a 45-year-old East Asian woman\",\n",
    "    \"a 50-year-old Hispanic man\", \"a 22-year-old South Asian person\", \"a 60-year-old Afro-Latina woman\",\n",
    "    \"a 35-year-old Middle Eastern man\", \"a senior white man with a beard\", \"a young Black woman with braids\"\n",
    "]\n",
    "\n",
    "MOUTH_SHAPES = {\n",
    "    \"ah_sound\": \"mouth slightly open as if in the middle of saying 'ah' or 'father'\",\n",
    "    \"oh_sound\": \"mouth rounded as if beginning to say 'oh' or 'boat'\",\n",
    "    \"ff_sound\": \"lips slightly parted, upper teeth may be touching the lower lip, as in the 'f' or 'v' sound\",\n",
    "    \"th_sound\": \"lips parted with the tip of the tongue slightly visible between the teeth\",\n",
    "    \"ee_sound\": \"lips are slightly stretched horizontally, as in the 'ee' sound in 'see', without smiling\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f1ae3d-c9a5-4604-a019-1280c2781e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechInpaintingPipeline:\n",
    "    \"\"\"\n",
    "    An automated pipeline to convert emotional facial expressions to neutral speech expressions.\n",
    "    This version uses the modern GenerativeAI library to support longer timeouts.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize the modern Generative AI model with the timeout from your playbook\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=Config.MODEL_NAME,\n",
    "            request_options={\"timeout\": 1800}  # 30-minute timeout\n",
    "        )\n",
    "\n",
    "        # Initialize MediaPipe Face Mesh for landmark detection\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def create_lower_face_mask(self, image):\n",
    "        \"\"\"Creates a binary mask covering the lower half of the face using landmarks.\"\"\"\n",
    "        # This function is slightly modified to only need the image\n",
    "        ih, iw, _ = image.shape\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_image)\n",
    "        \n",
    "        if not results.multi_face_landmarks:\n",
    "            return None # Return None if no face is found\n",
    "\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "        mask = np.zeros((ih, iw), dtype=np.uint8)\n",
    "\n",
    "        jaw_points_indices = [\n",
    "            172, 136, 150, 149, 176, 148, 152, 377, 400, 378, 379, 365, 397, 288,\n",
    "            415, 318, 402, 317, 14, 87, 178, 88, 95, 58, 169, 4, 395, 394\n",
    "        ]\n",
    "        \n",
    "        points = [\n",
    "            [int(landmarks.landmark[idx].x * iw), int(landmarks.landmark[idx].y * ih)]\n",
    "            for idx in jaw_points_indices\n",
    "        ]\n",
    "\n",
    "        hull = cv2.convexHull(np.array(points))\n",
    "        cv2.fillConvexPoly(mask, hull, 255)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Executes the main processing pipeline.\"\"\"\n",
    "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "        all_image_paths = [os.path.join(r, f) for r, _, fs in os.walk(Config.INPUT_DIR) for f in fs if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        if not all_image_paths:\n",
    "            print(f\"No images found in '{Config.INPUT_DIR}'.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Found {len(all_image_paths)} images to process.\")\n",
    "        \n",
    "        for input_path in tqdm(all_image_paths, desc=\"Processing Images\"):\n",
    "            try:\n",
    "                source_image_cv = cv2.imread(input_path)\n",
    "                if source_image_cv is None:\n",
    "                    print(f\"Warning: Could not read {os.path.basename(input_path)}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                mask_array = self.create_lower_face_mask(source_image_cv)\n",
    "                if mask_array is None:\n",
    "                    print(f\"Warning: No face landmarks detected in {os.path.basename(input_path)}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # The new library can accept image data directly\n",
    "                source_image_pil = Image.open(input_path)\n",
    "                mask_image_pil = Image.fromarray(mask_array)\n",
    "\n",
    "                for i in range(Config.VARIATIONS_PER_IMAGE):\n",
    "                    demographic = random.choice(DEMOGRAPHICS)\n",
    "                    mouth_shape_key, mouth_shape_desc = random.choice(list(MOUTH_SHAPES.items()))\n",
    "                    \n",
    "                    dynamic_prompt = (\n",
    "                        f\"Inpaint the masked area of the image. The original image shows a person's face. \"\n",
    "                        f\"The masked area covers their lower face (mouth, chin, jaw). \"\n",
    "                        f\"Generate a new lower face for {demographic} where the mouth is shaped for speech: {mouth_shape_desc}. \"\n",
    "                        f\"The final expression must be completely neutral. Do not show teeth. \"\n",
    "                        f\"Match the lighting, focus, and skin texture of the original, unmasked part of the image.\"\n",
    "                    )\n",
    "                    \n",
    "                    # Call the API using the modern generate_content method\n",
    "                    response = self.model.generate_content(\n",
    "                        [dynamic_prompt, source_image_pil, mask_image_pil]\n",
    "                    )\n",
    "                    \n",
    "                    # The new library returns image bytes directly\n",
    "                    output_bytes = response.parts[0].data\n",
    "                    output_image = Image.open(io.BytesIO(output_bytes))\n",
    "                    \n",
    "                    # Save the Output\n",
    "                    filename = os.path.basename(input_path)\n",
    "                    base_name = os.path.splitext(filename)[0]\n",
    "                    demo_slug = demographic.replace(' ', '-').lower()\n",
    "                    output_filename = f\"{base_name}_{demo_slug}_{mouth_shape_key}_v{i+1}.png\"\n",
    "                    output_path = os.path.join(Config.OUTPUT_DIR, output_filename)\n",
    "                    output_image.save(output_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nAn error occurred while processing a variation for {os.path.basename(input_path)}: {e}\")\n",
    "\n",
    "        self.face_mesh.close()\n",
    "        print(\"\\nProcessing complete.\")\n",
    "\n",
    "# You will also need to add 'io' to your imports\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa53c177-c6e3-4dc7-a69e-677a64d9ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An unexpected error occurred during pipeline execution: GenerativeModel.__init__() got an unexpected keyword argument 'request_options'\n",
      "Please check your authentication, project ID, and file paths.\n"
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION BLOCK ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        pipeline = SpeechInpaintingPipeline()\n",
    "        pipeline.run()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during pipeline execution: {e}\")\n",
    "        print(\"Please check your authentication, project ID, and file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3769e9-a3de-4751-b605-207b4a19fc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (img_curation_v3)",
   "language": "python",
   "name": "img_curation_v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
