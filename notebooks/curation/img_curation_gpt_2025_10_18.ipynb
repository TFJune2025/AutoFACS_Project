{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9afec7-74f2-4d33-9529-ad6a6bd97d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import hashlib\n",
    "import imagehash\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fdf2c1-9c79-4306-91e2-6c72810eabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- Main Configuration ---\n",
    "# ==============================================================================\n",
    "INPUT_FOLDER = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/pexels_dataset_archive/pexels_scraped\"\n",
    "OUTPUT_FOLDER = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/pexels_dataset_archive/pexels_curated\"\n",
    "YUNET_MODEL_PATH = \"face_detection_yunet_2023mar.onnx\"\n",
    "\n",
    "# 1. MINIMUM SIZE: Lowered to 72 to capture smaller faces in group shots.\n",
    "MIN_WIDTH = 72\n",
    "MIN_HEIGHT = 72\n",
    "\n",
    "# 2. BLUR THRESHOLD (Sharpness): Measures edge clarity. Higher is sharper.\n",
    "BLUR_THRESHOLD = 100.0\n",
    "\n",
    "# 3. CONTRAST THRESHOLD: Measures the dynamic range of the image.\n",
    "CONTRAST_THRESHOLD = 15.0\n",
    "\n",
    "# --- Landmark and Pose Filtering ---\n",
    "REQUIRE_ALL_LANDMARKS = True\n",
    "MAX_EYE_ANGLE = 10.0\n",
    "\n",
    "# How similar two images can be to be considered duplicates.\n",
    "# A value of 0-1 is very strict (nearly identical). Higher values are more lenient.\n",
    "PERCEPTUAL_HASH_THRESHOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d78546-577b-441e-a2c2-c86fce4af7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==============================================================================\n",
    "# # DIAGNOSTIC TOOL: Visualize Face Detection\n",
    "# # ==============================================================================\n",
    "# def run_detection_debug_mode(image_paths, output_folder, model_path):\n",
    "#     \"\"\"\n",
    "#     Runs the face detector on specific images and saves a copy\n",
    "#     with the detected bounding boxes and confidence scores drawn on it.\n",
    "#     \"\"\"\n",
    "#     print(\"--- Running in Detector Debug Mode ---\")\n",
    "#     if not os.path.exists(model_path):\n",
    "#         print(f\"FATAL ERROR: Model file not found at '{model_path}'\")\n",
    "#         return\n",
    "\n",
    "#     try:\n",
    "#         # Initialize the detector with a LOWER threshold to find less confident faces\n",
    "#         face_detector = cv2.FaceDetectorYN.create(\n",
    "#             model=model_path, config=\"\", input_size=(320, 320), score_threshold=0.5\n",
    "#         )\n",
    "#         print(\"  Initialized detector with a lower confidence threshold for debugging.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  Could not initialize the face detector. Error: {e}\")\n",
    "#         return\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     for file_path in image_paths:\n",
    "#         print(f\"  Analyzing '{os.path.basename(file_path)}'...\")\n",
    "#         image = cv2.imread(file_path)\n",
    "#         if image is None:\n",
    "#             print(f\"    - Could not read image. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         height, width, _ = image.shape\n",
    "#         face_detector.setInputSize((width, height))\n",
    "#         _, faces = face_detector.detect(image)\n",
    "        \n",
    "#         if faces is not None:\n",
    "#             print(f\"    - Found {len(faces)} faces.\")\n",
    "#             # Draw results on a copy of the image\n",
    "#             for face_data in faces:\n",
    "#                 box = list(map(int, face_data[0:4]))\n",
    "                \n",
    "#                 # <--- THIS IS THE CORRECTED LINE ---\n",
    "#                 confidence = face_data[14] # Corrected index from 15 to 14\n",
    "                \n",
    "#                 cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (0, 255, 0), 4)\n",
    "                \n",
    "#                 label = f\"Confidence: {confidence:.2f}\"\n",
    "#                 cv2.putText(image, label, (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "#         else:\n",
    "#             print(\"    - No faces found at this confidence level.\")\n",
    "\n",
    "#         # Save the debug image\n",
    "#         base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "#         output_filename = f\"{base_filename}_DETECTED.jpg\"\n",
    "#         output_path = os.path.join(output_folder, output_filename)\n",
    "#         cv2.imwrite(output_path, image)\n",
    "#         print(f\"    - Saved debug image to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee443fd-e927-4a52-8386-d5301bc3e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: Detect, Filter, and Crop High-Quality Faces (with Tuned Thresholds)\n",
    "# ==============================================================================\n",
    "def crop_all_faces(input_folder, output_folder, model_path):\n",
    "    print(\"--- Starting Step 1: Face Detection and Cropping ---\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"FATAL ERROR: Model file not found at '{model_path}'\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Initialize the detector with a more lenient score_threshold\n",
    "    try:\n",
    "        face_detector = cv2.FaceDetectorYN.create(\n",
    "            model=model_path, config=\"\", input_size=(320, 320),\n",
    "            score_threshold=0.7 # <--- LOWERED from 0.9 to 0.7\n",
    "        )\n",
    "        print(\"  Successfully initialized OpenCV FaceDetectorYN with tuned threshold.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not initialize the face detector. Error: {e}\")\n",
    "        sys.exit()\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    total_faces_saved = 0\n",
    "    \n",
    "    ALLOWED_EXTS = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')\n",
    "\n",
    "    all_image_paths = [\n",
    "        os.path.join(dp, f)\n",
    "        for dp, dn, fn in os.walk(os.path.expanduser(input_folder))\n",
    "        for f in fn\n",
    "        if f.lower().endswith(ALLOWED_EXTS)\n",
    "    ]\n",
    "    print(f\"  Found {len(all_image_paths)} total images to process in all subfolders.\")\n",
    "\n",
    "    for file_path in tqdm(all_image_paths, desc=\"Step 1: Cropping faces\"):\n",
    "        try:\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None: continue\n",
    "            \n",
    "            height, width, _ = image.shape\n",
    "            face_detector.setInputSize((width, height))\n",
    "            _, faces = face_detector.detect(image)\n",
    "            \n",
    "            if faces is not None:\n",
    "                for i, face_data in enumerate(faces):\n",
    "                    box = list(map(int, face_data[:4]))\n",
    "                    landmarks = list(map(int, face_data[4:14]))\n",
    "                    \n",
    "                    if REQUIRE_ALL_LANDMARKS and len(landmarks) < 10: continue\n",
    "                    \n",
    "                    right_eye, left_eye = (landmarks[0], landmarks[1]), (landmarks[2], landmarks[3])\n",
    "                    angle = np.degrees(np.arctan2(left_eye[1] - right_eye[1], left_eye[0] - right_eye[0]))\n",
    "                    if abs(angle) > MAX_EYE_ANGLE: continue\n",
    "\n",
    "                    # Build a square, landmark-centered box with margin\n",
    "                    # ---- NEW: landmark-centered, square, margin-padded crop ----\n",
    "                    MARGIN_RATIO = 0.55  # adjust to taste: 0.45 (tighter) ... 0.70 (looser)\n",
    "                    \n",
    "                    # prefer landmarks to anchor the face; fallback to box if landmarks are missing\n",
    "                    if landmarks is not None and len(landmarks) >= 10:\n",
    "                        x_sq, y_sq, w_sq, h_sq = _expand_square_bbox_from_landmarks(\n",
    "                            landmarks, MARGIN_RATIO, width, height\n",
    "                        )\n",
    "                    else:\n",
    "                        # fallback: expand the detector box into a square with margin\n",
    "                        side = int(round(max(w, h) * (1.0 + MARGIN_RATIO)))\n",
    "                        cx = x + w // 2\n",
    "                        cy = y + h // 2\n",
    "                        x_sq = int(cx - side // 2)\n",
    "                        y_sq = int(cy - side // 2)\n",
    "                        w_sq = h_sq = side\n",
    "                    \n",
    "                    # optional: enforce a minimum saved size (keeps out tiny detections)\n",
    "                    if min(w_sq, h_sq) < MIN_WIDTH:\n",
    "                        continue\n",
    "                    \n",
    "                    # crop with safe padding so forehead/chin/ears arenâ€™t cut at the edges\n",
    "                    cropped_face, used_padding = _safe_crop_with_padding(image, x_sq, y_sq, w_sq, h_sq)\n",
    "                    \n",
    "                    # quality checks (reuse your existing thresholds/logic)\n",
    "                    gray_face = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2GRAY)\n",
    "                    sharpness_score = cv2.Laplacian(gray_face, cv2.CV_64F).var()\n",
    "                    if sharpness_score < BLUR_THRESHOLD:\n",
    "                        continue\n",
    "                    \n",
    "                    contrast_score = gray_face.std()\n",
    "                    if contrast_score < CONTRAST_THRESHOLD:\n",
    "                        continue\n",
    "\n",
    "                    # keep your original naming; if you prefer, you can append a suffix when padding occurred\n",
    "                    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                    # e.g., optional: suffix = \"_padded\" if used_padding else \"\"\n",
    "                    suffix = \"\"\n",
    "                    output_filename = f\"{base_filename}_face_{i+1}{suffix}.jpg\"\n",
    "                    output_path = os.path.join(output_folder, output_filename)\n",
    "                    cv2.imwrite(output_path, cropped_face)\n",
    "                    total_faces_saved += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  WARNING: An error occurred on {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"--- Step 1 Complete ---\")\n",
    "    print(f\"  Total high-quality faces found and saved: {total_faces_saved}\\n\")\n",
    "\n",
    "\n",
    "# ---- full-face square cropping helpers ----\n",
    "def _expand_square_bbox_from_landmarks(landmarks, margin_ratio, img_w, img_h):\n",
    "    \"\"\"\n",
    "    landmarks: [x_r_eye, y_r_eye, x_l_eye, y_l_eye,\n",
    "                x_nose, y_nose, x_r_mouth, y_r_mouth, x_l_mouth, y_l_mouth]\n",
    "    Returns: x, y, w, h for a square box centered on landmarks, with margin.\n",
    "    \"\"\"\n",
    "    xs = landmarks[0::2]\n",
    "    ys = landmarks[1::2]\n",
    "    x_min, x_max = min(xs), max(xs)\n",
    "    y_min, y_max = min(ys), max(ys)\n",
    "\n",
    "    cx = (x_min + x_max) / 2.0\n",
    "    cy = (y_min + y_max) / 2.0\n",
    "    side = max(x_max - x_min, y_max - y_min)\n",
    "    side = int(round(side * (1.0 + margin_ratio)))  # add forehead/chin/ears margin\n",
    "\n",
    "    half = side // 2\n",
    "    x0 = int(round(cx - half))\n",
    "    y0 = int(round(cy - half))\n",
    "    return x0, y0, side, side\n",
    "\n",
    "\n",
    "def _safe_crop_with_padding(image, x, y, w, h, pad_mode=cv2.BORDER_REFLECT_101):\n",
    "    \"\"\"\n",
    "    Crops [x:x+w, y:y+h]. If the box spills outside the image, pad the image first.\n",
    "    Returns the cropped image and a bool indicating if padding was used.\n",
    "    \"\"\"\n",
    "    H, W = image.shape[:2]\n",
    "    left   = max(0, -x)\n",
    "    top    = max(0, -y)\n",
    "    right  = max(0, x + w - W)\n",
    "    bottom = max(0, y + h - H)\n",
    "\n",
    "    if any(v > 0 for v in (left, top, right, bottom)):\n",
    "        padded = cv2.copyMakeBorder(image, top, bottom, left, right, pad_mode)\n",
    "        x_p = x + left\n",
    "        y_p = y + top\n",
    "        return padded[y_p:y_p+h, x_p:x_p+w], True\n",
    "    else:\n",
    "        return image[y:y+h, x:x+w], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a9a72-cb23-4a20-9375-5ee365ecd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 2: Remove Perceptual Duplicates\n",
    "# ==============================================================================\n",
    "def remove_perceptual_duplicates(target_folder, hash_threshold):\n",
    "    print(f\"--- Starting Step 2: Removing Perceptual Duplicates (Threshold: {hash_threshold}) ---\")\n",
    "    hashes = {}\n",
    "    duplicates_to_remove = []\n",
    "    \n",
    "    image_paths = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(target_folder)) for f in fn if f.lower().endswith(('.png', '.jpg', 'jpeg'))]\n",
    "\n",
    "    for file_path in tqdm(image_paths, desc=\"Step 2: Hashing images\"):\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            # Use perceptual hash (phash) which is robust against compression and small changes\n",
    "            h = imagehash.phash(img)\n",
    "            \n",
    "            # Check if this hash is a close match to any we've already seen\n",
    "            found_match = False\n",
    "            for seen_hash in hashes:\n",
    "                if (h - seen_hash) <= hash_threshold:\n",
    "                    duplicates_to_remove.append(file_path)\n",
    "                    found_match = True\n",
    "                    break\n",
    "            \n",
    "            if not found_match:\n",
    "                hashes[h] = file_path\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  WARNING: Could not process {file_path} for duplicate check: {e}\")\n",
    "            \n",
    "    if duplicates_to_remove:\n",
    "        print(f\"  Found {len(duplicates_to_remove)} perceptual duplicate images. Removing...\")\n",
    "        for dup_path in duplicates_to_remove:\n",
    "            try: os.remove(dup_path)\n",
    "            except Exception as e: print(f\"  Failed to remove duplicate {dup_path}: {e}\")\n",
    "    else:\n",
    "        print(\"  No duplicate files were found.\")\n",
    "    print(f\"--- Step 2 Complete ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44416a41-bd42-4452-a500-7e8bc8ef837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Main Execution Block\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Tuned Image Curation Pipeline...\")\n",
    "    print(\"=========================================\")\n",
    "\n",
    "    # Step 1: Crop faces using all the integrated quality filters\n",
    "    crop_all_faces(INPUT_FOLDER, OUTPUT_FOLDER, YUNET_MODEL_PATH)\n",
    "\n",
    "    # Step 2: Remove visually similar duplicates from the output\n",
    "    # Note: Ensure PERCEPTUAL_HASH_THRESHOLD is set in your configuration.\n",
    "    remove_perceptual_duplicates(OUTPUT_FOLDER, hash_threshold=PERCEPTUAL_HASH_THRESHOLD)\n",
    "    \n",
    "    print(\"=========================================\")\n",
    "    print(\"Pipeline finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44041c87-3d77-4a18-a1ec-1eb8548ba8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
