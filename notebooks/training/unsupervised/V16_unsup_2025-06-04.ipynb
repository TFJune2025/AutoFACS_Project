{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c4d2b-421d-4aed-8b78-b1ad767cd028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V16_unsup recap -> moved to unsup learning to unsup transfer learning\n",
    "    # config section:  swapped SSL for V13 model\n",
    "    # section #1:  added torch.device\n",
    "    # section #3:  added outputs = model(**inputs)\n",
    "        # added if hasattr(outputs, \"pooler_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2df23ee-83b8-4152-96b6-b8c13c476ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== IMPORTS SECTION ==========\n",
    "\n",
    "# ===== STANDARD LIBRARY =====\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import platform\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# ===== THIRD PARTY: CORE =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "# ===== VISUALIZATION =====\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ===== ML/DEEP LEARNING =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ===== IMAGE/PROCESSING =====\n",
    "from PIL import Image\n",
    "from scipy.stats import entropy\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== TRANSFORMERS =====\n",
    "from transformers import AutoImageProcessor, AutoModel, ViTForImageClassification\n",
    "\n",
    "# ===== OPTIONAL: UMAP =====\n",
    "try:\n",
    "    import umap\n",
    "    HAVE_UMAP = True\n",
    "except ImportError:\n",
    "    HAVE_UMAP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a20e76bf-d5d2-4a81-8353-858eff6b91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG SECTION (Version-controlled) ==========\n",
    "CFG_VERSION = \"V16_unsup_2025_06_04\"\n",
    "V13_MODEL_PATH = \"V13_20250527_161430\"\n",
    "V13_PROCESSOR_PATH = V13_MODEL_PATH\n",
    "UNSUP_BASE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions_unsup\"\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/temp_autogluon_images\"\n",
    "DATASET_NAME = os.path.basename(os.path.normpath(IMAGE_DIR))\n",
    "DATESTAMP = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "N_CLUSTERS = 20\n",
    "KL_OOD_THRESHOLD = 0.55\n",
    "EUCLIDEAN_OUTLIER_PERCENTILE = 98\n",
    "BATCH_SIZE = 32\n",
    "VALID_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.webp', '.jfif')\n",
    "T_SNE_SUBSAMPLE = 5000\n",
    "CLUSTER_N_INIT = 10\n",
    "\n",
    "# Auto-versioned output filenames\n",
    "OUT_PREFIX = f\"{CFG_VERSION}_{DATASET_NAME}_clusters{N_CLUSTERS}_kl{KL_OOD_THRESHOLD}_eu{EUCLIDEAN_OUTLIER_PERCENTILE}_{DATESTAMP}\"\n",
    "\n",
    "#define label names\n",
    "LABEL_NAMES = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"happiness\",\n",
    "    \"sadness\", \"surprise\", \"neutral\", \"questioning\"\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe1da61-9dd2-4679-ae82-0f5a32d54652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Unsupervised output directory created: ./V16_unsup_20250604_142640\n"
     ]
    }
   ],
   "source": [
    "# ======= DYNAMIC UNSUPERVISED OUTPUT FOLDER CREATION ========\n",
    "def get_next_unsup_version(base_dir=\".\", prefix=\"V\"):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, f\"{prefix}*_*\"))\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(prefix) and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"{prefix}{next_version}\"\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_unsup_version(base_dir=\".\")\n",
    "VERSION_TAG = VERSION + \"_unsup_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\".\", VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Unsupervised output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa19316a-a936-4a1a-9b90-b8c600f02a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ARTIFACT PREFIXES ==========\n",
    "OUT_PREFIX = os.path.join(\n",
    "    SAVE_DIR, f\"{VERSION_TAG}_{DATASET_NAME}_clusters{N_CLUSTERS}_kl{KL_OOD_THRESHOLD}_eu{EUCLIDEAN_OUTLIER_PERCENTILE}\"\n",
    ")\n",
    "EMBED_FILE = f\"{OUT_PREFIX}_embeddings.npy\"\n",
    "PATHS_FILE = f\"{OUT_PREFIX}_paths.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e696c3-13e8-4753-ac73-bcf5c64d2128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SYSTEM INFO =====\n",
      "Python: 3.10.16\n",
      "Platform: macOS-15.5-arm64-arm-64bit\n",
      "CPU: arm\n",
      "RAM (GB): 17.18\n",
      "=======================\n",
      "ðŸ–¥ï¸ Using CPU only for feature extraction (edit if using GPU)\n"
     ]
    }
   ],
   "source": [
    "# ========== HARDWARE/ENV LOG ==========\n",
    "print(\"===== SYSTEM INFO =====\")\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"CPU:\", platform.processor())\n",
    "print(\"RAM (GB):\", round(psutil.virtual_memory().total / 1e9, 2))\n",
    "print(\"=======================\")\n",
    "print(\"ðŸ–¥ï¸ Using CPU only for feature extraction (edit if using GPU)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b31e90-3cdb-426c-ba93-3a9954defab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at V13_20250527_161430 were not used when initializing ViTForImageClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at V13_20250527_161430 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading V13 backbone: V13_20250527_161430\n",
      "âœ… Model config loaded:\n",
      "  Model arch: ['ViTForImageClassification']\n",
      "  # Labels: 8\n",
      "  id2label: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral', 7: 'questioning'}\n",
      "âœ… Processor loaded: ViTImageProcessorFast\n",
      "ðŸ–¥ï¸ Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. LOAD SUPERVISED (V13) MODEL ==========\n",
    "print(f\"ðŸ”„ Loading V13 backbone: {V13_MODEL_PATH}\")\n",
    "device = torch.device(\"cpu\")  # Change to \"cuda\" if using GPU\n",
    "model = ViTForImageClassification.from_pretrained(V13_MODEL_PATH, local_files_only=True).to(device).eval()\n",
    "processor = AutoImageProcessor.from_pretrained(V13_MODEL_PATH, local_files_only=True)\n",
    "\n",
    "# Print model info\n",
    "print(\"âœ… Model config loaded:\")\n",
    "print(f\"  Model arch: {model.config.architectures}\")\n",
    "print(f\"  # Labels: {model.config.num_labels}\")\n",
    "if hasattr(model.config, \"id2label\"):\n",
    "    print(f\"  id2label: {model.config.id2label}\")\n",
    "print(\"âœ… Processor loaded:\", type(processor).__name__)\n",
    "print(\"ðŸ–¥ï¸ Model device:\", model.device)\n",
    "model = model.to(\"cpu\").eval()  # Or use CUDA/MPS as available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92eddf46-e74b-4ec9-a0e9-c86b2712b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¼ï¸ Found 31002 images.\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. IMAGE PATHS ==========\n",
    "image_paths = [\n",
    "    os.path.join(IMAGE_DIR, f)\n",
    "    for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith(VALID_EXTENSIONS)\n",
    "]\n",
    "print(f\"ðŸ–¼ï¸ Found {len(image_paths)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4e0cd8e-8414-45b8-8418-f42d05cfa38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: (31002, 768)\n",
      "Loaded file paths: 31002\n"
     ]
    }
   ],
   "source": [
    "# # ========== LOADING SAVED NPY FILES WHEN CODE FAILS ==========\n",
    "\n",
    "# # Adjust these to your actual saved file locations (if you use dynamic folders, fill in SAVE_DIR and OUT_PREFIX accordingly)\n",
    "# EMBED_FILE = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/V16_unsup_2025_06_04_temp_autogluon_images_clusters20_kl0.55_eu98_20250604_embeddings.npy\"  # e.g., \"/Users/natalyagrokh/AI/ml_expressions/img_expressions_unsup/V16_unsup_20250604_191044/V16_unsup_20250604_191044_temp_autogluon_images_clusters20_kl0.55_eu98_embeddings.npy\"\n",
    "# PATHS_FILE = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/V16_unsup_2025_06_04_temp_autogluon_images_clusters20_kl0.55_eu98_20250604_paths.npy\"\n",
    "\n",
    "# # Load arrays\n",
    "# all_embeddings = np.load(EMBED_FILE)\n",
    "# all_files = np.load(PATHS_FILE, allow_pickle=True).tolist()\n",
    "\n",
    "# print(\"Loaded embeddings:\", all_embeddings.shape)\n",
    "# print(\"Loaded file paths:\", len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d730c9-f0f3-4dd1-a8e8-32329889cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 3. EMBEDDING EXTRACTION (SUPERVISED BACKBONE) ==========\n",
    "EMBED_FILE = f\"{OUT_PREFIX}_embeddings.npy\"\n",
    "PATHS_FILE = f\"{OUT_PREFIX}_paths.npy\"\n",
    "if os.path.exists(EMBED_FILE) and os.path.exists(PATHS_FILE):\n",
    "    all_embeddings = np.load(EMBED_FILE)\n",
    "    all_files = np.load(PATHS_FILE, allow_pickle=True).tolist()\n",
    "    print(f\"âœ… Loaded embeddings: {all_embeddings.shape}\")\n",
    "    print(f\"âœ… Loaded image paths: {len(all_files)}\")\n",
    "else:\n",
    "    all_embeddings = []\n",
    "    all_files = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(image_paths), BATCH_SIZE)):\n",
    "            batch_paths = image_paths[i:i+BATCH_SIZE]\n",
    "            images = []\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    images.append(img)\n",
    "                    all_files.append(path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Skip: {path} ({e})\")\n",
    "            if not images:\n",
    "                continue\n",
    "            inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            # outputs.hidden_states is a tuple: (layer_0, ..., layer_N)\n",
    "            # Use the last layer's [CLS] token or mean-pool all tokens (recommended for ViT)\n",
    "            last_hidden = outputs.hidden_states[-1]  # [batch, seq, dim]\n",
    "            feats = last_hidden.mean(dim=1).cpu().numpy()\n",
    "            all_embeddings.append(feats)\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    assert all_embeddings.shape[0] == len(all_files), \"Some images were skipped unexpectedly.\"\n",
    "    print(\"âœ… Extracted embeddings shape:\", all_embeddings.shape)\n",
    "    np.save(EMBED_FILE, all_embeddings)\n",
    "    np.save(PATHS_FILE, np.array(all_files))\n",
    "    print(f\"ðŸ’¾ Saved embeddings and image paths to disk as {EMBED_FILE}, {PATHS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad5d690-1e4d-4d55-baea-e31dae4fa7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Starting MiniBatchKMeans clustering...\n",
      "Init 1/10 with method random\n",
      "Inertia for init 1/10: 225331920.0\n",
      "Init 2/10 with method random\n",
      "Inertia for init 2/10: 242456064.0\n",
      "Init 3/10 with method random\n",
      "Inertia for init 3/10: 183423856.0\n",
      "Init 4/10 with method random\n",
      "Inertia for init 4/10: 187563840.0\n",
      "Init 5/10 with method random\n",
      "Inertia for init 5/10: 204947136.0\n",
      "Init 6/10 with method random\n",
      "Inertia for init 6/10: 179880256.0\n",
      "Init 7/10 with method random\n",
      "Inertia for init 7/10: 223376432.0\n",
      "Init 8/10 with method random\n",
      "Inertia for init 8/10: 202090608.0\n",
      "Init 9/10 with method random\n",
      "Inertia for init 9/10: 183129440.0\n",
      "Init 10/10 with method random\n",
      "Inertia for init 10/10: 195172272.0\n",
      "Minibatch step 1/3027: mean batch inertia: 57913.8828125\n",
      "Minibatch step 2/3027: mean batch inertia: 35938.33984375, ewa inertia: 35938.33984375\n",
      "Minibatch step 3/3027: mean batch inertia: 33940.296875, ewa inertia: 35806.352874747\n",
      "Minibatch step 4/3027: mean batch inertia: 34400.15625, ewa inertia: 35713.462164574375\n",
      "Minibatch step 5/3027: mean batch inertia: 33854.51171875, ewa inertia: 35590.663386615845\n",
      "Minibatch step 6/3027: mean batch inertia: 34248.53515625, ewa inertia: 35502.004914345765\n",
      "Minibatch step 7/3027: mean batch inertia: 33735.078125, ewa inertia: 35385.28504644329\n",
      "Minibatch step 8/3027: mean batch inertia: 33030.42578125, ewa inertia: 35229.72746249606\n",
      "Minibatch step 9/3027: mean batch inertia: 32892.34765625, ewa inertia: 35075.32453880507\n",
      "Minibatch step 10/3027: mean batch inertia: 33582.578125, ewa inertia: 34976.716511985964\n",
      "Minibatch step 11/3027: mean batch inertia: 34135.8828125, ewa inertia: 34921.17261570021\n",
      "Minibatch step 12/3027: mean batch inertia: 33562.30859375, ewa inertia: 34831.408608444326\n",
      "Minibatch step 13/3027: mean batch inertia: 33842.32421875, ewa inertia: 34766.07154976955\n",
      "Minibatch step 14/3027: mean batch inertia: 33052.96875, ewa inertia: 34652.90719361279\n",
      "Minibatch step 15/3027: mean batch inertia: 33875.95703125, ewa inertia: 34601.583323906016\n",
      "Minibatch step 16/3027: mean batch inertia: 33350.28125, ewa inertia: 34518.92465708798\n",
      "Minibatch step 17/3027: mean batch inertia: 33653.49609375, ewa inertia: 34461.756070250696\n",
      "Minibatch step 18/3027: mean batch inertia: 33280.078125, ewa inertia: 34383.696642715506\n",
      "Minibatch step 19/3027: mean batch inertia: 33434.390625, ewa inertia: 34320.987268645855\n",
      "Minibatch step 20/3027: mean batch inertia: 32215.26171875, ewa inertia: 34181.88699040869\n",
      "Minibatch step 21/3027: mean batch inertia: 33832.875, ewa inertia: 34158.83191327561\n",
      "Minibatch step 22/3027: mean batch inertia: 33107.671875, ewa inertia: 34089.39425374626\n",
      "Minibatch step 23/3027: mean batch inertia: 32969.4765625, ewa inertia: 34015.41459269176\n",
      "Minibatch step 24/3027: mean batch inertia: 33464.60546875, ewa inertia: 33979.029175608484\n",
      "Minibatch step 25/3027: mean batch inertia: 33567.35546875, ewa inertia: 33951.83478307724\n",
      "Minibatch step 26/3027: mean batch inertia: 32563.064453125, ewa inertia: 33860.095221236705\n",
      "Minibatch step 27/3027: mean batch inertia: 33185.98046875, ewa inertia: 33815.56446572618\n",
      "Minibatch step 28/3027: mean batch inertia: 33523.80078125, ewa inertia: 33796.29110425125\n",
      "Minibatch step 29/3027: mean batch inertia: 33612.84375, ewa inertia: 33784.17291628535\n",
      "Minibatch step 30/3027: mean batch inertia: 33211.86328125, ewa inertia: 33746.36721578694\n",
      "Minibatch step 31/3027: mean batch inertia: 34721.0, ewa inertia: 33810.749628523394\n",
      "Minibatch step 32/3027: mean batch inertia: 34734.2109375, ewa inertia: 33871.75174963374\n",
      "Minibatch step 33/3027: mean batch inertia: 33296.5703125, ewa inertia: 33833.7563432779\n",
      "Minibatch step 34/3027: mean batch inertia: 33160.984375, ewa inertia: 33789.31428957234\n",
      "Minibatch step 35/3027: mean batch inertia: 32845.32421875, ewa inertia: 33726.95607697859\n",
      "Minibatch step 36/3027: mean batch inertia: 33298.25390625, ewa inertia: 33698.636816079576\n",
      "Minibatch step 37/3027: mean batch inertia: 31815.62890625, ewa inertia: 33574.24884719492\n",
      "Minibatch step 38/3027: mean batch inertia: 32726.03515625, ewa inertia: 33518.21744252262\n",
      "Minibatch step 39/3027: mean batch inertia: 32899.234375, ewa inertia: 33477.32858266111\n",
      "Minibatch step 40/3027: mean batch inertia: 33860.0078125, ewa inertia: 33502.60765445126\n",
      "Minibatch step 41/3027: mean batch inertia: 32723.521484375, ewa inertia: 33451.14268408336\n",
      "Minibatch step 42/3027: mean batch inertia: 33275.6171875, ewa inertia: 33439.547799168904\n",
      "Minibatch step 43/3027: mean batch inertia: 33125.0859375, ewa inertia: 33418.775038703854\n",
      "Minibatch step 44/3027: mean batch inertia: 32824.16796875, ewa inertia: 33379.49641149792\n",
      "Minibatch step 45/3027: mean batch inertia: 32706.828125, ewa inertia: 33335.061206816186\n",
      "Minibatch step 46/3027: mean batch inertia: 33702.23828125, ewa inertia: 33359.316235311504\n",
      "Minibatch step 47/3027: mean batch inertia: 34227.640625, ewa inertia: 33416.67611500321\n",
      "Minibatch step 48/3027: mean batch inertia: 33285.95703125, ewa inertia: 33408.04105763694\n",
      "Minibatch step 49/3027: mean batch inertia: 33227.5625, ewa inertia: 33396.11898280417\n",
      "Minibatch step 50/3027: mean batch inertia: 32880.77734375, ewa inertia: 33362.07648121455\n",
      "Minibatch step 51/3027: mean batch inertia: 33563.8515625, ewa inertia: 33375.40536443464\n",
      "Minibatch step 52/3027: mean batch inertia: 33120.90625, ewa inertia: 33358.59363052624\n",
      "Minibatch step 53/3027: mean batch inertia: 32939.42578125, ewa inertia: 33330.904189010325\n",
      "Minibatch step 54/3027: mean batch inertia: 33359.8046875, ewa inertia: 33332.81330170609\n",
      "Minibatch step 55/3027: mean batch inertia: 33427.76953125, ewa inertia: 33339.0859320356\n",
      "Minibatch step 56/3027: mean batch inertia: 32615.162109375, ewa inertia: 33291.264882820724\n",
      "Minibatch step 57/3027: mean batch inertia: 33250.5078125, ewa inertia: 33288.572547239746\n",
      "Minibatch step 58/3027: mean batch inertia: 34182.24609375, ewa inertia: 33347.606944661056\n",
      "Minibatch step 59/3027: mean batch inertia: 32540.33203125, ewa inertia: 33294.27987880724\n",
      "Minibatch step 60/3027: mean batch inertia: 34711.46875, ewa inertia: 33387.8967161521\n",
      "Minibatch step 61/3027: mean batch inertia: 33616.7109375, ewa inertia: 33403.01175422327\n",
      "Minibatch step 62/3027: mean batch inertia: 33609.421875, ewa inertia: 33416.64681945407\n",
      "Minibatch step 63/3027: mean batch inertia: 34217.65625, ewa inertia: 33469.55999926757\n",
      "Minibatch step 64/3027: mean batch inertia: 33221.2421875, ewa inertia: 33453.15659061357\n",
      "Minibatch step 65/3027: mean batch inertia: 33879.3203125, ewa inertia: 33481.308166345705\n",
      "Minibatch step 66/3027: mean batch inertia: 32551.29296875, ewa inertia: 33419.87310765216\n",
      "Minibatch step 67/3027: mean batch inertia: 33290.546875, ewa inertia: 33411.330059415806\n",
      "Converged (lack of improvement in inertia) at step 67/3027\n",
      "âœ… MiniBatchKMeans done in 0.42 sec\n"
     ]
    }
   ],
   "source": [
    "# ========== 4. MINI-BATCH KMEANS CLUSTERING ==========\n",
    "print(\"â³ Starting MiniBatchKMeans clustering...\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    batch_size=1024,\n",
    "    max_iter=100,\n",
    "    n_init=CLUSTER_N_INIT,\n",
    "    init=\"random\",\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(all_embeddings)\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"âœ… MiniBatchKMeans done in {end - start:.2f} sec\")\n",
    "np.save(f\"{OUT_PREFIX}_cluster_labels.npy\", cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8714d68d-877e-4b2b-b892-83598b559f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¼ï¸ Saved TSNE visualization to ./V16_unsup_20250604_142640/V16_unsup_20250604_142640_temp_autogluon_images_clusters20_kl0.55_eu98_tsne.png\n"
     ]
    }
   ],
   "source": [
    "# ========== 5. CLUSTER VISUALIZATION (t-SNE/UMAP, subsampled) ==========\n",
    "def visualize_embeddings(embeddings, cluster_labels, method=\"tsne\", out_path=\"embedding_grid.png\", subsample=T_SNE_SUBSAMPLE):\n",
    "    n_points = embeddings.shape[0]\n",
    "    if n_points > subsample:\n",
    "        idx = np.random.choice(n_points, subsample, replace=False)\n",
    "        emb = embeddings[idx]\n",
    "        lbl = np.array(cluster_labels)[idx]\n",
    "    else:\n",
    "        emb = embeddings\n",
    "        lbl = np.array(cluster_labels)\n",
    "    if method == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "    elif method == \"umap\" and HAVE_UMAP:\n",
    "        reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    else:\n",
    "        print(\"Unknown or unavailable method; using t-SNE.\")\n",
    "        reducer = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "    coords = reducer.fit_transform(emb)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for k in range(N_CLUSTERS):\n",
    "        mask = (lbl == k)\n",
    "        plt.scatter(coords[mask, 0], coords[mask, 1], s=7, alpha=0.7, label=f\"C{k}\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"{V13_MODEL_PATH} SSL {method.upper()} by cluster\")\n",
    "    plt.savefig(out_path)\n",
    "    print(f\"ðŸ–¼ï¸ Saved {method.upper()} visualization to {out_path}\")\n",
    "    plt.close()\n",
    "\n",
    "visualize_embeddings(\n",
    "    all_embeddings, cluster_labels,\n",
    "    method=\"tsne\",\n",
    "    out_path=f\"{OUT_PREFIX}_tsne.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe7e5395-cfd0-4462-a624-8a0442ef28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged 621 outlier images by Euclidean distance.\n"
     ]
    }
   ],
   "source": [
    "# ========== 6. EUCLIDEAN OUTLIER DETECTION ==========\n",
    "distances = np.linalg.norm(all_embeddings - kmeans.cluster_centers_[cluster_labels], axis=1)\n",
    "distance_threshold = np.percentile(distances, EUCLIDEAN_OUTLIER_PERCENTILE)\n",
    "euclid_outlier_indices = np.where(distances > distance_threshold)[0]\n",
    "print(f\"Flagged {len(euclid_outlier_indices)} outlier images by Euclidean distance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedc0338-e67c-4a06-b922-76e00d22850d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged 3019 OOD images by KL-divergence.\n"
     ]
    }
   ],
   "source": [
    "# ========== 7. KL-DIVERGENCE OOD DETECTION ==========\n",
    "def compute_kl_divergence(feats, cluster_centers, labels):\n",
    "    kl_scores = []\n",
    "    for f, lbl in zip(feats, labels):\n",
    "        p = torch.softmax(torch.tensor(f), dim=0)\n",
    "        q = torch.softmax(torch.tensor(cluster_centers[lbl]), dim=0)\n",
    "        kl = torch.sum(p * torch.log((p + 1e-8) / (q + 1e-8))).item()\n",
    "        kl_scores.append(kl)\n",
    "    return kl_scores\n",
    "\n",
    "kl_scores = compute_kl_divergence(all_embeddings, kmeans.cluster_centers_, cluster_labels)\n",
    "kl_outlier_indices = np.where(np.array(kl_scores) > KL_OOD_THRESHOLD)[0]\n",
    "print(f\"Flagged {len(kl_outlier_indices)} OOD images by KL-divergence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de4f7e5b-4b0b-4480-80ee-b25328b97746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved pseudo-labels and OOD flags to ./V16_unsup_20250604_142640/V16_unsup_20250604_142640_temp_autogluon_images_clusters20_kl0.55_eu98_ssl_pseudolabels.csv\n"
     ]
    }
   ],
   "source": [
    "# ========== 8. SAVE LABELS/OUTLIERS ==========\n",
    "out_csv = f\"{OUT_PREFIX}_ssl_pseudolabels.csv\"\n",
    "with open(out_csv, \"w\") as f:\n",
    "    f.write(\"path,cluster,euclid_dist,kl,euclid_ood,kl_ood\\n\")\n",
    "    for idx, (p, c, dist, kl) in enumerate(zip(all_files, cluster_labels, distances, kl_scores)):\n",
    "        euclid_flag = int(idx in euclid_outlier_indices)\n",
    "        kl_flag = int(idx in kl_outlier_indices)\n",
    "        f.write(f\"{p},{c},{dist:.3f},{kl:.3f},{euclid_flag},{kl_flag}\\n\")\n",
    "print(f\"âœ… Saved pseudo-labels and OOD flags to {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed9b5a1-87e0-4409-941c-44111f28f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 1524 samples\n",
      "Cluster 1: 2085 samples\n",
      "Cluster 2: 2374 samples\n",
      "Cluster 3: 850 samples\n",
      "Cluster 4: 790 samples\n",
      "Cluster 5: 1055 samples\n",
      "Cluster 6: 2324 samples\n",
      "Cluster 7: 984 samples\n",
      "Cluster 8: 2694 samples\n",
      "Cluster 9: 1263 samples\n",
      "Cluster 10: 1371 samples\n",
      "Cluster 11: 1509 samples\n",
      "Cluster 12: 1579 samples\n",
      "Cluster 13: 658 samples\n",
      "Cluster 14: 2371 samples\n",
      "Cluster 15: 1505 samples\n",
      "Cluster 16: 599 samples\n",
      "Cluster 17: 1728 samples\n",
      "Cluster 18: 1071 samples\n",
      "Cluster 19: 2668 samples\n"
     ]
    }
   ],
   "source": [
    "# ========== 9. PER-CLUSTER REPORT ==========\n",
    "for c in range(N_CLUSTERS):\n",
    "    cluster_indices = np.where(cluster_labels == c)[0]\n",
    "    print(f\"Cluster {c}: {len(cluster_indices)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364613b4-0941-48eb-bc96-bcbb68195b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OOD (junk) images symlinked to ./V16_unsup_20250604_142640/V16_unsup_20250604_142640_ood_flagged\n"
     ]
    }
   ],
   "source": [
    "# ========== 10. SAVE OOD/JUNK FILES ==========\n",
    "OOD_DIR = os.path.join(SAVE_DIR, f\"{VERSION_TAG}_ood_flagged\")\n",
    "os.makedirs(OOD_DIR, exist_ok=True)\n",
    "for idx in set(list(euclid_outlier_indices) + list(kl_outlier_indices)):\n",
    "    src = all_files[idx]\n",
    "    try:\n",
    "        fname = os.path.basename(src)\n",
    "        os.symlink(src, os.path.join(OOD_DIR, fname))\n",
    "    except FileExistsError:\n",
    "        continue\n",
    "\n",
    "print(f\"âœ… OOD (junk) images symlinked to {OOD_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57d4a8d-d9f9-413d-88e6-06bfef1ccb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 00: 1524 total, 40 centroid faces exported\n",
      "Cluster 01: 2085 total, 40 centroid faces exported\n",
      "Cluster 02: 2374 total, 40 centroid faces exported\n",
      "Cluster 03: 850 total, 40 centroid faces exported\n",
      "Cluster 04: 790 total, 40 centroid faces exported\n",
      "Cluster 05: 1055 total, 40 centroid faces exported\n",
      "Cluster 06: 2324 total, 40 centroid faces exported\n",
      "Cluster 07: 984 total, 40 centroid faces exported\n",
      "Cluster 08: 2694 total, 40 centroid faces exported\n",
      "Cluster 09: 1263 total, 40 centroid faces exported\n",
      "Cluster 10: 1371 total, 40 centroid faces exported\n",
      "Cluster 11: 1509 total, 40 centroid faces exported\n",
      "Cluster 12: 1579 total, 40 centroid faces exported\n",
      "Cluster 13: 658 total, 40 centroid faces exported\n",
      "Cluster 14: 2371 total, 40 centroid faces exported\n",
      "Cluster 15: 1505 total, 40 centroid faces exported\n",
      "Cluster 16: 599 total, 40 centroid faces exported\n",
      "Cluster 17: 1728 total, 40 centroid faces exported\n",
      "Cluster 18: 1071 total, 40 centroid faces exported\n",
      "Cluster 19: 2668 total, 40 centroid faces exported\n",
      "âœ… Cluster browser and centroid faces exported to ./V16_unsup_20250604_142640/V16_unsup_20250604_142640_cluster_browser/cluster_##/ and centroid_faces/\n"
     ]
    }
   ],
   "source": [
    "# ========== 11. CLUSTER EXPLORER & CENTROID FACES ==========\n",
    "N_PER_CENTROID = 40   # Or adjust to your preferred # of centroids per cluster\n",
    "CLUSTER_BROWSER_ROOT = os.path.join(SAVE_DIR, f\"{VERSION_TAG}_cluster_browser\")\n",
    "\n",
    "centroids = np.vstack([\n",
    "    all_embeddings[cluster_labels == c].mean(axis=0)\n",
    "    for c in range(N_CLUSTERS)\n",
    "])\n",
    "\n",
    "for c in range(N_CLUSTERS):\n",
    "    cdir = os.path.join(CLUSTER_BROWSER_ROOT, f\"cluster_{c:02d}\")\n",
    "    centroids_dir = os.path.join(cdir, \"centroid_faces\")\n",
    "    os.makedirs(cdir, exist_ok=True)\n",
    "    os.makedirs(centroids_dir, exist_ok=True)\n",
    "    indices = np.where(cluster_labels == c)[0]\n",
    "    if len(indices) == 0:\n",
    "        continue\n",
    "    # 1. Symlink/copy all cluster images\n",
    "    for idx in indices:\n",
    "        src = all_files[idx]\n",
    "        dst = os.path.join(cdir, os.path.basename(src))\n",
    "        try:\n",
    "            os.symlink(src, dst)\n",
    "        except FileExistsError:\n",
    "            continue\n",
    "        except OSError:\n",
    "            shutil.copy(src, dst)\n",
    "    # 2. Find and symlink/copy centroid images\n",
    "    c_embeds = all_embeddings[indices]\n",
    "    dists = np.linalg.norm(c_embeds - centroids[c], axis=1)\n",
    "    sorted_idx = np.argsort(dists)\n",
    "    chosen = indices[sorted_idx[:N_PER_CENTROID]]\n",
    "    for idx in chosen:\n",
    "        src = all_files[idx]\n",
    "        dst = os.path.join(centroids_dir, os.path.basename(src))\n",
    "        try:\n",
    "            os.symlink(src, dst)\n",
    "        except FileExistsError:\n",
    "            continue\n",
    "        except OSError:\n",
    "            shutil.copy(src, dst)\n",
    "    print(f\"Cluster {c:02d}: {len(indices)} total, {len(chosen)} centroid faces exported\")\n",
    "\n",
    "print(f\"âœ… Cluster browser and centroid faces exported to {CLUSTER_BROWSER_ROOT}/cluster_##/ and centroid_faces/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f34da65e-2c45-4dfe-924c-121e8436e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V13 Prediction Extraction: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 969/969 [22:44<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved V13 logits/probabilities for all images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========== 12. V13 PREDICTION EXTRACTION FOR ALL IMAGES ==========\n",
    "\n",
    "# Store logits and predict V13 softmax for all images\n",
    "all_logits = []\n",
    "all_probs = []\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(all_files), BATCH_SIZE), desc=\"V13 Prediction Extraction\"):\n",
    "        batch_paths = all_files[i:i+BATCH_SIZE]\n",
    "        images = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Skip: {path} ({e})\")\n",
    "        if not images:\n",
    "            continue\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.cpu().numpy()\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        all_logits.append(logits)\n",
    "        all_probs.append(probs)\n",
    "\n",
    "all_logits = np.vstack(all_logits)\n",
    "all_probs = np.vstack(all_probs)\n",
    "np.save(os.path.join(SAVE_DIR, \"v13_logits.npy\"), all_logits)\n",
    "np.save(os.path.join(SAVE_DIR, \"v13_probs.npy\"), all_probs)\n",
    "print(\"âœ… Saved V13 logits/probabilities for all images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd665c1f-b98f-489a-acf5-1864f158fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 13. LABEL CONSISTENCY HEATMAP ==========\n",
    "\n",
    "all_preds = np.argmax(all_probs, axis=1)\n",
    "n_clusters = np.max(cluster_labels) + 1\n",
    "n_labels = len(LABEL_NAMES)  # Or len(id2label)\n",
    "\n",
    "cluster_label_counts = np.zeros((n_clusters, n_labels), dtype=int)\n",
    "for cl, lab in zip(cluster_labels, V13_preds):\n",
    "    cluster_label_counts[cl, lab] += 1\n",
    "\n",
    "cluster_label_perc = cluster_label_counts / cluster_label_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(\n",
    "    cluster_label_perc, annot=True, fmt=\".2f\",\n",
    "    xticklabels=LABEL_NAMES,\n",
    "    yticklabels=[f\"C{k}\" for k in range(n_clusters)]\n",
    ")\n",
    "plt.xlabel(\"V13 Predicted Label\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.title(\"Cluster Label Consistency Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, f\"{VERSION_TAG}_cluster_label_heatmap.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0080fe22-e28f-49a5-b8c6-db2d88eb37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 14. CLUSTER ENTROPY BARPLOT ==========\n",
    "entropy_per_image = entropy(all_probs, axis=1)\n",
    "avg_entropy_per_cluster = [\n",
    "    np.mean(entropy_per_image[cluster_labels == k])\n",
    "    for k in range(n_clusters)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(n_clusters), avg_entropy_per_cluster)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Avg Entropy\")\n",
    "plt.title(\"Mean Prediction Entropy per Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, f\"{VERSION_TAG}_cluster_entropy_barplot.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a719b64-924e-41f6-9513-c050a6111361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained 1480/31002 images with confidence >= 0.20\n"
     ]
    }
   ],
   "source": [
    "# ========== 15. CLUSTER REFINEMENT: REMOVE LOW-CONFIDENCE SAMPLES ==========\n",
    "CONFIDENCE_THRESH = 0.20\n",
    "\n",
    "pred_confidences = all_probs.max(axis=1)\n",
    "keep_mask = pred_confidences >= CONFIDENCE_THRESH\n",
    "\n",
    "# Filter arrays\n",
    "refined_embeddings = all_embeddings[keep_mask]\n",
    "refined_files = [all_files[i] for i, keep in enumerate(keep_mask) if keep]\n",
    "refined_cluster_labels = cluster_labels[keep_mask]\n",
    "refined_probs = all_probs[keep_mask]\n",
    "refined_preds = np.argmax(refined_probs, axis=1)\n",
    "print(f\"Retained {len(refined_files)}/{len(all_files)} images with confidence >= {CONFIDENCE_THRESH:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da419a71-0327-4527-b8bd-3bc66e7f2b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/10 with method random\n",
      "Inertia for init 1/10: 38619104.0\n",
      "Init 2/10 with method random\n",
      "Inertia for init 2/10: 40644380.0\n",
      "Init 3/10 with method random\n",
      "Inertia for init 3/10: 38730072.0\n",
      "Init 4/10 with method random\n",
      "Inertia for init 4/10: 39801012.0\n",
      "Init 5/10 with method random\n",
      "Inertia for init 5/10: 40222928.0\n",
      "Init 6/10 with method random\n",
      "Inertia for init 6/10: 39472344.0\n",
      "Init 7/10 with method random\n",
      "Inertia for init 7/10: 39328016.0\n",
      "Init 8/10 with method random\n",
      "Inertia for init 8/10: 39305020.0\n",
      "Init 9/10 with method random\n",
      "Inertia for init 9/10: 38297216.0\n",
      "Init 10/10 with method random\n",
      "Inertia for init 10/10: 38536700.0\n",
      "Minibatch step 1/144: mean batch inertia: 25759.5625\n",
      "Minibatch step 2/144: mean batch inertia: 16651.068359375, ewa inertia: 16651.068359375\n",
      "Minibatch step 3/144: mean batch inertia: 16800.64453125, ewa inertia: 16800.64453125\n",
      "Minibatch step 4/144: mean batch inertia: 16487.076171875, ewa inertia: 16487.076171875\n",
      "Minibatch step 5/144: mean batch inertia: 16046.20703125, ewa inertia: 16046.20703125\n",
      "Minibatch step 6/144: mean batch inertia: 16282.3046875, ewa inertia: 16282.3046875\n",
      "Minibatch step 7/144: mean batch inertia: 16406.94921875, ewa inertia: 16406.94921875\n",
      "Minibatch step 8/144: mean batch inertia: 15706.5625, ewa inertia: 15706.5625\n",
      "Minibatch step 9/144: mean batch inertia: 15252.13671875, ewa inertia: 15252.13671875\n",
      "Minibatch step 10/144: mean batch inertia: 15410.359375, ewa inertia: 15410.359375\n",
      "Minibatch step 11/144: mean batch inertia: 15858.57421875, ewa inertia: 15858.57421875\n",
      "Minibatch step 12/144: mean batch inertia: 15830.484375, ewa inertia: 15830.484375\n",
      "Minibatch step 13/144: mean batch inertia: 15455.8916015625, ewa inertia: 15455.8916015625\n",
      "Minibatch step 14/144: mean batch inertia: 15917.13671875, ewa inertia: 15917.13671875\n",
      "Minibatch step 15/144: mean batch inertia: 15815.69921875, ewa inertia: 15815.69921875\n",
      "Minibatch step 16/144: mean batch inertia: 16729.671875, ewa inertia: 16729.671875\n",
      "Minibatch step 17/144: mean batch inertia: 16012.962890625, ewa inertia: 16012.962890625\n",
      "Minibatch step 18/144: mean batch inertia: 15792.802734375, ewa inertia: 15792.802734375\n",
      "Minibatch step 19/144: mean batch inertia: 15790.1875, ewa inertia: 15790.1875\n",
      "Converged (lack of improvement in inertia) at step 19/144\n",
      "âœ… KMeans on PCA-reduced, confidence-filtered embeddings complete.\n"
     ]
    }
   ],
   "source": [
    "# ========== 16. KMEANS ON PCA-REDUCED EMBEDDINGS ==========\n",
    "PCA_DIM = 64\n",
    "pca = PCA(n_components=PCA_DIM, random_state=42)\n",
    "pca_embeds = pca.fit_transform(refined_embeddings)\n",
    "\n",
    "kmeans_pca = MiniBatchKMeans(\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    batch_size=1024,\n",
    "    max_iter=100,\n",
    "    n_init=CLUSTER_N_INIT,\n",
    "    init=\"random\",\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "refined_cluster_labels_pca = kmeans_pca.fit_predict(pca_embeds)\n",
    "np.save(os.path.join(SAVE_DIR, \"refined_pca_cluster_labels.npy\"), refined_cluster_labels_pca)\n",
    "print(\"âœ… KMeans on PCA-reduced, confidence-filtered embeddings complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3047ee82-cba2-4e5c-9014-8131055725d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary file written to ./V16_unsup_20250604_142640/run_summary.md\n"
     ]
    }
   ],
   "source": [
    "# ========== 17. SUMMARY/EXPORT/README ==========\n",
    "summary_path = os.path.join(SAVE_DIR, \"run_summary.md\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(f\"# V17_unsup Run Summary\\n\")\n",
    "    f.write(f\"Date: {datetime.datetime.now()}\\n\\n\")\n",
    "    f.write(f\"Model: {V13_MODEL_PATH}\\n\")\n",
    "    f.write(f\"Image Dir: {IMAGE_DIR}\\n\")\n",
    "    f.write(f\"Num Images: {len(all_files)}\\n\")\n",
    "    f.write(f\"Num Clusters: {N_CLUSTERS}\\n\")\n",
    "    f.write(f\"Confidence threshold: {CONFIDENCE_THRESH}\\n\")\n",
    "    f.write(f\"Kept after filtering: {len(refined_files)} images\\n\\n\")\n",
    "\n",
    "    # (Add confusion/purity stats here as you compute them)\n",
    "    f.write(\"## Cluster Sizes (after refinement):\\n\")\n",
    "    unique, counts = np.unique(refined_cluster_labels, return_counts=True)\n",
    "    for cid, sz in zip(unique, counts):\n",
    "        f.write(f\"- Cluster {cid}: {sz} images\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    # If you have cluster entropy/purity, include those stats as well.\n",
    "\n",
    "    f.write(\"\\n## Next Steps\\n- Review high-entropy clusters\\n- Use pure clusters for semi-supervised fine-tuning\\n\")\n",
    "print(f\"âœ… Summary file written to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "311995b2-3704-45b9-b1c6-a0ca5ca4fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved confidence histogram to ./V16_unsup_20250604_142640\n"
     ]
    }
   ],
   "source": [
    "# ========== 18. SAVE CONFIDENCE HISTOGRAM ==========\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(pred_confidences, bins=40, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"V13 max softmax confidence\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.title(\"V13 model confidence histogram\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, f\"{VERSION_TAG}_v13_confidence_histogram.png\"))\n",
    "plt.close()\n",
    "print(f\"âœ… Saved confidence histogram to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e27544a8-5f03-4011-9b82-a07af65f40e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved high/low confidence sample image grid.\n"
     ]
    }
   ],
   "source": [
    "# ========== 19. SHOW EXTREMES: HIGH/LOW CONFIDENCE IMAGE SAMPLES ==========\n",
    "def show_images_by_confidence(image_paths, confidences, top_n=5):\n",
    "    sorted_idx = np.argsort(confidences)\n",
    "    fig, axes = plt.subplots(2, top_n, figsize=(top_n*3, 6))\n",
    "    # Lowest confidence\n",
    "    for i in range(top_n):\n",
    "        img = Image.open(image_paths[sorted_idx[i]]).convert(\"RGB\")\n",
    "        axes[0, i].imshow(img)\n",
    "        axes[0, i].set_title(f\"Low conf: {confidences[sorted_idx[i]]:.2f}\")\n",
    "        axes[0, i].axis('off')\n",
    "    # Highest confidence\n",
    "    for i in range(top_n):\n",
    "        img = Image.open(image_paths[sorted_idx[-(i+1)]]).convert(\"RGB\")\n",
    "        axes[1, i].imshow(img)\n",
    "        axes[1, i].set_title(f\"High conf: {confidences[sorted_idx[-(i+1)]]:.2f}\")\n",
    "        axes[1, i].axis('off')\n",
    "    plt.suptitle(\"V13 confidence extremes (top: lowest, bottom: highest)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"{VERSION_TAG}_confidence_extremes.png\"))\n",
    "    plt.close()\n",
    "    print(\"âœ… Saved high/low confidence sample image grid.\")\n",
    "\n",
    "show_images_by_confidence(all_files, pred_confidences, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f724137-b611-421f-99e5-bb5c01a4ac33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cluster  size  mean_conf  median_conf  mean_entropy dominant_label  \\\n",
      "3         3   850   0.159202     0.159180      2.071729       surprise   \n",
      "1         1  2085   0.160253     0.160141      2.071323       surprise   \n",
      "19       19  2668   0.161803     0.161622      2.070526       surprise   \n",
      "2         2  2374   0.165737     0.165109      2.068169       surprise   \n",
      "17       17  1728   0.147811     0.147790      2.071930        sadness   \n",
      "14       14  2371   0.174562     0.172189      2.060905       surprise   \n",
      "9         9  1263   0.148008     0.147792      2.071489        sadness   \n",
      "11       11  1509   0.148457     0.147948      2.071152        sadness   \n",
      "4         4   790   0.165656     0.164423      2.052084    questioning   \n",
      "16       16   599   0.186200     0.185668      2.000031          anger   \n",
      "12       12  1579   0.144430     0.144077      2.072072          anger   \n",
      "6         6  2324   0.186145     0.187796      2.038842        sadness   \n",
      "15       15  1505   0.160233     0.157867      2.061882        disgust   \n",
      "18       18  1071   0.191801     0.193441      2.014013          anger   \n",
      "13       13   658   0.152247     0.151714      2.066794      happiness   \n",
      "10       10  1371   0.154716     0.150698      2.067508        sadness   \n",
      "7         7   984   0.191180     0.189769      1.996583          anger   \n",
      "8         8  2694   0.147725     0.145939      2.070908          anger   \n",
      "5         5  1055   0.159532     0.155875      2.062587          anger   \n",
      "0         0  1524   0.173731     0.171011      2.050959       surprise   \n",
      "\n",
      "      purity  \n",
      "3   1.000000  \n",
      "1   1.000000  \n",
      "19  0.999625  \n",
      "2   0.988627  \n",
      "17  0.968750  \n",
      "14  0.920709  \n",
      "9   0.882027  \n",
      "11  0.788602  \n",
      "4   0.786076  \n",
      "16  0.550918  \n",
      "12  0.531349  \n",
      "6   0.506024  \n",
      "15  0.491030  \n",
      "18  0.480859  \n",
      "13  0.466565  \n",
      "10  0.432531  \n",
      "7   0.425813  \n",
      "8   0.410171  \n",
      "5   0.330806  \n",
      "0   0.218504  \n",
      "Clusters flagged for curation (purity < 0.8):\n",
      "     cluster  size  mean_conf  median_conf  mean_entropy dominant_label  \\\n",
      "11       11  1509   0.148457     0.147948      2.071152        sadness   \n",
      "4         4   790   0.165656     0.164423      2.052084    questioning   \n",
      "16       16   599   0.186200     0.185668      2.000031          anger   \n",
      "12       12  1579   0.144430     0.144077      2.072072          anger   \n",
      "6         6  2324   0.186145     0.187796      2.038842        sadness   \n",
      "15       15  1505   0.160233     0.157867      2.061882        disgust   \n",
      "18       18  1071   0.191801     0.193441      2.014013          anger   \n",
      "13       13   658   0.152247     0.151714      2.066794      happiness   \n",
      "10       10  1371   0.154716     0.150698      2.067508        sadness   \n",
      "7         7   984   0.191180     0.189769      1.996583          anger   \n",
      "8         8  2694   0.147725     0.145939      2.070908          anger   \n",
      "5         5  1055   0.159532     0.155875      2.062587          anger   \n",
      "0         0  1524   0.173731     0.171011      2.050959       surprise   \n",
      "\n",
      "      purity  \n",
      "11  0.788602  \n",
      "4   0.786076  \n",
      "16  0.550918  \n",
      "12  0.531349  \n",
      "6   0.506024  \n",
      "15  0.491030  \n",
      "18  0.480859  \n",
      "13  0.466565  \n",
      "10  0.432531  \n",
      "7   0.425813  \n",
      "8   0.410171  \n",
      "5   0.330806  \n",
      "0   0.218504  \n",
      "Clusters flagged for curation (mean_conf < 0.4):\n",
      "     cluster  size  mean_conf  median_conf  mean_entropy dominant_label  \\\n",
      "3         3   850   0.159202     0.159180      2.071729       surprise   \n",
      "1         1  2085   0.160253     0.160141      2.071323       surprise   \n",
      "19       19  2668   0.161803     0.161622      2.070526       surprise   \n",
      "2         2  2374   0.165737     0.165109      2.068169       surprise   \n",
      "17       17  1728   0.147811     0.147790      2.071930        sadness   \n",
      "14       14  2371   0.174562     0.172189      2.060905       surprise   \n",
      "9         9  1263   0.148008     0.147792      2.071489        sadness   \n",
      "11       11  1509   0.148457     0.147948      2.071152        sadness   \n",
      "4         4   790   0.165656     0.164423      2.052084    questioning   \n",
      "16       16   599   0.186200     0.185668      2.000031          anger   \n",
      "12       12  1579   0.144430     0.144077      2.072072          anger   \n",
      "6         6  2324   0.186145     0.187796      2.038842        sadness   \n",
      "15       15  1505   0.160233     0.157867      2.061882        disgust   \n",
      "18       18  1071   0.191801     0.193441      2.014013          anger   \n",
      "13       13   658   0.152247     0.151714      2.066794      happiness   \n",
      "10       10  1371   0.154716     0.150698      2.067508        sadness   \n",
      "7         7   984   0.191180     0.189769      1.996583          anger   \n",
      "8         8  2694   0.147725     0.145939      2.070908          anger   \n",
      "5         5  1055   0.159532     0.155875      2.062587          anger   \n",
      "0         0  1524   0.173731     0.171011      2.050959       surprise   \n",
      "\n",
      "      purity  \n",
      "3   1.000000  \n",
      "1   1.000000  \n",
      "19  0.999625  \n",
      "2   0.988627  \n",
      "17  0.968750  \n",
      "14  0.920709  \n",
      "9   0.882027  \n",
      "11  0.788602  \n",
      "4   0.786076  \n",
      "16  0.550918  \n",
      "12  0.531349  \n",
      "6   0.506024  \n",
      "15  0.491030  \n",
      "18  0.480859  \n",
      "13  0.466565  \n",
      "10  0.432531  \n",
      "7   0.425813  \n",
      "8   0.410171  \n",
      "5   0.330806  \n",
      "0   0.218504  \n"
     ]
    }
   ],
   "source": [
    "# ========== 20. CLUSTER CONFIDENCE & ENTROPY DIAGNOSTICS ==========\n",
    "cluster_diag = []\n",
    "for c in range(N_CLUSTERS):\n",
    "    indices = np.where(cluster_labels == c)[0]\n",
    "    if len(indices) == 0: continue\n",
    "    confs = pred_confidences[indices]\n",
    "    entrs = entropy(all_probs[indices], axis=1)\n",
    "    preds = all_preds[indices]\n",
    "    dominant = np.argmax(np.bincount(preds))\n",
    "    dominant_label = LABEL_NAMES[dominant]\n",
    "    purity = (preds == dominant).sum() / len(preds)\n",
    "    cluster_diag.append({\n",
    "        \"cluster\": c,\n",
    "        \"size\": len(indices),\n",
    "        \"mean_conf\": np.mean(confs),\n",
    "        \"median_conf\": np.median(confs),\n",
    "        \"mean_entropy\": np.mean(entrs),\n",
    "        \"dominant_label\": dominant_label,\n",
    "        \"purity\": purity\n",
    "    })\n",
    "\n",
    "df_diag = pd.DataFrame(cluster_diag)\n",
    "df_diag.sort_values(\"purity\", ascending=False, inplace=True)\n",
    "df_diag.to_csv(os.path.join(SAVE_DIR, f\"{VERSION_TAG}_cluster_diagnostics.csv\"), index=False)\n",
    "print(df_diag)\n",
    "\n",
    "# Highlight clusters for curation if purity < 0.8 or mean_conf < 0.4\n",
    "low_purity = df_diag[df_diag.purity < 0.8]\n",
    "low_conf = df_diag[df_diag.mean_conf < 0.4]\n",
    "print(\"Clusters flagged for curation (purity < 0.8):\\n\", low_purity)\n",
    "print(\"Clusters flagged for curation (mean_conf < 0.4):\\n\", low_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3dcab2b-7d75-4922-aa04-909352c01a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported cluster 03 (surprise) to ./V16_unsup_20250604_142640/cluster_03_pure_surprise\n",
      "Exported cluster 01 (surprise) to ./V16_unsup_20250604_142640/cluster_01_pure_surprise\n",
      "Exported cluster 19 (surprise) to ./V16_unsup_20250604_142640/cluster_19_pure_surprise\n",
      "Exported cluster 02 (surprise) to ./V16_unsup_20250604_142640/cluster_02_pure_surprise\n",
      "Exported cluster 17 (sadness) to ./V16_unsup_20250604_142640/cluster_17_pure_sadness\n",
      "Exported cluster 14 (surprise) to ./V16_unsup_20250604_142640/cluster_14_pure_surprise\n"
     ]
    }
   ],
   "source": [
    "# ========== 21. EXPORT HIGH-PURITY CLUSTERS FOR MANUAL REVIEW/INJECTION ==========\n",
    "EXPORT_PURITY = 0.9  # Only clusters with at least 90% purity will be exported\n",
    "for _, row in df_diag.iterrows():\n",
    "    if row['purity'] >= EXPORT_PURITY and row['size'] > 10:\n",
    "        c = int(row['cluster'])\n",
    "        indices = np.where(cluster_labels == c)[0]\n",
    "        export_dir = os.path.join(SAVE_DIR, f\"cluster_{c:02d}_pure_{row['dominant_label']}\")\n",
    "        os.makedirs(export_dir, exist_ok=True)\n",
    "        for idx in indices:\n",
    "            src = all_files[idx]\n",
    "            dst = os.path.join(export_dir, os.path.basename(src))\n",
    "            try:\n",
    "                os.symlink(src, dst)\n",
    "            except FileExistsError:\n",
    "                continue\n",
    "            except OSError:\n",
    "                import shutil\n",
    "                shutil.copy(src, dst)\n",
    "        print(f\"Exported cluster {c:02d} ({row['dominant_label']}) to {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f42670c8-919c-4363-89b8-6ed13029265a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved per-cluster confidence plot to ./V16_unsup_20250604_142640\n"
     ]
    }
   ],
   "source": [
    "# ========== 22. CLUSTER-WISE CONFIDENCE SUMMARY AND PLOT ==========\n",
    "# (included in diagnostics but as a stand-alone plot)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(df_diag['cluster'], df_diag['mean_conf'])\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Mean V13 Confidence\")\n",
    "plt.title(\"Mean Confidence per Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, f\"{VERSION_TAG}_mean_conf_per_cluster.png\"))\n",
    "plt.close()\n",
    "print(f\"âœ… Saved per-cluster confidence plot to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e703240-1f62-42e0-81a6-ccb0e0b3d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 00: exported 20 highest-confidence faces\n",
      "Cluster 01: exported 20 highest-confidence faces\n",
      "Cluster 02: exported 20 highest-confidence faces\n",
      "Cluster 03: exported 20 highest-confidence faces\n",
      "Cluster 04: exported 20 highest-confidence faces\n",
      "Cluster 05: exported 20 highest-confidence faces\n",
      "Cluster 06: exported 20 highest-confidence faces\n",
      "Cluster 07: exported 20 highest-confidence faces\n",
      "Cluster 08: exported 20 highest-confidence faces\n",
      "Cluster 09: exported 20 highest-confidence faces\n",
      "Cluster 10: exported 20 highest-confidence faces\n",
      "Cluster 11: exported 20 highest-confidence faces\n",
      "Cluster 12: exported 20 highest-confidence faces\n",
      "Cluster 13: exported 20 highest-confidence faces\n",
      "Cluster 14: exported 20 highest-confidence faces\n",
      "Cluster 15: exported 20 highest-confidence faces\n",
      "Cluster 16: exported 20 highest-confidence faces\n",
      "Cluster 17: exported 20 highest-confidence faces\n",
      "Cluster 18: exported 20 highest-confidence faces\n",
      "Cluster 19: exported 20 highest-confidence faces\n",
      "âœ… Cluster purification complete (see *_top_conf folders in ./V16_unsup_20250604_142640)\n"
     ]
    }
   ],
   "source": [
    "# ========== 23. CLUSTER PURIFICATION: EXPORT TOP-N HIGH-CONFIDENCE FACES PER CLUSTER ==========\n",
    "N_TOP = 20  # how many high-confidence faces per cluster to copy/symlink\n",
    "\n",
    "for c in range(N_CLUSTERS):\n",
    "    indices = np.where(cluster_labels == c)[0]\n",
    "    if len(indices) == 0:\n",
    "        continue\n",
    "    confs = pred_confidences[indices]\n",
    "    sorted_idx = indices[np.argsort(confs)[::-1]]  # Descending\n",
    "    chosen = sorted_idx[:N_TOP]\n",
    "    cluster_dir = os.path.join(SAVE_DIR, f\"cluster_{c:02d}_top_conf\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "    for idx in chosen:\n",
    "        src = all_files[idx]\n",
    "        dst = os.path.join(cluster_dir, os.path.basename(src))\n",
    "        try:\n",
    "            os.symlink(src, dst)\n",
    "        except FileExistsError:\n",
    "            continue\n",
    "        except OSError:\n",
    "            import shutil\n",
    "            shutil.copy(src, dst)\n",
    "    print(f\"Cluster {c:02d}: exported {len(chosen)} highest-confidence faces\")\n",
    "print(f\"âœ… Cluster purification complete (see *_top_conf folders in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c68da1b-e028-4691-8032-734d95236528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 24. OPTIONAL: TEMPERATURE SCALING FOR CALIBRATION ==========\n",
    "# (Requires a clean, labeled validation set with logits/labels)\n",
    "def temperature_scale(logits, labels):\n",
    "    temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    optimizer = optim.LBFGS([temperature], lr=0.01, max_iter=50)\n",
    "    nll_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def eval():\n",
    "        optimizer.zero_grad()\n",
    "        loss = nll_criterion(logits / temperature, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval)\n",
    "    return temperature.item()\n",
    "\n",
    "# Example usage (with val_logits, val_labels as tensors):\n",
    "# T_opt = temperature_scale(val_logits, val_labels)\n",
    "# scaled_probs = torch.softmax(torch.tensor(all_logits) / T_opt, dim=1).numpy()\n",
    "# Use scaled_probs for confidence diagnostics if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73382abf-cf7a-466b-bb67-fdc1e6e48011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary file written to ./V16_unsup_20250604_142640/run_summary.md\n"
     ]
    }
   ],
   "source": [
    "# ========== 25. SUMMARY/EXPORT/README (ALREADY IN YOUR PIPELINE, RECAP) ==========\n",
    "summary_path = os.path.join(SAVE_DIR, \"run_summary.md\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(f\"# V17_unsup Run Summary\\n\")\n",
    "    f.write(f\"Date: {datetime.datetime.now()}\\n\\n\")\n",
    "    f.write(f\"Model: {V13_MODEL_PATH}\\n\")\n",
    "    f.write(f\"Image Dir: {IMAGE_DIR}\\n\")\n",
    "    f.write(f\"Num Images: {len(all_files)}\\n\")\n",
    "    f.write(f\"Num Clusters: {N_CLUSTERS}\\n\")\n",
    "    f.write(f\"Confidence threshold: {CONFIDENCE_THRESH}\\n\")\n",
    "    f.write(f\"Kept after filtering: {len(refined_files)} images\\n\\n\")\n",
    "\n",
    "    # (Add confusion/purity stats here as you compute them)\n",
    "    f.write(\"## Cluster Sizes (after refinement):\\n\")\n",
    "    unique, counts = np.unique(refined_cluster_labels, return_counts=True)\n",
    "    for cid, sz in zip(unique, counts):\n",
    "        f.write(f\"- Cluster {cid}: {sz} images\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    # If you have cluster entropy/purity, include those stats as well.\n",
    "\n",
    "    f.write(\"\\n## Next Steps\\n- Review high-entropy clusters\\n- Use pure clusters for semi-supervised fine-tuning\\n\")\n",
    "print(f\"âœ… Summary file written to {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8b706-5b24-4179-862e-ba147d8155ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_unsupervised)",
   "language": "python",
   "name": "ml_unsupervised"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
