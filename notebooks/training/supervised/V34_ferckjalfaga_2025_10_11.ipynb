{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  V34  -  Verified full training + inference pipeline\n",
    "#  Summary: merged V32 artifact structure, added RandomErasing + calibration RGB normalization\n",
    "# ==============================================================================\n",
    "\n",
    "# V33 to V34 changes:\n",
    "    # overview: Elevate Stage 1 to deployment readiness while preserving Stage 2 gains.\n",
    "    #   Adds relevant-only focal CE, asymmetric augmentation, calibration + œÑ tuning,\n",
    "    #   and restores V32 diagnostic artifacts (review folders, mining, shortlist,\n",
    "    #   curated patch). Introduces optional QE/XAI utilities and ablation scaffolding.\n",
    "\n",
    "    # section #4 (Stage 1 loss):\n",
    "    #   - Added RelevantFocalCrossEntropy (Œ≥=2.0) that applies focal scaling only for\n",
    "    #     the 'relevant' class (uses existing class_weights_s1 internally).\n",
    "    #   - Rationale: Increase recall on difficult positives without inflating easy FPs.\n",
    "    #   - Expected Impact: F1(relevant) ‚Üë, Recall(relevant) ‚Üë, Calibration ‚Üë.\n",
    "\n",
    "    # section #4 (Stage 1 augmentation):\n",
    "    #   - Applied stronger augmentation ONLY to 'relevant' via augment_map_s1.\n",
    "    #   - Rationale: Expand boundary coverage for lighting/occlusion/perspective.\n",
    "    #   - Expected Impact: Recall(relevant) ‚Üë 5‚Äì10pp, stable precision.\n",
    "\n",
    "    # section #4 (Stage 1 calibration):\n",
    "    #   - Implemented temperature scaling (fit on eval) + œÑ sweep (0.30‚Äì0.55).\n",
    "    #   - Persisted stage1_calibration.json {T, œÑ} for hierarchical inference use.\n",
    "    #   - Expected Impact: Better-calibrated probabilities; optimal œÑ selection -> F1 ‚Üë.\n",
    "    # ‚Äúhierarchical_predict(...) now uses stage1_calibration.json (T and œÑ) \n",
    "        # to gate relevance during inference.‚Äù\n",
    "\n",
    "    # section #4 (Collator & Aug fix):\n",
    "    #   - Removed PIL-level RandomErasing from strong_pos_aug; applied RandomErasing\n",
    "    #     only at the tensor stage inside DataCollatorWithAugmentation to prevent\n",
    "    #     AttributeError on PIL images and keep occlusion realism.\n",
    "    #   - Defined collator helpers (ToTensor/ToPILImage) and fixed attribute mismatch:\n",
    "    #     now uses `self.random_erasing` (configurable) instead of undefined\n",
    "    #     `self.post_tensor_erase`.\n",
    "    #   - Expected Impact: Stable training with correct augmentation; no PIL/tensor\n",
    "    #     shape errors; consistent occlusion regularization.\n",
    "\n",
    "    # section #4 (Calibration/Eval robustness ‚Äì update):\n",
    "    #   - Added _ensure_rgb() and applied it inside both S1 calibration loops\n",
    "    #     to convert all eval images to 3-channel RGB (expand grayscale arrays),\n",
    "    #     preventing ViTImageProcessor error on 2-D images.\n",
    "    #   - Harmonized division by temperature to logits / max(T, 1e-3) for numeric stability.\n",
    "    #   - Expected Impact: Stable S1 calibration/œÑ-sweep on heterogeneous eval data.\n",
    "\n",
    "    # section #5 (Stage 2 micro-tweaks):\n",
    "    #   - Lowered focal_gamma from 1.5 ‚Üí 1.2 for ['sadness','speech_action'].\n",
    "    #   - (Optional) Mild aug added for 'neutral_speech' only.\n",
    "    #   - Expected Impact: +1‚Äì2pp macro-F1; improved calibration on fragile classes.\n",
    "\n",
    "    # section #5 (Inference safety):\n",
    "    #   - In hierarchical_predict, ensured device/dtype-safe `torch.where` for S1\n",
    "    #     gating (fill tensors now created on `logits_s1.device`).\n",
    "    #   - Expected Impact: Avoids device mismatch when switching CPU‚ÜîGPU in future runs.\n",
    "\n",
    "    # section #6 - Aug pipeline wiring:\n",
    "        #   ‚ÄúWired Stage-2 augment_dict to include neutral_speech mild aug and \n",
    "        # pass the merged augment_dict to the collator.‚Äù\n",
    "\n",
    "    # section #7 (Artifacts restoration & merge):\n",
    "    #   - Restored V32-style artifacts for V34:\n",
    "    #       * V34_full_inference_log.csv\n",
    "    #       * review_candidates_by_predicted_class/ (CONF_THRESHOLD=0.85)\n",
    "    #       * hard_negatives_*_vs_*.csv (pair mining)\n",
    "    #       * curation_shortlist_V34.csv + curated_additions_V34.csv\n",
    "    #   - Merged overlapping files with V32:\n",
    "    #       * curation_shortlist_merged.csv\n",
    "    #       * curated_additions_merged.csv\n",
    "    #   - Rationale: Full diagnostics parity + reproducibility across versions.\n",
    "    #   - Mining pairs now prefer THIS run‚Äôs full log first (fall back to prior runs only if missing).\n",
    "    #   - Added curation_shortlist_{VXX}.csv and curated_additions_{VXX}.csv emission in post-training flow.\n",
    "    #   - Added canonical merges:\n",
    "    #       * curation_shortlist_merged.csv (V32 + current)\n",
    "    #       * curated_additions_merged.csv (V32 + current)\n",
    "    #   - Rationale: Full parity with V32 artifacts while keeping continuity across versions.\n",
    "    #   - Expected Impact: Faster, targeted curation and stable longitudinal review.\n",
    "\n",
    "\n",
    "    # section #8 (Optional analysis utilities):\n",
    "    #   - Added QE (qualitative error bucketing) helper.\n",
    "    #   - Added S1 ablation summary helper for clean experiment tracking.\n",
    "    #   - Expected Impact: Data collection guided by dominant failure modes; faster iteration.\n",
    "\n",
    "    # section #10 - Operational defaults:\n",
    "    #   Set RUN_INFERENCE=True \n",
    "\n",
    "    # Follow-Up Actions:\n",
    "    #   - Run V34a/b/c ablations (loss-only; aug-only; curated-only), then V34d with best combo.\n",
    "    #   - Confirm S1 deployment gate: F1(relevant) ‚â• 0.80, Recall ‚â• 0.85, Precision ‚â• 0.70.\n",
    "    #   - Rebuild merged artifacts and refresh curation stream for next cycle.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageStat, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamically loading latest checkpoint: V33_20251010_122749\n",
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V34_20251013_211825\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- üìÇ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- ‚öôÔ∏è Run Configuration ---\n",
    "# default safer for daily dev runs; flip to True when you want full-corpus inference\n",
    "RUN_INFERENCE = True\n",
    "# default safer; run once when dataset layout changes\n",
    "PREPARE_DATASETS = False\n",
    "\n",
    "# Finds the most recent V* model directory based on modification time.\n",
    "def find_latest_checkpoint(root_dir):\n",
    "    all_run_dirs = [\n",
    "        os.path.join(root_dir, d)\n",
    "        for d in os.listdir(root_dir)\n",
    "        if d.startswith(\"V\") and os.path.isdir(os.path.join(root_dir, d))\n",
    "    ]\n",
    "    if not all_run_dirs:\n",
    "        return None\n",
    "\n",
    "    # Sort directories by modification time, newest first\n",
    "    sorted_dirs = sorted(all_run_dirs, key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    # The newest directory is the current run's empty folder.\n",
    "    # We need the second newest, which is the latest *completed* run.\n",
    "    if len(sorted_dirs) > 1:\n",
    "        return sorted_dirs[1] # <-- Return the second item in the list\n",
    "    else:\n",
    "        # If there's only one (or zero), no previous checkpoint exists\n",
    "        return None\n",
    "\n",
    "# --- ü§ñ Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Dynamically find the latest checkpoint to train from\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = latest_checkpoint\n",
    "    print(f\"‚úÖ Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    # If no checkpoint is found, fall back to the base model from Hugging Face\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(f\"‚ö†Ô∏è No local checkpoint found. Starting from base model: {BASE_MODEL_NAME}\")\n",
    "    \n",
    "# --- üè∑Ô∏è Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# single source of truth for review gating\n",
    "REVIEW_CONF_THRESHOLD = 0.85  \n",
    "\n",
    "# --- üñºÔ∏è File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- üî¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"üóÇÔ∏è Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\n‚úÖ Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# üì¶ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self,\n",
    "                 processor,\n",
    "                 augment_dict=None,\n",
    "                 base_augment=None,\n",
    "                 # --- NEW: tensor-level erasing controls (applied after processor) ---\n",
    "                 random_erasing_prob: float = 0.10,\n",
    "                 random_erasing_scale = (0.02, 0.08),\n",
    "                 skip_erasing_label_ids=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processor: HF image processor that yields pixel_value tensors\n",
    "            augment_dict: dict[int label_id -> PIL transform], class-specific\n",
    "            base_augment: fallback PIL transform when class-specific not found\n",
    "            random_erasing_prob: probability for applying tensor-level RandomErasing\n",
    "            random_erasing_scale: area range for erasing region\n",
    "            skip_erasing_label_ids: iterable of label ids to skip erasing for\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict or {}\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = base_augment or T.Compose([T.Resize((224, 224))])\n",
    "\n",
    "        # --- NEW: tensor-level RandomErasing (applied AFTER processor) ---\n",
    "        # Keep None to disable; expects CHW tensors in [0,1]\n",
    "        self.random_erasing = (\n",
    "            T.RandomErasing(p=random_erasing_prob, scale=random_erasing_scale, value=\"random\")\n",
    "            if random_erasing_prob and random_erasing_prob > 0.0 else None\n",
    "        )\n",
    "                \n",
    "        # --- NEW: define tensor <-> PIL helpers used in __call__ ---\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.to_pil = T.ToPILImage()\n",
    "        \n",
    "        # Labels to skip erasing for (can be overridden when constructing the collator)\n",
    "        self.skip_erasing_label_ids = set()  # e.g., {label2id_s2['sadness'], ...}\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "\n",
    "            # 1) apply class-specific PIL pipeline if present; else base PIL pipeline\n",
    "            pil_aug = self.augment_dict.get(label, self.base_augment)\n",
    "\n",
    "            img = pil_aug(rgb_image)\n",
    "\n",
    "            # ‚¨áÔ∏è INSERT THE NEW LINES HERE\n",
    "            # --- Tensor-level RandomErasing ---\n",
    "            img_t = self.to_tensor(img)                 # PIL ‚Üí Tensor [C,H,W]\n",
    "            if self.random_erasing is not None and label not in self.skip_erasing_label_ids:\n",
    "                img_t = self.random_erasing(img_t)      # RandomErasing on tensor\n",
    "            img = self.to_pil(img_t)  \n",
    "        \n",
    "            processed_images.append(img)\n",
    "\n",
    "        batch = self.processor(images=processed_images, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- normalize any image-like object to 3-channel RGB (PIL) ---\n",
    "def _ensure_rgb(img):\n",
    "    # If already PIL, force RGB mode\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert(\"RGB\")\n",
    "    # Else coerce to array and expand grayscale to 3 channels\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr, arr, arr], axis=-1)\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# üèãÔ∏è Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# üîÑ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None, focal_gamma=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.focal_gamma = focal_gamma  # NEW (None disables focal scaling)\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        ce_per_sample = -(smooth_labels * log_probs).sum(dim=1)\n",
    "\n",
    "        # NEW: optional focal scaling\n",
    "        if self.focal_gamma is not None and self.focal_gamma > 0:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pt = (probs * smooth_labels).sum(dim=1).clamp_min(1e-6)\n",
    "            ce_per_sample = ((1 - pt) ** self.focal_gamma) * ce_per_sample\n",
    "\n",
    "        return ce_per_sample.mean()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1 loss function: focal-modulated cross-entropy (relevant-only)\n",
    "#   - We keep class weights for imbalance handling.\n",
    "#   - We add focal modulation ONLY when the ground truth is \"relevant\"\n",
    "#     to emphasize difficult positives without exploding FP on easy negatives.\n",
    "# ------------------------------------------------------------------------------\n",
    "class RelevantFocalCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, class_weights: torch.Tensor, gamma: float = 2.0, relevant_id: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            class_weights: Tensor of per-class weights (size 2 for S1)\n",
    "            gamma: focal exponent (higher -> more emphasis on hard examples)\n",
    "            relevant_id: integer id for the 'relevant' class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=class_weights, reduction=\"none\")\n",
    "        self.gamma = gamma\n",
    "        self.relevant_id = relevant_id\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes cross-entropy per-sample, then applies focal scaling only\n",
    "        for samples whose target == 'relevant'. Non-relevant samples keep vanilla CE.\n",
    "        \"\"\"\n",
    "        # base cross-entropy (per-sample)\n",
    "        ce = self.ce(logits, targets)  # shape: [B]\n",
    "\n",
    "        # compute p_t = softmax(logits)[range(B), targets]\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            p_t = probs[torch.arange(probs.size(0)), targets]  # [B]\n",
    "\n",
    "        # mask: 1 for relevant targets, 0 otherwise\n",
    "        mask = (targets == self.relevant_id).float()\n",
    "\n",
    "        # focal factor: (1 - p_t)^gamma for relevant samples; 1.0 for others\n",
    "        focal = (1.0 - p_t).pow(self.gamma) * mask + (1.0 - mask)\n",
    "\n",
    "        # mean reduced loss\n",
    "        return (focal * ce).mean()\n",
    "\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# üìä Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(eval_pred, label_names, stage_name=\"\"):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(f\"\\nüìà Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw logits/labels for later analysis like temperature scaling\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in label_names]\n",
    "    recalls = [report[name][\"recall\"] for name in label_names]\n",
    "    precisions = [report[name][\"precision\"] for name in label_names]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    \n",
    "    # Create a dictionary for entropies in the correct order for the CSV\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    # Access the trainer instance through its global-like availability during compute_metrics call\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names}\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "    # --- End Re-integration ---\n",
    "\n",
    "    # Generate and save a heatmap of the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((label_names[i], label_names[j]), cm[i][j])\n",
    "        for i in range(len(label_names))\n",
    "        for j in range(len(label_names)) if i != j and cm[i][j] > 0\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute and print entropy metrics\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nüîç Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "    # --- End Re-integration ---\n",
    "    \n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "#   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "#   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "#   - Persist T and œÑ for hierarchical inference.\n",
    "# ------------------------------------------------------------------------------\n",
    "def fit_temperature(model, eval_ds, processor, device):\n",
    "    \"\"\"\n",
    "    Fits a single temperature scalar T by minimizing NLL on eval set.\n",
    "    Returns:\n",
    "        float: learned temperature T (>= ~1e-3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        #Normalize every eval image to 3-channel RGB in fit_temperature\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "        \n",
    "            # --- Ensure 3-channel RGB for the processor ---\n",
    "            # If PIL: convert directly; if numpy/other: coerce to array and expand gray to 3-channels\n",
    "            if isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "            else:\n",
    "                arr = np.array(img)\n",
    "                if arr.ndim == 2:                      # grayscale -> stack to RGB\n",
    "                    arr = np.stack([arr, arr, arr], axis=-1)\n",
    "                img = Image.fromarray(arr.astype(np.uint8))  # ensure PIL RGB\n",
    "        \n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            logits_list.append(logits.cpu())\n",
    "            labels_list.append(lab)\n",
    "\n",
    "    logits = torch.cat(logits_list, dim=0)  # [N, 2]\n",
    "    labels = torch.tensor(labels_list)\n",
    "\n",
    "    T = torch.nn.Parameter(torch.ones(1))\n",
    "    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def _closure():\n",
    "        \"\"\"\n",
    "        LBFGS closure for temperature scaling:\n",
    "        Scales logits by 1/T, computes CE loss, backprops to adjust T.\n",
    "        \"\"\"\n",
    "        opt.zero_grad()\n",
    "        scaled = logits / T.clamp(min=1e-3)\n",
    "        loss = ce(scaled, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    opt.step(_closure)\n",
    "    return float(T.data.item())\n",
    "\n",
    "def sweep_tau(model, eval_ds, processor, device, T=1.0):\n",
    "    \"\"\"\n",
    "    Sweeps œÑ (threshold on P(relevant)) over [0.30, 0.55] to maximize F1(relevant).\n",
    "    Returns:\n",
    "        dict: {'tau', 'f1', 'prec', 'rec'} with 3-decimal rounding for logging.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "    \n",
    "            # Normalize to 3-channel RGB to avoid ndim==2 errors\n",
    "            img = _ensure_rgb(img)\n",
    "    \n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits / max(T, 1e-3)  # robustness vs tiny T\n",
    "            prob_rel = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(prob_rel)\n",
    "    \n",
    "    y_true = np.array(y_true, dtype=bool)\n",
    "    y_prob = np.array(y_prob, dtype=float)\n",
    "\n",
    "\n",
    "    best = {\"tau\": None, \"f1\": -1.0, \"prec\": None, \"rec\": None}\n",
    "    for tau in np.linspace(0.30, 0.55, 26):\n",
    "        pred = (y_prob >= tau)\n",
    "        tp = ((pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((pred == 1) & (y_true == 0)).sum()\n",
    "        fn = ((pred == 0) & (y_true == 1)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"tau\": round(float(tau), 3),\n",
    "                    \"f1\": round(float(f1), 3),\n",
    "                    \"prec\": round(float(prec), 3),\n",
    "                    \"rec\": round(float(rec), 3)}\n",
    "    return best\n",
    "    \n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# üíæ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"üíæ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"‚úÖ {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "# ==========================================================================\n",
    "#   POST-TRAINING ANALYSIS UTILITIES (OFFLINE / OPTIONAL)\n",
    "#   - Qualitative error bucketing (QE)\n",
    "#   - Attention rollout (XAI) for S1 inspection\n",
    "#   - Ablation helpers\n",
    "# ==========================================================================\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"‚ö†Ô∏è Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - ‚ùå {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - ‚úÖ {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")\n",
    "\n",
    "# --- Qualitative Error Bucketing (Stage 1) ---\n",
    "# Scans an inference CSV and tags each row with simple visual heuristics:\n",
    "# blur/shadow/occlusion/low-res. Outputs a QE report CSV for targeting data fixes.\n",
    "def variance_of_laplacian(gray):\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def is_dark(img_pil, thresh=40):\n",
    "    stat = ImageStat.Stat(img_pil.convert(\"L\"))\n",
    "    return stat.mean[0] < thresh\n",
    "\n",
    "def qualitative_buckets_s1(inference_csv, out_csv):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(inference_csv)\n",
    "    # consider only S1 mistakes if you logged them; otherwise filter low conf or S2 mismatches\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        path = r['filepath']\n",
    "        if not os.path.exists(path): continue\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        arr = np.array(img)\n",
    "        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        blur = variance_of_laplacian(gray) < 60         # motion blur proxy\n",
    "        dark = is_dark(img, thresh=45)                  # shadows proxy\n",
    "        lowres = min(img.size) < 80\n",
    "        # Cheap occlusion proxy: large random erasing candidate on face area would help, but without faces we use entropy\n",
    "        ent = cv2.calcHist([gray],[0],None,[256],[0,256]).flatten()\n",
    "        ent = -np.sum((ent/ent.sum()+1e-9)*np.log2(ent/ent.sum()+1e-9))\n",
    "        occl = ent < 4.5                                 # low entropy proxy\n",
    "        rows.append([path, r.get('true_label','?'), r.get('predicted_label','?'), r.get('confidence',np.nan),\n",
    "                     int(blur), int(dark), int(occl), int(lowres)])\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"filepath\",\"true\",\"pred\",\"conf\",\"blur\",\"shadow\",\"occlusion\",\"lowres\"])\n",
    "        w.writerows(rows)\n",
    "    return out_csv\n",
    "\n",
    "# --- Ablation summary utility for Stage 1 ---\n",
    "# Summarizes precision/recall/F1 for S1 given (T, tau).\n",
    "def summarize_s1(eval_ds, model, processor, device, T: float, tau: float):\n",
    "    import numpy as np\n",
    "    y_true, y_prob = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "    \n",
    "            # Normalize to 3-channel RGB to avoid ndim==2 errors\n",
    "            img = _ensure_rgb(img)\n",
    "    \n",
    "            logits = model(**processor(images=img, return_tensors=\"pt\").to(device)).logits\n",
    "            logits = logits / max(T, 1e-3)\n",
    "            p = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(p)\n",
    "\n",
    "    y_true = np.array(y_true, bool); y_prob = np.array(y_prob, float)\n",
    "    pred = (y_prob >= tau)\n",
    "    tp = ((pred==1)&(y_true==1)).sum(); fp=((pred==1)&(y_true==0)).sum(); fn=((pred==0)&(y_true==1)).sum()\n",
    "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
    "    return {\"precision\":round(prec,3), \"recall\":round(rec,3), \"f1\":round(f1,3), \"tau\":tau, \"T\":T}\n",
    "\n",
    "\n",
    "# --- Attention Rollout heatmaps for ViT (offline) ---\n",
    "def vit_attention_rollout(model, inputs, discard_ratio=0.9):\n",
    "    # returns a [H,W] mask normalized 0..1; you can overlay it\n",
    "    # (Implementation omitted for brevity; use a standard attention-rollout snippet for ViT)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # --- Define specific model paths from the latest checkpoint ---\n",
    "    s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "    s2_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"emotion_classifier_model\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"‚úÖ Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        s1_checkpoint_path, # <-- Use the specific path for the Stage 1 model\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"‚öñÔ∏è Stage 1 Class Weights: {class_weights_s1}\")\n",
    "\n",
    "    # --- Define Early Stopping ---\n",
    "    # Stops training if validation loss doesn't improve for 2 consecutive epochs\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    # The complex discriminative learning rate and layer freezing strategy in \n",
    "        # V31 caused a severe performance drop. This change reverts Stage 1 to \n",
    "        # V30's simpler and more effective approach of using a single, uniform \n",
    "        # learning rate for the entire model, which is managed by the Hugging \n",
    "        # Face Trainer's default optimizer.\n",
    "    training_args_s1.learning_rate = 3e-5 # Set learning rate directly\n",
    "    \n",
    "    loss_fct_s1 = RelevantFocalCrossEntropy(\n",
    "        class_weights=class_weights_s1,   # <-- we KEEP and USE class_weights here\n",
    "        gamma=2.0,\n",
    "        relevant_id=label2id_s1['relevant']\n",
    "    )\n",
    "\n",
    "    strong_pos_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.8, 1.0)),     # encourage scale & crop robustness\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.2, 0.2, 0.2, 0.05),             # moderate color/lighting jitter\n",
    "        T.RandomPerspective(distortion_scale=0.05, p=0.2),\n",
    "    ])\n",
    "\n",
    "    # Map label id -> transform. 1 == relevant\n",
    "    augment_map_s1 = { label2id_s1['relevant']: strong_pos_aug }\n",
    "\n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    # Apply stronger augmentation ONLY to the \"relevant\" class to expand coverage\n",
    "        # near the decision boundary (lighting, small occlusions, slight perspective).\n",
    "        # Keep \"irrelevant\" mild as before to avoid over-creating near-face artifacts.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=list(id2label_s1.values()), \n",
    "                                stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(\n",
    "            processor=processor,\n",
    "            augment_dict=augment_map_s1,                # your existing class‚ÜíPIL map\n",
    "            random_erasing_prob=0.10,                   # enable erasing\n",
    "            random_erasing_scale=(0.02, 0.08),\n",
    "            skip_erasing_label_ids=[]                   # or [label2id_s1['relevant']] to skip\n",
    "        ),\n",
    "        loss_fct=loss_fct_s1,         # <-- NEW: custom loss uses class_weights + focal on relevant\n",
    "        callbacks=[early_stop_callback]\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"üöÄ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"‚åõ Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\n‚úÖ Stage 1 Training Complete.\")\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "    #   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "    #   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "    #   - Persist T and œÑ for hierarchical inference.\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\nüß™ Calibrating Stage 1...\")\n",
    "    T_s1 = fit_temperature(trainer_s1.model, eval_dataset_s1, processor, device)\n",
    "    best_s1 = sweep_tau(trainer_s1.model, eval_dataset_s1, processor, device, T=T_s1)\n",
    "    print(f\"‚úÖ S1 calibration done: T={T_s1:.3f} | best œÑ={best_s1['tau']} | F1={best_s1['f1']} (P={best_s1['prec']}, R={best_s1['rec']})\")\n",
    "    \n",
    "    # Persist calibration for inference\n",
    "    with open(os.path.join(SAVE_DIR, \"stage1_calibration.json\"), \"w\") as f:\n",
    "        json.dump({\"T\": T_s1, \"tau\": best_s1[\"tau\"]}, f)\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(train_dataset_s2['label']))\n",
    "\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        s2_checkpoint_path, # <-- Use the specific path for the Stage 2 model\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=11),  # was 9\n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "\n",
    "    # The addition of 'sadness' and 'speech_action' to the heavy augmentation pipeline in V31 \n",
    "        # was counterproductive, causing the F1-scores for these classes to collapse. \n",
    "        # This change reverts the list to the V30 definition, removing the aggressive \n",
    "        # augmentation from the classes it harmed.\n",
    "    minority_classes_s2 = [label2id_s2[name] for name in ['disgust', 'questioning', 'contempt', 'fear']]\n",
    "    minority_augment_map_s2 = {label_id: minority_aug for label_id in minority_classes_s2}\n",
    "\n",
    "    # NEW: very mild, targeted aug ONLY for the weakest classes (no RandAugment)\n",
    "    mild_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.05, 0.05, 0.05, 0.02),\n",
    "        T.RandomPerspective(distortion_scale=0.05, p=0.3),\n",
    "    ])\n",
    "\n",
    "    # --- NEW: targeted mild augmentation for fragile classes\n",
    "    #     - Keep 'sadness' and 'speech_action' on very mild pipeline (no RandAug)\n",
    "    #     - Extend to 'neutral_speech' to preserve subtle mouth/phoneme cues\n",
    "    targeted_mild_classes = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['speech_action'],\n",
    "        label2id_s2['neutral_speech']   # <-- NEW\n",
    "    ]\n",
    "    targeted_mild_map_s2 = {label_id: mild_aug for label_id in targeted_mild_classes}\n",
    "\n",
    "    # MERGE: single mapping passed to the collator (class id -> transform)\n",
    "    augment_dict = {**minority_augment_map_s2, **targeted_mild_map_s2}\n",
    "\n",
    "    # Use the custom loss function to turn off label smoothing for historically difficult classes.\n",
    "        # Turn OFF smoothing for the hardest classes (sharper targets) and apply mild focal emphasis\n",
    "        # Stage 2 loss: slightly softer focal gamma for fragile classes\n",
    "        # Reduces over-focus; improves probability calibration a bit.\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,\n",
    "        target_class_names=['sadness', 'speech_action'],\n",
    "        label2id_map=label2id_s2,\n",
    "        focal_gamma=1.2   # <-- slightly softer focal\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # Adding weight decay, cosine scheduler + warmup, grad accumulation improves stability \n",
    "        # (especially on CPU/small batch) without altering your high-level flow.\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=6,                       # +1 epoch for minorities\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "        weight_decay=0.05,                        # NEW\n",
    "        lr_scheduler_type=\"cosine\",               # NEW\n",
    "        warmup_ratio=0.10,                        # NEW\n",
    "        gradient_accumulation_steps=2,            # NEW\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # As with Stage 1, the complex fine-tuning strategy implemented in V31 failed. \n",
    "        # This change reverts the Stage 2 training process to V30's more effective \n",
    "        # uniform learning rate strategy to restore model performance.\n",
    "    training_args_s2.learning_rate = 4e-5 # Set learning rate directly\n",
    "\n",
    "    # skip erasing for fragile classes like sadness and neutral_speech\n",
    "    fragile_ids = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['neutral_speech']\n",
    "    ]\n",
    "    # Use the CustomLossTrainer again, passing the targeted loss function.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=RELEVANT_CLASSES, \n",
    "                                stage_name=\"Stage2\"),\n",
    "        data_collator=DataCollatorWithAugmentation(\n",
    "            processor=processor,\n",
    "            augment_dict=augment_dict,                  # your merged S2 map\n",
    "            random_erasing_prob=0.10,\n",
    "            random_erasing_scale=(0.02, 0.08),\n",
    "            skip_erasing_label_ids=fragile_ids          # <-- skip erasing for fragile classes\n",
    "        ),\n",
    "        loss_fct=loss_fct_s2, # Pass custom loss function\n",
    "        callbacks=[early_stop_callback] # Keep early stopping\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"üöÄ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"‚åõ Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\n‚úÖ Stage 2 Training Complete.\")\n",
    "    print(\"\\nüéâ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "\n",
    "    \n",
    "    # Return the trained models and processor to be used by analysis functions\n",
    "    return trainer_s1.model, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for new images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"üî¨ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # --- Stage 1 Prediction: \n",
    "        # apply learned T and œÑ; fall back safely if file missing)\n",
    "        # --- Apply Stage-1 temperature scaling + threshold from saved calibration ---\n",
    "        calib_path = os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "        T_s1, tau = 1.0, 0.45  # safe defaults\n",
    "        if os.path.exists(calib_path):\n",
    "            try:\n",
    "                with open(calib_path, \"r\") as f:\n",
    "                    _c = json.load(f)\n",
    "                    T_s1 = float(_c.get(\"T\", 1.0))\n",
    "                    tau  = float(_c.get(\"tau\", 0.45))\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits / max(T_s1, 1e-3)  # temperature scaling\n",
    "            probs_s1 = F.softmax(logits_s1, dim=-1)\n",
    "        \n",
    "        # Create a mask of images that were classified as 'relevant'\n",
    "            # Gate on calibrated œÑ\n",
    "        relevant_mask = (probs_s1[:, label2id_s1['relevant']] >= tau)\n",
    "        dev = logits_s1.device\n",
    "        preds_s1 = torch.where(\n",
    "            relevant_mask,\n",
    "            torch.tensor(label2id_s1['relevant'], device=dev, dtype=torch.long),\n",
    "            torch.tensor(label2id_s1['irrelevant'], device=dev, dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        # --- Stage 2 Prediction (only on relevant images) ---\n",
    "        if relevant_mask.any():\n",
    "            # Filter the input tensors to only include the relevant images\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "\n",
    "        # --- Aggregate Results ---\n",
    "        # Loop through the original batch and assign the correct prediction\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            if relevant_mask[j]:\n",
    "                # If relevant, get the prediction from the Stage 2 model\n",
    "                pred_label = id2label_s2[preds_s2[s2_idx].item()]\n",
    "                confidence = confs_s2[s2_idx].item()\n",
    "                s2_idx += 1\n",
    "            else:\n",
    "                # If not relevant, label it and stop\n",
    "                pred_label = \"irrelevant\"\n",
    "                confidence = torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item()\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    import pandas as pd   # ensure pd is local; prevents UnboundLocalError in notebooks\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\n‚úÖ Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = REVIEW_CONF_THRESHOLD\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"üìÇ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- NEW: Generate shortlist and curated patch CSVs for THIS run ---\n",
    "    #     - Shortlist: low-confidence items in focus classes (for targeted manual review)\n",
    "    #     - Curated patch: template CSV for corrected labels to be fed back into VNext\n",
    "    focus_classes = ['sadness','speech_action','neutral','neutral_speech','happiness']\n",
    "    \n",
    "    # Defensive: ensure the expected columns exist\n",
    "    has_pred = 'prediction' in df.columns or 'predicted_label' in df.columns\n",
    "    pred_col = 'prediction' if 'prediction' in df.columns else ('predicted_label' if 'predicted_label' in df.columns else None)\n",
    "    if pred_col is not None:\n",
    "        # Sort by confidence ascending (uncertain first)\n",
    "        df_focus = df[df[pred_col].isin(focus_classes)].copy()\n",
    "        if 'confidence' in df_focus.columns:\n",
    "            df_focus = df_focus.sort_values('confidence', ascending=True)\n",
    "    \n",
    "        short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "        patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "        # Write shortlist with a stable set of columns\n",
    "        keep_cols = [c for c in ['image_path','filepath','true_label',pred_col,'confidence'] if c in df_focus.columns]\n",
    "        df_focus[keep_cols].to_csv(short_csv, index=False)\n",
    "        print(f\"‚úÖ Shortlist written: {short_csv}\")\n",
    "    \n",
    "        # Create empty curated patch template\n",
    "        src_path_col = 'image_path' if 'image_path' in df_focus.columns else 'filepath'\n",
    "        patch_df = pd.DataFrame({\n",
    "            \"filepath\": df_focus[src_path_col],\n",
    "            \"correct_label\": \"\",\n",
    "            \"notes\": \"\"\n",
    "        })\n",
    "        patch_df.to_csv(patch_csv, index=False)\n",
    "        print(f\"‚úÖ Curated patch template written: {patch_csv}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped shortlist/patch CSVs: no predicted label column found in full log.\")\n",
    "\n",
    "    # --- NEW: Merge this run's shortlist/patch with V32 to create canonical merged artifacts ---\n",
    "    def _merge_csvs(csv_list, key_cols, out_csv):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "    \n",
    "        # Normalize common column name variants so we can dedupe safely\n",
    "        def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            colmap = {}\n",
    "            # path columns\n",
    "            if \"image_path\" not in df.columns:\n",
    "                if \"filepath\" in df.columns:\n",
    "                    colmap[\"filepath\"] = \"image_path\"\n",
    "                elif \"path\" in df.columns:\n",
    "                    colmap[\"path\"] = \"image_path\"\n",
    "            # predicted label columns\n",
    "            if \"predicted_label\" not in df.columns:\n",
    "                if \"prediction\" in df.columns:\n",
    "                    colmap[\"prediction\"] = \"predicted_label\"\n",
    "                elif \"predicted\" in df.columns:\n",
    "                    colmap[\"predicted\"] = \"predicted_label\"\n",
    "            return df.rename(columns=colmap)\n",
    "    \n",
    "        frames = []\n",
    "        for p in csv_list:\n",
    "            if os.path.exists(p):\n",
    "                try:\n",
    "                    df = pd.read_csv(p)\n",
    "                    df = _normalize_cols(df)\n",
    "                    frames.append(df)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "        if not frames:\n",
    "            return\n",
    "    \n",
    "        merged = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "        # Keep only keys that actually exist after normalization\n",
    "        available_keys = [k for k in key_cols if k in merged.columns]\n",
    "        if not available_keys:\n",
    "            print(f\"‚ÑπÔ∏è Skipped merge for {out_csv}: none of the key columns {key_cols} exist in merged data.\")\n",
    "            return\n",
    "    \n",
    "        merged = merged.drop_duplicates(subset=available_keys, keep=\"first\")\n",
    "        merged.to_csv(out_csv, index=False)\n",
    "        print(f\"‚úÖ Merged: {out_csv} ({len(merged)} rows)\")\n",
    "\n",
    "    \n",
    "    # Paths for this run (already defined above)\n",
    "    short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "    patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "    # V32 paths (if present)\n",
    "    v32_short = os.path.join(save_dir, \"curation_shortlist_V32.csv\")\n",
    "    v32_patch = os.path.join(save_dir, \"curated_additions_V32.csv\")\n",
    "    \n",
    "    # Canonical merged outputs\n",
    "    short_merged = os.path.join(save_dir, \"curation_shortlist_merged.csv\")\n",
    "    patch_merged = os.path.join(save_dir, \"curated_additions_merged.csv\")\n",
    "    \n",
    "    # Merge (shortlist merges on [filepath, predicted_label]; patch merges on [filepath])\n",
    "    if pred_col is not None:\n",
    "        # Figure out the filepath column available\n",
    "        avail_path_cols = ['image_path','filepath']\n",
    "        path_col = next((c for c in avail_path_cols if c in df.columns), None)\n",
    "    \n",
    "        if path_col is not None:\n",
    "            _merge_csvs([v32_short, short_csv], key_cols=[path_col, pred_col], out_csv=short_merged)\n",
    "            _merge_csvs([v32_patch, patch_csv], key_cols=[path_col], out_csv=patch_merged)\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Skipped merge: no filepath column present in full log.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped merge: no predicted label column present in full log.\")\n",
    "\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs (toggleable & robust) ---\n",
    "    MINING_HARD_NEGATIVES = True  # ‚Üê set False for deployment runs\n",
    "\n",
    "    if MINING_HARD_NEGATIVES:\n",
    "        import pandas as pd\n",
    "         \n",
    "        # Prefer the freshly generated full log from THIS run; fallback to prior runs only if missing.\n",
    "        inference_log_path = full_log_path\n",
    "        if not os.path.exists(inference_log_path):\n",
    "            v33_log = os.path.join(SAVE_DIR, \"V33_full_inference_log.csv\")\n",
    "            v32_log = os.path.join(SAVE_DIR, \"V32_full_inference_log.csv\")\n",
    "            inference_log_path = v33_log if os.path.exists(v33_log) else (v32_log if os.path.exists(v32_log) else None)\n",
    "\n",
    "    \n",
    "        if not os.path.exists(inference_log_path):\n",
    "            print(\"‚è© Skipping hard-negative mining: no full inference log found.\")\n",
    "        else:\n",
    "            print(\"\\n‚õèÔ∏è  Mining for hard negative confusion pairs...\")\n",
    "            print(f\"   using: {inference_log_path}\")\n",
    "            df = pd.read_csv(inference_log_path)\n",
    "    \n",
    "            # Normalize column names between runs (V32 used 'prediction', V33 uses 'predicted_label')\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            col_true = cols.get(\"true_label\", \"true_label\")\n",
    "            col_pred = cols.get(\"predicted_label\") or cols.get(\"prediction\")\n",
    "            if col_pred is None:\n",
    "                raise RuntimeError(f\"Could not find predicted label column in {df.columns.tolist()}\")\n",
    "    \n",
    "            # (Optional) keep a stable sort by confidence descending if available\n",
    "            col_conf = cols.get(\"confidence\")\n",
    "            if col_conf:\n",
    "                df = df.sort_values(col_conf, ascending=False)\n",
    "    \n",
    "            # Which pairs to mine\n",
    "            confusion_pairs_to_mine = [\n",
    "                ('contempt', 'questioning'),\n",
    "                ('contempt', 'neutral'),\n",
    "                ('fear', 'surprise')\n",
    "            ]\n",
    "    \n",
    "            # Save to the current run folder\n",
    "            save_dir = SAVE_DIR\n",
    "    \n",
    "            for c1, c2 in confusion_pairs_to_mine:\n",
    "                mask = ((df[col_true] == c1) & (df[col_pred] == c2)) | \\\n",
    "                       ((df[col_true] == c2) & (df[col_pred] == c1))\n",
    "                hard_negatives = df.loc[mask]\n",
    "    \n",
    "                if not hard_negatives.empty:\n",
    "                    out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "                    hard_negatives.to_csv(out_path, index=False)\n",
    "                    print(f\"  - Found {len(hard_negatives)} hard negatives for ({c1} ‚Üî {c2}). Saved: {out_path}\")\n",
    "                else:\n",
    "                    print(f\"  - No hard negatives found for ({c1} ‚Üî {c2}).\")\n",
    "    else:\n",
    "        print(\"‚è© Hard-negative mining disabled (set MINING_HARD_NEGATIVES=True to enable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"üìä Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5adb1d-f527-427e-b27d-26c819181e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- DEBUG sanity: verify that `main` is defined and callable ---\n",
    "# try:\n",
    "#     import inspect\n",
    "#     print(\"[DEBUG] __name__:\", __name__)\n",
    "#     print(\"[DEBUG] has `main` in globals():\", 'main' in globals())\n",
    "#     if 'main' in globals():\n",
    "#         print(\"[DEBUG] `main` callable:\", callable(main))\n",
    "#         print(\"[DEBUG] `main` defined at line:\", inspect.getsourcelines(main)[1])\n",
    "# except Exception as _e:\n",
    "#     print(\"[DEBUG] sanity check error:\", repr(_e))\n",
    "\n",
    "\n",
    "# from transformers import AutoImageProcessor\n",
    "# from datasets import load_dataset\n",
    "# import os\n",
    "\n",
    "# # 1) Recreate processor as in Stage-1\n",
    "# processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# # 2) Point to prepared Stage-1 dataset (your script uses this when PREPARE_DATASETS=False)\n",
    "# prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "# stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "\n",
    "# ds_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split=\"train\")\n",
    "# sample = [ds_s1[i] for i in range(4)]\n",
    "\n",
    "# # 3) Minimal Stage-1 augment map (or use your full augment_map_s1 if it‚Äôs available)\n",
    "# try:\n",
    "#     _ = augment_map_s1  # see if your map exists\n",
    "# except NameError:\n",
    "#     augment_map_s1 = {}  # fall back to empty (base_augment only)\n",
    "\n",
    "# # 4) Collator and test\n",
    "# coll = DataCollatorWithAugmentation(\n",
    "#     processor=processor,\n",
    "#     augment_dict=augment_map_s1,\n",
    "#     random_erasing_prob=0.10,\n",
    "#     random_erasing_scale=(0.02, 0.08),\n",
    "#     skip_erasing_label_ids=[]\n",
    "# )\n",
    "\n",
    "# batch = coll(sample)\n",
    "# print(\"pixel_values shape:\", batch[\"pixel_values\"].shape)  # (4, 3, 224, 224)\n",
    "# print(\"labels shape:\", batch[\"labels\"].shape)              # (4,)\n",
    "\n",
    "# from transformers import ViTForImageClassification, AutoImageProcessor\n",
    "# from datasets import load_dataset\n",
    "# import os, torch\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # Paths mirror your script‚Äôs logic\n",
    "# prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "# stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "# s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "\n",
    "# # Recreate processor & model as Stage 1 does\n",
    "# processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "# model_s1 = ViTForImageClassification.from_pretrained(\n",
    "#     s1_checkpoint_path,\n",
    "#     num_labels=2,\n",
    "#     label2id=label2id_s1,\n",
    "#     id2label=id2label_s1,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# ).to(device).eval()\n",
    "\n",
    "# # Eval split\n",
    "# dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split=\"train\").train_test_split(test_size=0.2, seed=42)\n",
    "# eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "\n",
    "# print(\"\\nüß™ Calibrating Stage 1 (standalone)‚Ä¶\")\n",
    "# T_hat = fit_temperature(model_s1, eval_dataset_s1, processor, device)\n",
    "# _ = sweep_tau(model_s1, eval_dataset_s1, processor, device, T=T_hat)\n",
    "# print(\"‚úÖ S1 calibration helpers completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca42a3-7962-4c1a-8ae5-308de0a67325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3682e-b86d-4920-99cc-2dc7e12a14d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è Using device: cpu\n",
      "‚úÖ Skipping dataset preparation, using existing directories.\n",
      "\n",
      "============================================================\n",
      "  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfa6eb3383c4c79a7424326270a49b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: 21504 training samples, 5377 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions_v5/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Stage 1 Class Weights: tensor([0.6492, 2.1761])\n",
      "üöÄ Starting Stage 1 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6720' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6720/6720 10:08:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.230733</td>\n",
       "      <td>0.930816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.226137</td>\n",
       "      <td>0.953134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>0.117083</td>\n",
       "      <td>0.963920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.115449</td>\n",
       "      <td>0.966524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>0.962247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.93      0.99      0.96      4132\n",
      "    relevant       0.94      0.75      0.83      1245\n",
      "\n",
      "    accuracy                           0.93      5377\n",
      "   macro avg       0.94      0.87      0.89      5377\n",
      "weighted avg       0.93      0.93      0.93      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - relevant ‚Üí irrelevant: 316 instances\n",
      "  - irrelevant ‚Üí relevant: 56 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.1981\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4571\n",
      "  - irrelevant: entropy = 0.1201\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.95      0.99      0.97      4132\n",
      "    relevant       0.96      0.84      0.89      1245\n",
      "\n",
      "    accuracy                           0.95      5377\n",
      "   macro avg       0.95      0.91      0.93      5377\n",
      "weighted avg       0.95      0.95      0.95      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - relevant ‚Üí irrelevant: 205 instances\n",
      "  - irrelevant ‚Üí relevant: 47 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.1233\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.3648\n",
      "  - irrelevant: entropy = 0.0505\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.98      0.98      0.98      4132\n",
      "    relevant       0.92      0.92      0.92      1245\n",
      "\n",
      "    accuracy                           0.96      5377\n",
      "   macro avg       0.95      0.95      0.95      5377\n",
      "weighted avg       0.96      0.96      0.96      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - relevant ‚Üí irrelevant: 100 instances\n",
      "  - irrelevant ‚Üí relevant: 94 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.1160\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.2966\n",
      "  - irrelevant: entropy = 0.0615\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.98      0.97      0.98      4132\n",
      "    relevant       0.92      0.94      0.93      1245\n",
      "\n",
      "    accuracy                           0.97      5377\n",
      "   macro avg       0.95      0.96      0.95      5377\n",
      "weighted avg       0.97      0.97      0.97      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant ‚Üí relevant: 105 instances\n",
      "  - relevant ‚Üí irrelevant: 75 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0854\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.2082\n",
      "  - irrelevant: entropy = 0.0485\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.98      0.97      0.98      4132\n",
      "    relevant       0.90      0.94      0.92      1245\n",
      "\n",
      "    accuracy                           0.96      5377\n",
      "   macro avg       0.94      0.95      0.95      5377\n",
      "weighted avg       0.96      0.96      0.96      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant ‚Üí relevant: 129 instances\n",
      "  - relevant ‚Üí irrelevant: 74 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0775\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.1710\n",
      "  - irrelevant: entropy = 0.0494\n",
      "‚åõ Stage 1 training took: 10:08:29\n",
      "üíæ Saving relevance_filter_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V34_20251013_211825\n",
      "‚úÖ relevance_filter_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 1 Training Complete.\n",
      "\n",
      "üß™ Calibrating Stage 1...\n",
      "‚úÖ S1 calibration done: T=3.184 | best œÑ=0.3 | F1=0.542 (P=0.685, R=0.448)\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4ee5a1c6214512be4cb04d659b01fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n",
      "üöÄ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1854' max='1854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1854/1854 2:06:23, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.209700</td>\n",
       "      <td>0.262585</td>\n",
       "      <td>0.889069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.243608</td>\n",
       "      <td>0.897166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.232779</td>\n",
       "      <td>0.897166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.234519</td>\n",
       "      <td>0.902834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.055800</td>\n",
       "      <td>0.227180</td>\n",
       "      <td>0.910931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.210232</td>\n",
       "      <td>0.918219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.81      1.00      0.89        85\n",
      "      contempt       0.83      0.80      0.81        60\n",
      "       disgust       1.00      0.69      0.82        26\n",
      "          fear       0.88      0.93      0.90        71\n",
      "     happiness       0.96      0.94      0.95       167\n",
      "       neutral       0.96      0.96      0.96       135\n",
      "   questioning       0.76      0.84      0.80        92\n",
      "       sadness       0.76      0.47      0.58        40\n",
      "      surprise       0.98      0.97      0.98       147\n",
      "neutral_speech       0.90      0.86      0.88       381\n",
      " speech_action       0.63      0.84      0.72        31\n",
      "\n",
      "      accuracy                           0.89      1235\n",
      "     macro avg       0.86      0.85      0.85      1235\n",
      "  weighted avg       0.89      0.89      0.89      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 14 instances\n",
      "  - neutral_speech ‚Üí questioning: 13 instances\n",
      "  - neutral_speech ‚Üí anger: 12 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.7590\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - sadness: entropy = 1.1754\n",
      "  - disgust: entropy = 1.0940\n",
      "  - contempt: entropy = 0.9170\n",
      "  - speech_action: entropy = 0.8290\n",
      "  - questioning: entropy = 0.8209\n",
      "  - neutral_speech: entropy = 0.8188\n",
      "  - neutral: entropy = 0.7190\n",
      "  - happiness: entropy = 0.6974\n",
      "  - anger: entropy = 0.6174\n",
      "  - surprise: entropy = 0.5956\n",
      "  - fear: entropy = 0.5648\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.92      0.96      0.94        85\n",
      "      contempt       0.86      0.63      0.73        60\n",
      "       disgust       0.82      0.88      0.85        26\n",
      "          fear       0.92      0.85      0.88        71\n",
      "     happiness       0.91      0.99      0.95       167\n",
      "       neutral       0.94      0.96      0.95       135\n",
      "   questioning       0.80      0.84      0.82        92\n",
      "       sadness       0.76      0.55      0.64        40\n",
      "      surprise       0.96      0.97      0.96       147\n",
      "neutral_speech       0.89      0.92      0.91       381\n",
      " speech_action       0.82      0.58      0.68        31\n",
      "\n",
      "      accuracy                           0.90      1235\n",
      "     macro avg       0.87      0.83      0.85      1235\n",
      "  weighted avg       0.90      0.90      0.89      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 15 instances\n",
      "  - speech_action ‚Üí neutral_speech: 12 instances\n",
      "  - neutral_speech ‚Üí happiness: 11 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.6050\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - sadness: entropy = 0.9020\n",
      "  - contempt: entropy = 0.8456\n",
      "  - disgust: entropy = 0.8277\n",
      "  - speech_action: entropy = 0.7960\n",
      "  - questioning: entropy = 0.7329\n",
      "  - anger: entropy = 0.6414\n",
      "  - fear: entropy = 0.6155\n",
      "  - neutral_speech: entropy = 0.6066\n",
      "  - surprise: entropy = 0.5667\n",
      "  - neutral: entropy = 0.5397\n",
      "  - happiness: entropy = 0.3667\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.91      0.95      0.93        85\n",
      "      contempt       0.79      0.70      0.74        60\n",
      "       disgust       0.92      0.92      0.92        26\n",
      "          fear       0.87      0.86      0.87        71\n",
      "     happiness       0.98      0.93      0.96       167\n",
      "       neutral       0.94      0.95      0.94       135\n",
      "   questioning       0.77      0.86      0.81        92\n",
      "       sadness       0.66      0.47      0.55        40\n",
      "      surprise       0.99      0.95      0.97       147\n",
      "neutral_speech       0.89      0.93      0.91       381\n",
      " speech_action       0.83      0.77      0.80        31\n",
      "\n",
      "      accuracy                           0.90      1235\n",
      "     macro avg       0.87      0.85      0.85      1235\n",
      "  weighted avg       0.90      0.90      0.90      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 16 instances\n",
      "  - happiness ‚Üí neutral_speech: 11 instances\n",
      "  - contempt ‚Üí questioning: 9 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.5511\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.8472\n",
      "  - sadness: entropy = 0.8184\n",
      "  - contempt: entropy = 0.7374\n",
      "  - speech_action: entropy = 0.6717\n",
      "  - questioning: entropy = 0.6395\n",
      "  - neutral: entropy = 0.5749\n",
      "  - neutral_speech: entropy = 0.5113\n",
      "  - anger: entropy = 0.5011\n",
      "  - surprise: entropy = 0.4942\n",
      "  - fear: entropy = 0.4931\n",
      "  - happiness: entropy = 0.4750\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.94      0.98      0.96        85\n",
      "      contempt       0.88      0.75      0.81        60\n",
      "       disgust       0.92      0.85      0.88        26\n",
      "          fear       0.88      0.92      0.90        71\n",
      "     happiness       0.99      0.93      0.96       167\n",
      "       neutral       0.98      0.93      0.95       135\n",
      "   questioning       0.75      0.84      0.79        92\n",
      "       sadness       0.88      0.35      0.50        40\n",
      "      surprise       0.97      0.97      0.97       147\n",
      "neutral_speech       0.87      0.95      0.91       381\n",
      " speech_action       0.79      0.71      0.75        31\n",
      "\n",
      "      accuracy                           0.90      1235\n",
      "     macro avg       0.89      0.83      0.85      1235\n",
      "  weighted avg       0.91      0.90      0.90      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 23 instances\n",
      "  - contempt ‚Üí questioning: 11 instances\n",
      "  - happiness ‚Üí neutral_speech: 11 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.5119\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.9396\n",
      "  - contempt: entropy = 0.8169\n",
      "  - sadness: entropy = 0.7455\n",
      "  - questioning: entropy = 0.5923\n",
      "  - neutral: entropy = 0.5654\n",
      "  - speech_action: entropy = 0.5570\n",
      "  - fear: entropy = 0.4813\n",
      "  - anger: entropy = 0.4812\n",
      "  - neutral_speech: entropy = 0.4533\n",
      "  - happiness: entropy = 0.4259\n",
      "  - surprise: entropy = 0.4216\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.95      0.96      0.96        85\n",
      "      contempt       0.79      0.82      0.80        60\n",
      "       disgust       0.90      0.73      0.81        26\n",
      "          fear       0.91      0.89      0.90        71\n",
      "     happiness       0.99      0.95      0.97       167\n",
      "       neutral       0.99      0.94      0.97       135\n",
      "   questioning       0.81      0.95      0.87        92\n",
      "       sadness       0.82      0.35      0.49        40\n",
      "      surprise       0.98      0.97      0.97       147\n",
      "neutral_speech       0.88      0.95      0.92       381\n",
      " speech_action       0.76      0.71      0.73        31\n",
      "\n",
      "      accuracy                           0.91      1235\n",
      "     macro avg       0.89      0.84      0.85      1235\n",
      "  weighted avg       0.91      0.91      0.91      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 22 instances\n",
      "  - happiness ‚Üí neutral_speech: 9 instances\n",
      "  - speech_action ‚Üí neutral_speech: 9 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4988\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.8886\n",
      "  - sadness: entropy = 0.7692\n",
      "  - contempt: entropy = 0.7378\n",
      "  - questioning: entropy = 0.5719\n",
      "  - neutral: entropy = 0.5359\n",
      "  - speech_action: entropy = 0.5303\n",
      "  - fear: entropy = 0.4846\n",
      "  - anger: entropy = 0.4745\n",
      "  - neutral_speech: entropy = 0.4505\n",
      "  - happiness: entropy = 0.4194\n",
      "  - surprise: entropy = 0.4089\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.93      0.95      0.94        85\n",
      "      contempt       0.88      0.75      0.81        60\n",
      "       disgust       0.88      0.88      0.88        26\n",
      "          fear       0.93      0.92      0.92        71\n",
      "     happiness       0.98      0.94      0.96       167\n",
      "       neutral       0.99      0.95      0.97       135\n",
      "   questioning       0.84      0.91      0.88        92\n",
      "       sadness       0.83      0.50      0.62        40\n",
      "      surprise       0.99      0.97      0.98       147\n",
      "neutral_speech       0.88      0.96      0.92       381\n",
      " speech_action       0.79      0.74      0.77        31\n",
      "\n",
      "      accuracy                           0.92      1235\n",
      "     macro avg       0.90      0.86      0.88      1235\n",
      "  weighted avg       0.92      0.92      0.92      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 19 instances\n",
      "  - happiness ‚Üí neutral_speech: 10 instances\n",
      "  - speech_action ‚Üí neutral_speech: 8 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4928\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.7748\n",
      "  - sadness: entropy = 0.7527\n",
      "  - contempt: entropy = 0.6988\n",
      "  - questioning: entropy = 0.6040\n",
      "  - neutral: entropy = 0.5407\n",
      "  - fear: entropy = 0.5041\n",
      "  - speech_action: entropy = 0.4913\n",
      "  - anger: entropy = 0.4597\n",
      "  - neutral_speech: entropy = 0.4399\n",
      "  - happiness: entropy = 0.4220\n",
      "  - surprise: entropy = 0.4058\n",
      "‚åõ Stage 2 training took: 02:06:26\n",
      "üíæ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V34_20251013_211825\n",
      "‚úÖ emotion_classifier_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 2 Training Complete.\n",
      "\n",
      "üéâ Hierarchical Training Pipeline Finished Successfully.\n",
      "\n",
      "============================================================\n",
      "  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\n",
      "============================================================\n",
      "Found 26902 images to process for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Running Hierarchical Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 841/841 [24:47<00:00,  1.77s/it]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pd' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# --- Step 2: Run Post-Training Analysis & Curation ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RUN_INFERENCE:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# This function runs the full inference pass and generates logs for review.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# It uses the in-memory models returned from main().\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mrun_post_training_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_s1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_s2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_DATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVERSION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# --- Step 3: Run Final Model Checks ---\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Check if the model is ready for \"deployment\" based on F1 scores\u001b[39;00m\n\u001b[1;32m     21\u001b[0m stage2_metrics_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(SAVE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_class_metrics_Stage2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mrun_post_training_analysis\u001b[0;34m(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_image_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images to process for inference.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n\u001b[0;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(predictions)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Derive true label from path for analysis\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m p: Path(p)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pd' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device once for the entire script run.\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # --- Step 1: Execute Training Pipeline ---\n",
    "    # The main function now returns the trained models and processor\n",
    "    model_s1, model_s2, processor = main(device)\n",
    "    \n",
    "    # --- Step 2: Run Post-Training Analysis & Curation ---\n",
    "    if RUN_INFERENCE:\n",
    "        # This function runs the full inference pass and generates logs for review.\n",
    "        # It uses the in-memory models returned from main().\n",
    "        run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "    \n",
    "    # --- Step 3: Run Final Model Checks ---\n",
    "    # Check if the model is ready for \"deployment\" based on F1 scores\n",
    "    stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "    check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "    \n",
    "    # --- Step 4: Calibrate the Stage 2 Model ---\n",
    "    logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "    labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "    \n",
    "    if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  CALIBRATING STAGE 2 MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        logits_s2 = np.load(logits_s2_path)\n",
    "        labels_s2 = np.load(labels_s2_path)\n",
    "        \n",
    "        optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "        print(f\"‚úÖ Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "        # plot_reliability_diagram(logits_s2, labels_s2, optimal_temp, SAVE_DIR, VERSION, \"Stage2\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping calibration, logits/labels files for Stage 2 not found.\")\n",
    "\n",
    "    # COME BACK LATER TO MAKE DYNAMIC AND AUTOMATED LOADING OF PATH\n",
    "    # --- Step 5: (Hypothetical) Run Ensemble Analysis ---\n",
    "    # Use the saved V32 artifacts as the \"previous\" models for ensembling\n",
    "    v_prev_path = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\"\n",
    "    \n",
    "    if os.path.exists(v_prev_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  RUNNING HIERARCHICAL ENSEMBLE ANALYSIS (current + V32)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the older V32 models for the ensemble\n",
    "        s1_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"relevance_filter_model\")\n",
    "        ).to(device).eval()\n",
    "        s2_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"emotion_classifier_model\")\n",
    "        ).to(device).eval()\n",
    "        \n",
    "        # Use the in-memory models from THIS run (e.g., V33 when you launch it)\n",
    "        # Assumes you have model_s1 and model_s2 already defined in memory\n",
    "        s1_models_ensemble = [model_s1, s1_model_prev]\n",
    "        s2_models_ensemble = [model_s2, s2_model_prev]\n",
    "\n",
    "        # NEW: auto-pick a real image from ANY non-empty predicted-class folder\n",
    "        review_root = os.path.join(v_prev_path, \"review_candidates_by_predicted_class\")\n",
    "        example_image_path = None\n",
    "        if os.path.isdir(review_root):\n",
    "            for cls in os.listdir(review_root):\n",
    "                cls_dir = os.path.join(review_root, cls)\n",
    "                if os.path.isdir(cls_dir):\n",
    "                    imgs = [f for f in os.listdir(cls_dir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\"))]\n",
    "                    if imgs:\n",
    "                        example_image_path = os.path.join(cls_dir, imgs[0])\n",
    "                        break\n",
    "    \n",
    "        if example_image_path and os.path.exists(example_image_path):\n",
    "            prediction, confidence = hierarchical_ensemble_predict(\n",
    "                example_image_path, processor, s1_models_ensemble, s2_models_ensemble, device\n",
    "            )\n",
    "            print(f\"Ensemble prediction for {Path(example_image_path).name}: {prediction} (Confidence: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Skipping ensemble demo: no example image found under 'review_candidates_by_predicted_class'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf30308-a3b6-42a8-9c9a-bc83fcc4871e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
