{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# V33 changes:\n",
    "    # overview: V33 is built on the V32 codebase but intentionally reverts V31â€™s\n",
    "    #   risky fine-tuning/augmentation choices back toward V30â€™s proven baseline.\n",
    "    #   On top of that baseline, V33 adds targeted improvements (class-specific\n",
    "    #   mild aug, targeted smoothing + focal, calibrated S1 threshold) to address\n",
    "    #   persistent weaknesses in 'sadness' and 'speech_action' without reintroducing\n",
    "    #   the V31 regressions.\n",
    "\n",
    "    # section #4 - Stage 1 simplification:\n",
    "    #   Reverted to a single, uniform learning rate (3e-5) managed by the Trainer.\n",
    "    #   (Undoes V31â€™s discriminative LR / freezing complexity.)\n",
    "\n",
    "    # section #4 - Augmentation scope correction:\n",
    "    #   Removed 'sadness' and 'speech_action' from the heavy RandAugment list\n",
    "    #   to avoid distorting subtle features (restores V30 behavior).\n",
    "\n",
    "    # section #5a - Stage 2 targeted loss:\n",
    "    #   Kept label smoothing at 0.05 overall, but turned OFF smoothing for\n",
    "    #   ['sadness','speech_action'] and added mild focal scaling (gamma=1.5)\n",
    "    #   inside TargetedSmoothedCrossEntropyLoss to emphasize hard examples.\n",
    "\n",
    "    # section #6b - Stage 2 optimization stability:\n",
    "    #   lr_scheduler_type=\"cosine\", warmup_ratio=0.10, weight_decay=0.05,\n",
    "    #   gradient_accumulation_steps=2, num_train_epochs=6, learning_rate=4e-5.\n",
    "\n",
    "    # section #6c - Aug pipeline wiring:\n",
    "    #   Introduced a *mild* augmentation pipeline specifically for\n",
    "    #   ['sadness','speech_action'] and passed the MERGED `augment_dict`\n",
    "    #   (minority_augment_map_s2 âˆª targeted_mild_map_s2) to the collator.\n",
    "\n",
    "    # section #6d - Collator robustness (tensor-only erasing):\n",
    "    #   Moved RandomErasing to a tensor-only step (ToTensor -> RandomErasing -> ToPIL)\n",
    "    #   so torchvision sees a tensor (prevents 'Image' has no attribute 'shape').\n",
    "\n",
    "    # section #6e - Fragile-class policy:\n",
    "    #   Enabled per-class skip for RandomErasing via `skip_erasing_label_ids`\n",
    "    #   for ['sadness','speech_action'] to preserve delicate cues, while\n",
    "    #   keeping erasing active for other classes\n",
    "\n",
    "    # section #6f - Minor collator naming fix:\n",
    "    #   - Corrected attribute reference in DataCollatorWithAugmentation.__call__\n",
    "    #     from self.base_augment_pil to self.base_augment to match the defined\n",
    "    #     attribute name and prevent AttributeError.\n",
    "\n",
    "    # section #7 - Stage 1 inference calibration:\n",
    "    #   Switched hierarchical_predict to a probability threshold (tau=0.45)\n",
    "    #   for 'relevant' instead of argmax to lift recall without retraining.\n",
    "\n",
    "    # section #8 - Ensemble analysis demo:\n",
    "    #   Made the demo runnable by auto-picking an image from any non-empty\n",
    "    #   folder under 'review_candidates_by_predicted_class' with proper guards.\n",
    "\n",
    "    # section #9 - Consistency cleanup:\n",
    "    #   Standardized the processor call in hierarchical_ensemble_predict to\n",
    "    #   use `images=image` for parity with the rest of the codebase.\n",
    "\n",
    "    # section #10 - Operational defaults:\n",
    "    #   Set RUN_INFERENCE=False and PREPARE_DATASETS=False by default to\n",
    "    #   avoid heavy reprocessing during everyday runs. Toggle to True when needed.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dynamically loading latest checkpoint: V31_20251007_153512\n",
      "ðŸ“ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V33_20251010_122749\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- ðŸ“‚ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- âš™ï¸ Run Configuration ---\n",
    "# default safer for daily dev runs; flip to True when you want full-corpus inference\n",
    "RUN_INFERENCE = False\n",
    "# default safer; run once when dataset layout changes\n",
    "PREPARE_DATASETS = False\n",
    "\n",
    "# Finds the most recent V* model directory based on modification time.\n",
    "def find_latest_checkpoint(root_dir):\n",
    "    all_run_dirs = [\n",
    "        os.path.join(root_dir, d)\n",
    "        for d in os.listdir(root_dir)\n",
    "        if d.startswith(\"V\") and os.path.isdir(os.path.join(root_dir, d))\n",
    "    ]\n",
    "    if not all_run_dirs:\n",
    "        return None\n",
    "\n",
    "    # Sort directories by modification time, newest first\n",
    "    sorted_dirs = sorted(all_run_dirs, key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    # The newest directory is the current run's empty folder.\n",
    "    # We need the second newest, which is the latest *completed* run.\n",
    "    if len(sorted_dirs) > 1:\n",
    "        return sorted_dirs[1] # <-- Return the second item in the list\n",
    "    else:\n",
    "        # If there's only one (or zero), no previous checkpoint exists\n",
    "        return None\n",
    "\n",
    "# --- ðŸ¤– Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Dynamically find the latest checkpoint to train from\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = latest_checkpoint\n",
    "    print(f\"âœ… Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    # If no checkpoint is found, fall back to the base model from Hugging Face\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(f\"âš ï¸ No local checkpoint found. Starting from base model: {BASE_MODEL_NAME}\")\n",
    "    \n",
    "# --- ðŸ·ï¸ Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# --- ðŸ–¼ï¸ File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- ðŸ”¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"ðŸ—‚ï¸ Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\nâœ… Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# ðŸ“¦ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self, processor, augment_dict):\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = T.Compose([\n",
    "            T.RandomResizedCrop(size=(224, 224)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        ])\n",
    "\n",
    "        # Tensor-domain ops\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.to_pil = T.ToPILImage()\n",
    "        self.post_tensor_erase = T.RandomErasing(p=0.2, scale=(0.02, 0.08))\n",
    "\n",
    "        # Skip erasing on fragile classes\n",
    "        self.skip_erasing_label_ids = {\n",
    "            label2id_s2['sadness'],\n",
    "            label2id_s2['speech_action']\n",
    "        }\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "\n",
    "            # 1) apply class-specific PIL pipeline if present; else base PIL pipeline\n",
    "            pil_aug = self.augment_dict.get(label, self.base_augment)\n",
    "\n",
    "            img = pil_aug(rgb_image)\n",
    "\n",
    "            # 2) apply RandomErasing on tensor (only if not skipped)\n",
    "            if label not in self.skip_erasing_label_ids:\n",
    "                img_t = self.to_tensor(img)                 # PIL -> Tensor [C,H,W]\n",
    "                img_t = self.post_tensor_erase(img_t)       # RandomErasing on tensor\n",
    "                img = self.to_pil(img_t)                    # Tensor -> PIL (processor expects PIL/np)\n",
    "\n",
    "            processed_images.append(img)\n",
    "\n",
    "        batch = self.processor(images=processed_images, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# ðŸ‹ï¸ Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ðŸ”„ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None, focal_gamma=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.focal_gamma = focal_gamma  # NEW (None disables focal scaling)\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        ce_per_sample = -(smooth_labels * log_probs).sum(dim=1)\n",
    "\n",
    "        # NEW: optional focal scaling\n",
    "        if self.focal_gamma is not None and self.focal_gamma > 0:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pt = (probs * smooth_labels).sum(dim=1).clamp_min(1e-6)\n",
    "            ce_per_sample = ((1 - pt) ** self.focal_gamma) * ce_per_sample\n",
    "\n",
    "        return ce_per_sample.mean()\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# ðŸ“Š Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(eval_pred, label_names, stage_name=\"\"):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw logits/labels for later analysis like temperature scaling\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in label_names]\n",
    "    recalls = [report[name][\"recall\"] for name in label_names]\n",
    "    precisions = [report[name][\"precision\"] for name in label_names]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    \n",
    "    # Create a dictionary for entropies in the correct order for the CSV\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    # Access the trainer instance through its global-like availability during compute_metrics call\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names}\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "    # --- End Re-integration ---\n",
    "\n",
    "    # Generate and save a heatmap of the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((label_names[i], label_names[j]), cm[i][j])\n",
    "        for i in range(len(label_names))\n",
    "        for j in range(len(label_names)) if i != j and cm[i][j] > 0\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} â†’ {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute and print entropy metrics\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nðŸ§  Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nðŸ” Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "    # --- End Re-integration ---\n",
    "    \n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# ðŸ’¾ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"ðŸ’¾ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"âœ… {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"âš ï¸ Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - âŒ {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - âœ… {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # --- Define specific model paths from the latest checkpoint ---\n",
    "    s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "    s2_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"emotion_classifier_model\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"âœ… Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        s1_checkpoint_path, # <-- Use the specific path for the Stage 1 model\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"âš–ï¸ Stage 1 Class Weights: {class_weights_s1}\")\n",
    "\n",
    "    # --- Define Early Stopping ---\n",
    "    # Stops training if validation loss doesn't improve for 2 consecutive epochs\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    # The complex discriminative learning rate and layer freezing strategy in \n",
    "        # V31 caused a severe performance drop. This change reverts Stage 1 to \n",
    "        # V30's simpler and more effective approach of using a single, uniform \n",
    "        # learning rate for the entire model, which is managed by the Hugging \n",
    "        # Face Trainer's default optimizer.\n",
    "    training_args_s1.learning_rate = 3e-5 # Set learning rate directly\n",
    "    \n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=list(id2label_s1.values()), stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict={}), # Use base augmentation for all\n",
    "        class_weights=class_weights_s1, # Pass weights to the trainer\n",
    "        callbacks=[early_stop_callback] # Keep early stopping\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"ðŸš€ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"âŒ› Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\nâœ… Stage 1 Training Complete.\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(train_dataset_s2['label']))\n",
    "\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        s2_checkpoint_path, # <-- Use the specific path for the Stage 2 model\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=9),\n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "\n",
    "    # The addition of 'sadness' and 'speech_action' to the heavy augmentation pipeline in V31 \n",
    "        # was counterproductive, causing the F1-scores for these classes to collapse. \n",
    "        # This change reverts the list to the V30 definition, removing the aggressive \n",
    "        # augmentation from the classes it harmed.\n",
    "    minority_classes_s2 = [label2id_s2[name] for name in ['disgust', 'questioning', 'contempt', 'fear']]\n",
    "    minority_augment_map_s2 = {label_id: minority_aug for label_id in minority_classes_s2}\n",
    "\n",
    "    # NEW: very mild, targeted aug ONLY for the weakest classes (no RandAugment)\n",
    "    mild_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.05, 0.05, 0.05, 0.02),\n",
    "        T.RandomPerspective(distortion_scale=0.05, p=0.3),\n",
    "    ])\n",
    "    targeted_mild_classes = [label2id_s2['sadness'], label2id_s2['speech_action']]\n",
    "    targeted_mild_map_s2 = {label_id: mild_aug for label_id in targeted_mild_classes}\n",
    "\n",
    "    # MERGE: single mapping passed to the collator (class id -> transform)\n",
    "    augment_dict = {**minority_augment_map_s2, **targeted_mild_map_s2}\n",
    "\n",
    "    # Use the custom loss function to turn off label smoothing for historically difficult classes.\n",
    "        # Turn OFF smoothing for the hardest classes (sharper targets) and apply mild focal emphasis\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,\n",
    "        target_class_names=['sadness', 'speech_action'],   \n",
    "        label2id_map=label2id_s2,\n",
    "        focal_gamma=1.5                                    \n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # Adding weight decay, cosine scheduler + warmup, grad accumulation improves stability \n",
    "        # (especially on CPU/small batch) without altering your high-level flow.\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=6,                       # +1 epoch for minorities\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "        weight_decay=0.05,                        # NEW\n",
    "        lr_scheduler_type=\"cosine\",               # NEW\n",
    "        warmup_ratio=0.10,                        # NEW\n",
    "        gradient_accumulation_steps=2,            # NEW\n",
    "    )\n",
    "    training_args_s2.learning_rate = 4e-5\n",
    "\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # As with Stage 1, the complex fine-tuning strategy implemented in V31 failed. \n",
    "        # This change reverts the Stage 2 training process to V30's more effective \n",
    "        # uniform learning rate strategy to restore model performance.\n",
    "    training_args_s2.learning_rate = 4e-5 # Set learning rate directly\n",
    "\n",
    "    # Use the CustomLossTrainer again, passing the targeted loss function.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=RELEVANT_CLASSES, stage_name=\"Stage2\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict=augment_dict),\n",
    "        loss_fct=loss_fct_s2, # Pass custom loss function\n",
    "        callbacks=[early_stop_callback] # Keep early stopping\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"ðŸš€ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"âŒ› Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\nâœ… Stage 2 Training Complete.\")\n",
    "    print(\"\\nðŸŽ‰ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "    \n",
    "    # Return the trained models and processor to be used by analysis functions\n",
    "    return trainer_s1.model, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for new images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"ðŸ”¬ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # --- Stage 1 Prediction: Is the image relevant? ---\n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits\n",
    "            probs_s1 = F.softmax(logits_s1, dim=-1)\n",
    "        \n",
    "        # Calibrated threshold for 'relevant' (label id = 1)\n",
    "        tau = 0.45\n",
    "        # Create a mask of images that were classified as 'relevant'\n",
    "        relevant_mask = (probs_s1[:, label2id_s1['relevant']] >= tau)\n",
    "        preds_s1 = torch.where(relevant_mask, torch.tensor(label2id_s1['relevant']), torch.tensor(label2id_s1['irrelevant']))\n",
    "\n",
    "\n",
    "        # --- Stage 2 Prediction (only on relevant images) ---\n",
    "        if relevant_mask.any():\n",
    "            # Filter the input tensors to only include the relevant images\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "\n",
    "        # --- Aggregate Results ---\n",
    "        # Loop through the original batch and assign the correct prediction\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            if relevant_mask[j]:\n",
    "                # If relevant, get the prediction from the Stage 2 model\n",
    "                pred_label = id2label_s2[preds_s2[s2_idx].item()]\n",
    "                confidence = confs_s2[s2_idx].item()\n",
    "                s2_idx += 1\n",
    "            else:\n",
    "                # If not relevant, label it and stop\n",
    "                pred_label = \"irrelevant\"\n",
    "                confidence = torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item()\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\nâœ… Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = 0.85\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"ðŸ“‚ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs ---\n",
    "    # Find images where the model was confused between specific, problematic classes\n",
    "    confusion_pairs_to_mine = [('contempt', 'questioning'), ('contempt', 'neutral'), ('fear', 'surprise')]\n",
    "    \n",
    "    print(\"\\nâ›ï¸  Mining for hard negative confusion pairs...\")\n",
    "    for pair in confusion_pairs_to_mine:\n",
    "        c1, c2 = pair\n",
    "        # Find images where true is c1 but predicted is c2, OR true is c2 and predicted is c1\n",
    "        mask = ((df['true_label'] == c1) & (df['prediction'] == c2)) | \\\n",
    "               ((df['true_label'] == c2) & (df['prediction'] == c1))\n",
    "        \n",
    "        hard_negatives = df[mask]\n",
    "        \n",
    "        if not hard_negatives.empty:\n",
    "            out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "            hard_negatives.to_csv(out_path, index=False)\n",
    "            print(f\"  - Found {len(hard_negatives)} hard negatives for {pair}. Saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"ðŸ“Š Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ–¥ï¸ Using device: cpu\n",
      "âœ… Skipping dataset preparation, using existing directories.\n",
      "\n",
      "============================================================\n",
      "  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42d9c68fc084d5999ceeced660e6cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: 21504 training samples, 5377 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions_v5/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Stage 1 Class Weights: tensor([0.6492, 2.1761])\n",
      "ðŸš€ Starting Stage 1 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4032' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4032/6720 2:51:49 < 1:54:36, 0.39 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>0.747071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.516700</td>\n",
       "      <td>0.501957</td>\n",
       "      <td>0.805654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.501047</td>\n",
       "      <td>0.789846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.91      0.75      0.82      4132\n",
      "    relevant       0.47      0.75      0.58      1245\n",
      "\n",
      "    accuracy                           0.75      5377\n",
      "   macro avg       0.69      0.75      0.70      5377\n",
      "weighted avg       0.81      0.75      0.76      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 1053 instances\n",
      "  - relevant â†’ irrelevant: 307 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.5260\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - irrelevant: entropy = 0.5280\n",
      "  - relevant: entropy = 0.5195\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.89      0.86      0.87      4132\n",
      "    relevant       0.57      0.63      0.60      1245\n",
      "\n",
      "    accuracy                           0.81      5377\n",
      "   macro avg       0.73      0.74      0.74      5377\n",
      "weighted avg       0.81      0.81      0.81      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 585 instances\n",
      "  - relevant â†’ irrelevant: 460 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.4228\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4631\n",
      "  - irrelevant: entropy = 0.4106\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.90      0.82      0.86      4132\n",
      "    relevant       0.54      0.68      0.60      1245\n",
      "\n",
      "    accuracy                           0.79      5377\n",
      "   macro avg       0.72      0.75      0.73      5377\n",
      "weighted avg       0.81      0.79      0.80      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 737 instances\n",
      "  - relevant â†’ irrelevant: 393 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.4044\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4189\n",
      "  - irrelevant: entropy = 0.4000\n",
      "âŒ› Stage 1 training took: 02:51:51\n",
      "ðŸ’¾ Saving relevance_filter_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V33_20251010_122749\n",
      "âœ… relevance_filter_model saved successfully.\n",
      "\n",
      "âœ… Stage 1 Training Complete.\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d53e210796409882d5a1b44b5c7d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n",
      "ðŸš€ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1854' max='1854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1854/1854 1:46:53, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>0.442784</td>\n",
       "      <td>0.801619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.403600</td>\n",
       "      <td>0.338347</td>\n",
       "      <td>0.830769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.338208</td>\n",
       "      <td>0.844534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.291141</td>\n",
       "      <td>0.855870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.281384</td>\n",
       "      <td>0.859109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.280947</td>\n",
       "      <td>0.863158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.79      0.71      0.75        85\n",
      "      contempt       0.72      0.83      0.78        60\n",
      "       disgust       0.86      0.73      0.79        26\n",
      "          fear       0.94      0.90      0.92        71\n",
      "     happiness       0.86      0.81      0.83       167\n",
      "       neutral       0.80      0.73      0.76       135\n",
      "   questioning       0.73      0.68      0.71        92\n",
      "       sadness       1.00      0.30      0.46        40\n",
      "      surprise       0.74      0.84      0.78       147\n",
      "neutral_speech       0.80      0.89      0.84       381\n",
      " speech_action       0.84      0.84      0.84        31\n",
      "\n",
      "      accuracy                           0.80      1235\n",
      "     macro avg       0.83      0.75      0.77      1235\n",
      "  weighted avg       0.81      0.80      0.80      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral â†’ neutral_speech: 23 instances\n",
      "  - sadness â†’ neutral_speech: 18 instances\n",
      "  - happiness â†’ neutral_speech: 16 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.1384\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - disgust: entropy = 1.5271\n",
      "  - sadness: entropy = 1.4696\n",
      "  - questioning: entropy = 1.4484\n",
      "  - contempt: entropy = 1.3496\n",
      "  - anger: entropy = 1.2608\n",
      "  - neutral: entropy = 1.2604\n",
      "  - speech_action: entropy = 1.2403\n",
      "  - surprise: entropy = 1.1813\n",
      "  - neutral_speech: entropy = 0.9816\n",
      "  - happiness: entropy = 0.9771\n",
      "  - fear: entropy = 0.9373\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.88      0.72      0.79        85\n",
      "      contempt       0.81      0.77      0.79        60\n",
      "       disgust       0.89      0.92      0.91        26\n",
      "          fear       0.93      0.90      0.91        71\n",
      "     happiness       0.95      0.68      0.79       167\n",
      "       neutral       0.71      0.87      0.78       135\n",
      "   questioning       0.74      0.86      0.79        92\n",
      "       sadness       0.86      0.62      0.72        40\n",
      "      surprise       0.85      0.78      0.82       147\n",
      "neutral_speech       0.83      0.93      0.88       381\n",
      " speech_action       0.93      0.84      0.88        31\n",
      "\n",
      "      accuracy                           0.83      1235\n",
      "     macro avg       0.85      0.81      0.82      1235\n",
      "  weighted avg       0.84      0.83      0.83      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness â†’ neutral_speech: 35 instances\n",
      "  - surprise â†’ neutral: 22 instances\n",
      "  - anger â†’ neutral: 11 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.0178\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 1.3485\n",
      "  - anger: entropy = 1.2876\n",
      "  - contempt: entropy = 1.2294\n",
      "  - surprise: entropy = 1.1853\n",
      "  - questioning: entropy = 1.1378\n",
      "  - neutral: entropy = 1.1324\n",
      "  - disgust: entropy = 1.1242\n",
      "  - speech_action: entropy = 1.0846\n",
      "  - happiness: entropy = 0.9623\n",
      "  - neutral_speech: entropy = 0.8113\n",
      "  - fear: entropy = 0.7798\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.74      0.85      0.79        85\n",
      "      contempt       0.82      0.82      0.82        60\n",
      "       disgust       0.89      0.96      0.93        26\n",
      "          fear       0.97      0.94      0.96        71\n",
      "     happiness       0.90      0.77      0.83       167\n",
      "       neutral       0.91      0.64      0.75       135\n",
      "   questioning       0.87      0.84      0.85        92\n",
      "       sadness       0.96      0.65      0.78        40\n",
      "      surprise       0.77      0.86      0.81       147\n",
      "neutral_speech       0.84      0.95      0.89       381\n",
      " speech_action       0.77      0.77      0.77        31\n",
      "\n",
      "      accuracy                           0.84      1235\n",
      "     macro avg       0.86      0.82      0.83      1235\n",
      "  weighted avg       0.85      0.84      0.84      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness â†’ neutral_speech: 21 instances\n",
      "  - neutral â†’ neutral_speech: 21 instances\n",
      "  - neutral â†’ surprise: 15 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.9478\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 1.3239\n",
      "  - neutral: entropy = 1.2162\n",
      "  - surprise: entropy = 1.1290\n",
      "  - disgust: entropy = 1.1262\n",
      "  - contempt: entropy = 1.1110\n",
      "  - speech_action: entropy = 1.0428\n",
      "  - anger: entropy = 1.0156\n",
      "  - happiness: entropy = 0.9953\n",
      "  - questioning: entropy = 0.9398\n",
      "  - fear: entropy = 0.7290\n",
      "  - neutral_speech: entropy = 0.7045\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.84      0.78      0.80        85\n",
      "      contempt       0.88      0.77      0.82        60\n",
      "       disgust       0.86      0.92      0.89        26\n",
      "          fear       0.94      0.94      0.94        71\n",
      "     happiness       0.83      0.78      0.80       167\n",
      "       neutral       0.76      0.87      0.81       135\n",
      "   questioning       0.83      0.92      0.88        92\n",
      "       sadness       0.96      0.68      0.79        40\n",
      "      surprise       0.83      0.85      0.84       147\n",
      "neutral_speech       0.89      0.90      0.90       381\n",
      " speech_action       0.88      0.90      0.89        31\n",
      "\n",
      "      accuracy                           0.86      1235\n",
      "     macro avg       0.86      0.85      0.85      1235\n",
      "  weighted avg       0.86      0.86      0.86      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness â†’ neutral_speech: 18 instances\n",
      "  - neutral_speech â†’ happiness: 16 instances\n",
      "  - surprise â†’ neutral: 15 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.8614\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 1.1718\n",
      "  - contempt: entropy = 1.0395\n",
      "  - neutral: entropy = 1.0041\n",
      "  - speech_action: entropy = 0.9843\n",
      "  - anger: entropy = 0.9766\n",
      "  - surprise: entropy = 0.9607\n",
      "  - disgust: entropy = 0.9542\n",
      "  - happiness: entropy = 0.8711\n",
      "  - questioning: entropy = 0.8590\n",
      "  - neutral_speech: entropy = 0.7045\n",
      "  - fear: entropy = 0.6558\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.87      0.84      0.85        85\n",
      "      contempt       0.80      0.80      0.80        60\n",
      "       disgust       0.92      0.88      0.90        26\n",
      "          fear       0.90      0.92      0.91        71\n",
      "     happiness       0.92      0.81      0.86       167\n",
      "       neutral       0.76      0.83      0.79       135\n",
      "   questioning       0.82      0.82      0.82        92\n",
      "       sadness       0.94      0.72      0.82        40\n",
      "      surprise       0.83      0.85      0.84       147\n",
      "neutral_speech       0.88      0.92      0.90       381\n",
      " speech_action       0.96      0.87      0.92        31\n",
      "\n",
      "      accuracy                           0.86      1235\n",
      "     macro avg       0.87      0.84      0.85      1235\n",
      "  weighted avg       0.86      0.86      0.86      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness â†’ neutral_speech: 18 instances\n",
      "  - surprise â†’ neutral: 13 instances\n",
      "  - neutral â†’ surprise: 10 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.8311\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 1.1459\n",
      "  - disgust: entropy = 0.9991\n",
      "  - surprise: entropy = 0.9906\n",
      "  - contempt: entropy = 0.9880\n",
      "  - anger: entropy = 0.9571\n",
      "  - neutral: entropy = 0.9390\n",
      "  - questioning: entropy = 0.8759\n",
      "  - speech_action: entropy = 0.8696\n",
      "  - happiness: entropy = 0.8408\n",
      "  - fear: entropy = 0.6786\n",
      "  - neutral_speech: entropy = 0.6441\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.81      0.81      0.81        85\n",
      "      contempt       0.86      0.80      0.83        60\n",
      "       disgust       0.89      0.92      0.91        26\n",
      "          fear       0.94      0.89      0.91        71\n",
      "     happiness       0.83      0.85      0.84       167\n",
      "       neutral       0.86      0.81      0.83       135\n",
      "   questioning       0.83      0.91      0.87        92\n",
      "       sadness       0.94      0.72      0.82        40\n",
      "      surprise       0.85      0.86      0.85       147\n",
      "neutral_speech       0.88      0.91      0.90       381\n",
      " speech_action       0.86      0.81      0.83        31\n",
      "\n",
      "      accuracy                           0.86      1235\n",
      "     macro avg       0.87      0.84      0.85      1235\n",
      "  weighted avg       0.86      0.86      0.86      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech â†’ happiness: 17 instances\n",
      "  - happiness â†’ neutral_speech: 15 instances\n",
      "  - neutral â†’ neutral_speech: 9 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.8338\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 1.1394\n",
      "  - disgust: entropy = 1.0747\n",
      "  - surprise: entropy = 0.9933\n",
      "  - neutral: entropy = 0.9796\n",
      "  - anger: entropy = 0.9600\n",
      "  - contempt: entropy = 0.9493\n",
      "  - speech_action: entropy = 0.9192\n",
      "  - questioning: entropy = 0.8785\n",
      "  - happiness: entropy = 0.8501\n",
      "  - neutral_speech: entropy = 0.6384\n",
      "  - fear: entropy = 0.6327\n",
      "âŒ› Stage 2 training took: 01:46:55\n",
      "ðŸ’¾ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V33_20251010_122749\n",
      "âœ… emotion_classifier_model saved successfully.\n",
      "\n",
      "âœ… Stage 2 Training Complete.\n",
      "\n",
      "ðŸŽ‰ Hierarchical Training Pipeline Finished Successfully.\n",
      "\n",
      "============================================================\n",
      "  DEPLOYMENT READINESS CHECK\n",
      "============================================================\n",
      "Threshold: F1-Score >= 0.8\n",
      "\n",
      "  - âœ… anger           | F1-Score: 0.81\n",
      "  - âœ… contempt        | F1-Score: 0.83\n",
      "  - âœ… disgust         | F1-Score: 0.91\n",
      "  - âœ… fear            | F1-Score: 0.91\n",
      "  - âœ… happiness       | F1-Score: 0.84\n",
      "  - âœ… neutral         | F1-Score: 0.83\n",
      "  - âœ… questioning     | F1-Score: 0.87\n",
      "  - âœ… sadness         | F1-Score: 0.82\n",
      "  - âœ… surprise        | F1-Score: 0.85\n",
      "  - âœ… neutral_speech  | F1-Score: 0.90\n",
      "  - âœ… speech_action   | F1-Score: 0.83\n",
      "\n",
      " Model meets the minimum F1-score threshold for all classes.\n",
      "\n",
      "============================================================\n",
      "  CALIBRATING STAGE 2 MODEL\n",
      "============================================================\n",
      "âœ… Optimal temperature for Stage 2 model: 0.5628\n",
      "\n",
      "============================================================\n",
      "  RUNNING HIERARCHICAL ENSEMBLE ANALYSIS (current + V32)\n",
      "============================================================\n",
      "Ensemble prediction for Training_959639.jpg: neutral (Confidence: 0.40)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device once for the entire script run.\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # --- Step 1: Execute Training Pipeline ---\n",
    "    # The main function now returns the trained models and processor\n",
    "    model_s1, model_s2, processor = main(device)\n",
    "    \n",
    "    # --- Step 2: Run Post-Training Analysis & Curation ---\n",
    "    if RUN_INFERENCE:\n",
    "        # This function runs the full inference pass and generates logs for review.\n",
    "        # It uses the in-memory models returned from main().\n",
    "        run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "    \n",
    "    # --- Step 3: Run Final Model Checks ---\n",
    "    # Check if the model is ready for \"deployment\" based on F1 scores\n",
    "    stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "    check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "    \n",
    "    # --- Step 4: Calibrate the Stage 2 Model ---\n",
    "    logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "    labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "    \n",
    "    if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  CALIBRATING STAGE 2 MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        logits_s2 = np.load(logits_s2_path)\n",
    "        labels_s2 = np.load(labels_s2_path)\n",
    "        \n",
    "        optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "        print(f\"âœ… Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "        # plot_reliability_diagram(logits_s2, labels_s2, optimal_temp, SAVE_DIR, VERSION, \"Stage2\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Skipping calibration, logits/labels files for Stage 2 not found.\")\n",
    "\n",
    "    # COME BACK LATER TO MAKE DYNAMIC AND AUTOMATED LOADING OF PATH\n",
    "    # --- Step 5: (Hypothetical) Run Ensemble Analysis ---\n",
    "    # Use the saved V32 artifacts as the \"previous\" models for ensembling\n",
    "    v_prev_path = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\"\n",
    "    \n",
    "    if os.path.exists(v_prev_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  RUNNING HIERARCHICAL ENSEMBLE ANALYSIS (current + V32)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the older V32 models for the ensemble\n",
    "        s1_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"relevance_filter_model\")\n",
    "        ).to(device).eval()\n",
    "        s2_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"emotion_classifier_model\")\n",
    "        ).to(device).eval()\n",
    "        \n",
    "        # Use the in-memory models from THIS run (e.g., V33 when you launch it)\n",
    "        # Assumes you have model_s1 and model_s2 already defined in memory\n",
    "        s1_models_ensemble = [model_s1, s1_model_prev]\n",
    "        s2_models_ensemble = [model_s2, s2_model_prev]\n",
    "\n",
    "        # NEW: auto-pick a real image from ANY non-empty predicted-class folder\n",
    "        review_root = os.path.join(v_prev_path, \"review_candidates_by_predicted_class\")\n",
    "        example_image_path = None\n",
    "        if os.path.isdir(review_root):\n",
    "            for cls in os.listdir(review_root):\n",
    "                cls_dir = os.path.join(review_root, cls)\n",
    "                if os.path.isdir(cls_dir):\n",
    "                    imgs = [f for f in os.listdir(cls_dir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\"))]\n",
    "                    if imgs:\n",
    "                        example_image_path = os.path.join(cls_dir, imgs[0])\n",
    "                        break\n",
    "    \n",
    "        if example_image_path and os.path.exists(example_image_path):\n",
    "            prediction, confidence = hierarchical_ensemble_predict(\n",
    "                example_image_path, processor, s1_models_ensemble, s2_models_ensemble, device\n",
    "            )\n",
    "            print(f\"Ensemble prediction for {Path(example_image_path).name}: {prediction} (Confidence: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ Skipping ensemble demo: no example image found under 'review_candidates_by_predicted_class'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeda8df2-65d6-4fee-b9d3-1721204a955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote eval mismatches (indices only): /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V33_20251010_122749/v33_eval_mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, csv\n",
    "\n",
    "labels = np.load(os.path.join(SAVE_DIR, \"labels_eval_Stage2_V33.npy\"))\n",
    "logits = np.load(os.path.join(SAVE_DIR, \"logits_eval_Stage2_V33.npy\"))\n",
    "probs = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "pred = probs.argmax(axis=1)\n",
    "conf = probs.max(axis=1)\n",
    "\n",
    "out_csv = os.path.join(SAVE_DIR, \"v33_eval_mismatches.csv\")\n",
    "with open(out_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"index\", \"true_label\", \"predicted_label\", \"confidence\"])\n",
    "    for i, (t, p, c) in enumerate(zip(labels, pred, conf)):\n",
    "        if t != p:\n",
    "            w.writerow([i, id2label_s2[int(t)], id2label_s2[int(p)], float(c)])\n",
    "\n",
    "print(f\"âœ… Wrote eval mismatches (indices only): {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d43b2f-90be-4e9e-a4ad-7716853c1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SAVE_DIR = /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V33_20251010_122749\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e337192a4748708982a870301d5a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Eval dataset ready: 1235 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1235/1235 [01:11<00:00, 17.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference complete: wrote 1235 rows to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V33_20251010_122749/V33_full_inference_log.csv\n",
      "\n",
      "ðŸ“Š Quick Confusion Summary:\n",
      "predicted_label  anger  contempt  disgust  fear  happiness  neutral  \\\n",
      "true_label                                                            \n",
      "anger              163         0        0     0          0        0   \n",
      "contempt             0       140        0     0          0        3   \n",
      "disgust              0         0       31     1          0        1   \n",
      "fear                 0         0        0   110          0        0   \n",
      "happiness            0         0        0     0         25        0   \n",
      "neutral              0         0        1     0          1       73   \n",
      "neutral_speech       1         0        1     5          3       10   \n",
      "questioning         10         1       27     3          0        7   \n",
      "sadness              0         0        0     0          0        0   \n",
      "speech_action        0         0        0     0          0        0   \n",
      "surprise             1         0        3     5          0        2   \n",
      "\n",
      "predicted_label  neutral_speech  questioning  sadness  speech_action  surprise  \n",
      "true_label                                                                      \n",
      "anger                         0            4        0              0         0  \n",
      "contempt                      2            0        2              0         0  \n",
      "disgust                       1            5        0              1         0  \n",
      "fear                         18            0        1              0         6  \n",
      "happiness                     0            0        0              0         1  \n",
      "neutral                       9            0        0              1         0  \n",
      "neutral_speech               35            3        1              0         1  \n",
      "questioning                   1          254        5             19        54  \n",
      "sadness                       1            5       63              1         1  \n",
      "speech_action                 0            4        0             27         0  \n",
      "surprise                      1           54        0              6        20  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob, torch, pandas as pd, numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# ---------------------------\n",
    "# Config (auto-detect latest V33 folder)\n",
    "# ---------------------------\n",
    "ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "candidates = sorted(glob.glob(os.path.join(ROOT, \"V33_*\")), key=os.path.getmtime, reverse=True)\n",
    "SAVE_DIR = candidates[0] if candidates else \"/ABSOLUTE/PATH/TO/V33_YYYYMMDD_HHMMSS\"\n",
    "print(f\"Using SAVE_DIR = {SAVE_DIR}\")\n",
    "\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "prepared_data_path = os.path.join(ROOT, \"prepared_datasets\")\n",
    "stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "device = torch.device(\"cpu\")  # keep CPU for stability\n",
    "\n",
    "RELEVANT_CLASSES = [\n",
    "    'neutral', 'neutral_speech', 'happiness', 'sadness', 'anger',\n",
    "    'fear', 'surprise', 'disgust', 'contempt', 'questioning', 'speech_action'\n",
    "]\n",
    "label2id_s2 = {n:i for i,n in enumerate(RELEVANT_CLASSES)}\n",
    "id2label_s2 = {i:n for n,i in label2id_s2.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def ensure_rgb(img):\n",
    "    \"\"\"\n",
    "    Normalize any image to a proper RGB PIL.Image.\n",
    "    Handles grayscale (H,W), channel-first (C,H,W), and palette images.\n",
    "    \"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        # Convert any PIL mode ('L','P','LA','RGBA', etc.) to 'RGB'\n",
    "        return img.convert(\"RGB\")\n",
    "    # If it's a numpy array\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:  # grayscale HxW\n",
    "        arr = np.stack([arr]*3, axis=-1)\n",
    "    elif arr.ndim == 3:\n",
    "        # If channel-first (C,H,W), move to (H,W,C)\n",
    "        if arr.shape[0] in (1, 3) and arr.shape[-1] not in (1, 3):\n",
    "            arr = np.transpose(arr, (1, 2, 0))\n",
    "        # If single-channel HxWx1, tile to 3 channels\n",
    "        if arr.shape[-1] == 1:\n",
    "            arr = np.repeat(arr, 3, axis=-1)\n",
    "        # If more than 3 channels (e.g., RGBA), drop alpha\n",
    "        if arr.shape[-1] > 3:\n",
    "            arr = arr[..., :3]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported image ndim: {arr.ndim}\")\n",
    "    # Ensure uint8 range for PIL\n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(arr, mode=\"RGB\")\n",
    "\n",
    "# ---------------------------\n",
    "# Rehydrate model + eval set\n",
    "# ---------------------------\n",
    "s2_checkpoint_path = os.path.join(SAVE_DIR, \"emotion_classifier_model\")\n",
    "processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "model_s2 = ViTForImageClassification.from_pretrained(\n",
    "    s2_checkpoint_path,\n",
    "    num_labels=len(RELEVANT_CLASSES),\n",
    "    label2id=label2id_s2,\n",
    "    id2label=id2label_s2\n",
    ").to(device).eval()\n",
    "\n",
    "dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split=\"train\").train_test_split(test_size=0.2, seed=42)\n",
    "eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "print(f\"âœ… Eval dataset ready: {len(eval_dataset_s2)} samples\")\n",
    "\n",
    "# ---------------------------\n",
    "# Eval-only inference â†’ CSV\n",
    "# ---------------------------\n",
    "rows = []\n",
    "for ex in tqdm(eval_dataset_s2, desc=\"Evaluating\"):\n",
    "    raw_img = ex[\"image\"]\n",
    "    img = ensure_rgb(raw_img)  # <-- critical fix\n",
    "    true_id = int(ex[\"label\"])\n",
    "    true_label = id2label_s2[true_id]\n",
    "\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model_s2(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1).squeeze(0)\n",
    "        pred_id = int(torch.argmax(probs).item())\n",
    "        conf = float(probs[pred_id].item())\n",
    "\n",
    "    rows.append({\n",
    "        \"filepath\": getattr(raw_img, \"filename\", \"N/A\"),\n",
    "        \"true_label\": true_label,\n",
    "        \"predicted_label\": id2label_s2[pred_id],\n",
    "        \"confidence\": round(conf, 4)\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "out_csv = os.path.join(SAVE_DIR, \"V33_full_inference_log.csv\")\n",
    "df_results.to_csv(out_csv, index=False)\n",
    "print(f\"âœ… Inference complete: wrote {len(df_results)} rows to {out_csv}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Mini confusion summary\n",
    "# ---------------------------\n",
    "summary = df_results.groupby([\"true_label\", \"predicted_label\"]).size().unstack(fill_value=0)\n",
    "print(\"\\nðŸ“Š Quick Confusion Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d983367-2870-4413-b59e-66d0595d9f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
