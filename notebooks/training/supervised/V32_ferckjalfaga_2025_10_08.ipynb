{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# V32 changes:\n",
    "    # overview: Rolled back the failed fine-tuning and augmentation experiments from V31 to\n",
    "    #   restore the superior performance baseline established by V30. This version\n",
    "    #   focuses on correcting the specific regressions in accuracy and F1-score by\n",
    "    #   reverting to a simpler, more robust training strategy.\n",
    "    # section #4 - Removed the experimental discriminative learning rate and layer-freezing\n",
    "    #   strategy for the Stage 1 model. Reverted to V30's simpler approach of\n",
    "    #   using a single, uniform learning rate (3e-5) managed by the default\n",
    "    #   Trainer optimizer.\n",
    "    # section #4 - Corrected the counterproductive augmentation strategy by removing 'sadness'\n",
    "    #   and 'speech_action' from the `minority_classes_s2` list. This prevents\n",
    "    #   aggressive `RandAugment` from being applied to classes whose subtle features\n",
    "    #   were being distorted.\n",
    "    # section #4 - Removed the experimental discriminative learning rate and layer-freezing\n",
    "    #   strategy for the Stage 2 model. Reverted to V30's simpler approach\n",
    "    #   of using a single, uniform learning rate (4e-5) for all layers.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dynamically loading latest checkpoint: V30_20251007_075715\n",
      "ðŸ“ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- ðŸ“‚ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- âš™ï¸ Run Configuration ---\n",
    "# Set to True to run the hierarchical inference pipeline on the full dataset after training is complete.\n",
    "RUN_INFERENCE = True\n",
    "# Set to True on the first run to copy and organize files. Set to False on subsequent runs to save time.\n",
    "PREPARE_DATASETS = True\n",
    "\n",
    "# Finds the most recent V* model directory based on modification time.\n",
    "def find_latest_checkpoint(root_dir):\n",
    "    all_run_dirs = [\n",
    "        os.path.join(root_dir, d)\n",
    "        for d in os.listdir(root_dir)\n",
    "        if d.startswith(\"V\") and os.path.isdir(os.path.join(root_dir, d))\n",
    "    ]\n",
    "    if not all_run_dirs:\n",
    "        return None\n",
    "\n",
    "    # Sort directories by modification time, newest first\n",
    "    sorted_dirs = sorted(all_run_dirs, key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    # The newest directory is the current run's empty folder.\n",
    "    # We need the second newest, which is the latest *completed* run.\n",
    "    if len(sorted_dirs) > 1:\n",
    "        return sorted_dirs[1] # <-- Return the second item in the list\n",
    "    else:\n",
    "        # If there's only one (or zero), no previous checkpoint exists\n",
    "        return None\n",
    "\n",
    "# --- ðŸ¤– Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Dynamically find the latest checkpoint to train from\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = latest_checkpoint\n",
    "    print(f\"âœ… Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    # If no checkpoint is found, fall back to the base model from Hugging Face\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(f\"âš ï¸ No local checkpoint found. Starting from base model: {BASE_MODEL_NAME}\")\n",
    "    \n",
    "# --- ðŸ·ï¸ Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# --- ðŸ–¼ï¸ File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- ðŸ”¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"ðŸ—‚ï¸ Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\nâœ… Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# ðŸ“¦ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self, processor, augment_dict):\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = T.Compose([\n",
    "            T.RandomResizedCrop(size=(224, 224)), # <-- Use this instead of T.Resize\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "        ])\n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            # Select the correct augmentation pipeline, default to base_augment\n",
    "            aug_pipeline = self.augment_dict.get(label, self.base_augment)\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "            augmented_image = aug_pipeline(rgb_image)\n",
    "            processed_images.append(augmented_image)\n",
    "\n",
    "        batch = self.processor(\n",
    "            images=processed_images,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# ðŸ‹ï¸ Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ðŸ”„ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# ðŸ“Š Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(eval_pred, label_names, stage_name=\"\"):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw logits/labels for later analysis like temperature scaling\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in label_names]\n",
    "    recalls = [report[name][\"recall\"] for name in label_names]\n",
    "    precisions = [report[name][\"precision\"] for name in label_names]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    \n",
    "    # Create a dictionary for entropies in the correct order for the CSV\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    # Access the trainer instance through its global-like availability during compute_metrics call\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names}\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "    # --- End Re-integration ---\n",
    "\n",
    "    # Generate and save a heatmap of the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((label_names[i], label_names[j]), cm[i][j])\n",
    "        for i in range(len(label_names))\n",
    "        for j in range(len(label_names)) if i != j and cm[i][j] > 0\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} â†’ {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute and print entropy metrics\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nðŸ§  Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nðŸ” Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "    # --- End Re-integration ---\n",
    "    \n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# ðŸ’¾ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"ðŸ’¾ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"âœ… {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"âš ï¸ Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - âŒ {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - âœ… {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # --- Define specific model paths from the latest checkpoint ---\n",
    "    s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "    s2_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"emotion_classifier_model\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"âœ… Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        s1_checkpoint_path, # <-- Use the specific path for the Stage 1 model\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"âš–ï¸ Stage 1 Class Weights: {class_weights_s1}\")\n",
    "\n",
    "    # --- Define Early Stopping ---\n",
    "    # Stops training if validation loss doesn't improve for 2 consecutive epochs\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    # The complex discriminative learning rate and layer freezing strategy in \n",
    "        # V31 caused a severe performance drop. This change reverts Stage 1 to \n",
    "        # V30's simpler and more effective approach of using a single, uniform \n",
    "        # learning rate for the entire model, which is managed by the Hugging \n",
    "        # Face Trainer's default optimizer.\n",
    "    training_args_s1.learning_rate = 3e-5 # Set learning rate directly\n",
    "    \n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=list(id2label_s1.values()), stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict={}), # Use base augmentation for all\n",
    "        class_weights=class_weights_s1, # Pass weights to the trainer\n",
    "        callbacks=[early_stop_callback] # Keep early stopping\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"ðŸš€ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"âŒ› Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\nâœ… Stage 1 Training Complete.\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(sorted(train_dataset_s2['label'])))\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        s2_checkpoint_path, # <-- Use the specific path for the Stage 2 model\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=9),\n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "\n",
    "    # The addition of 'sadness' and 'speech_action' to the heavy augmentation pipeline in V31 \n",
    "        # was counterproductive, causing the F1-scores for these classes to collapse. \n",
    "        # This change reverts the list to the V30 definition, removing the aggressive \n",
    "        # augmentation from the classes it harmed.\n",
    "    minority_classes_s2 = [label2id_s2[name] for name in ['disgust', 'questioning', 'contempt', 'fear']]\n",
    "    minority_augment_map_s2 = {label_id: minority_aug for label_id in minority_classes_s2}\n",
    "\n",
    "    # Use the custom loss function to turn off label smoothing for historically difficult classes.\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,\n",
    "        target_class_names=['contempt', 'disgust'],\n",
    "        label2id_map=label2id_s2\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True, \n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # As with Stage 1, the complex fine-tuning strategy implemented in V31 failed. \n",
    "        # This change reverts the Stage 2 training process to V30's more effective \n",
    "        # uniform learning rate strategy to restore model performance.\n",
    "    training_args_s2.learning_rate = 4e-5 # Set learning rate directly\n",
    "\n",
    "    # Use the CustomLossTrainer again, passing the targeted loss function.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=RELEVANT_CLASSES, stage_name=\"Stage2\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict=minority_augment_map_s2),\n",
    "        loss_fct=loss_fct_s2, # Pass custom loss function\n",
    "        callbacks=[early_stop_callback] # Keep early stopping\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"ðŸš€ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"âŒ› Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\nâœ… Stage 2 Training Complete.\")\n",
    "    print(\"\\nðŸŽ‰ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "    \n",
    "    # Return the trained models and processor to be used by analysis functions\n",
    "    return trainer_s1.model, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for new images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"ðŸ”¬ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # --- Stage 1 Prediction: Is the image relevant? ---\n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits\n",
    "            preds_s1 = torch.argmax(logits_s1, dim=-1)\n",
    "\n",
    "        # Create a mask of images that were classified as 'relevant'\n",
    "        relevant_mask = (preds_s1 == label2id_s1['relevant'])\n",
    "\n",
    "        # --- Stage 2 Prediction (only on relevant images) ---\n",
    "        if relevant_mask.any():\n",
    "            # Filter the input tensors to only include the relevant images\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "\n",
    "        # --- Aggregate Results ---\n",
    "        # Loop through the original batch and assign the correct prediction\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            if relevant_mask[j]:\n",
    "                # If relevant, get the prediction from the Stage 2 model\n",
    "                pred_label = id2label_s2[preds_s2[s2_idx].item()]\n",
    "                confidence = confs_s2[s2_idx].item()\n",
    "                s2_idx += 1\n",
    "            else:\n",
    "                # If not relevant, label it and stop\n",
    "                pred_label = \"irrelevant\"\n",
    "                confidence = torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item()\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\nâœ… Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = 0.85\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"ðŸ“‚ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs ---\n",
    "    # Find images where the model was confused between specific, problematic classes\n",
    "    confusion_pairs_to_mine = [('contempt', 'questioning'), ('contempt', 'neutral'), ('fear', 'surprise')]\n",
    "    \n",
    "    print(\"\\nâ›ï¸  Mining for hard negative confusion pairs...\")\n",
    "    for pair in confusion_pairs_to_mine:\n",
    "        c1, c2 = pair\n",
    "        # Find images where true is c1 but predicted is c2, OR true is c2 and predicted is c1\n",
    "        mask = ((df['true_label'] == c1) & (df['prediction'] == c2)) | \\\n",
    "               ((df['true_label'] == c2) & (df['prediction'] == c1))\n",
    "        \n",
    "        hard_negatives = df[mask]\n",
    "        \n",
    "        if not hard_negatives.empty:\n",
    "            out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "            hard_negatives.to_csv(out_path, index=False)\n",
    "            print(f\"  - Found {len(hard_negatives)} hard negatives for {pair}. Saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"ðŸ“Š Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ–¥ï¸ Using device: cpu\n",
      "ðŸ—‚ï¸ Preparing hierarchical datasets at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/prepared_datasets\n",
      "\n",
      "--- Creating Stage 1 Dataset ---\n",
      "Processing 'irrelevant' classes...\n",
      "  Recursively copying from 'hard_case'...\n",
      "Processing 'relevant' classes...\n",
      "  Recursively copying from 'anger'...\n",
      "  Recursively copying from 'contempt'...\n",
      "  Recursively copying from 'disgust'...\n",
      "  Recursively copying from 'fear'...\n",
      "  Recursively copying from 'happiness'...\n",
      "  Recursively copying from 'neutral'...\n",
      "  Recursively copying from 'questioning'...\n",
      "  Recursively copying from 'sadness'...\n",
      "  Recursively copying from 'surprise'...\n",
      "  Recursively copying from 'neutral_speech'...\n",
      "  Recursively copying from 'speech_action'...\n",
      "\n",
      "--- Creating Stage 2 Dataset ---\n",
      "  Copying 'anger' to Stage 2 directory...\n",
      "  Copying 'contempt' to Stage 2 directory...\n",
      "  Copying 'disgust' to Stage 2 directory...\n",
      "  Copying 'fear' to Stage 2 directory...\n",
      "  Copying 'happiness' to Stage 2 directory...\n",
      "  Copying 'neutral' to Stage 2 directory...\n",
      "  Copying 'questioning' to Stage 2 directory...\n",
      "  Copying 'sadness' to Stage 2 directory...\n",
      "  Copying 'surprise' to Stage 2 directory...\n",
      "  Copying 'neutral_speech' to Stage 2 directory...\n",
      "  Copying 'speech_action' to Stage 2 directory...\n",
      "\n",
      "âœ… Hierarchical dataset preparation complete.\n",
      "\n",
      "============================================================\n",
      "  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04a4af184f54c7381fecf4125a4689f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599b2521c8d948639d037727dd28a6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: 21504 training samples, 5377 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions_v5/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Stage 1 Class Weights: tensor([0.6492, 2.1761])\n",
      "ðŸš€ Starting Stage 1 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6720' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6720/6720 5:58:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.447000</td>\n",
       "      <td>0.541452</td>\n",
       "      <td>0.775339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.513679</td>\n",
       "      <td>0.808443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.480882</td>\n",
       "      <td>0.777385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.495220</td>\n",
       "      <td>0.798215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.495406</td>\n",
       "      <td>0.797843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.89      0.80      0.85      4132\n",
      "    relevant       0.51      0.68      0.58      1245\n",
      "\n",
      "    accuracy                           0.78      5377\n",
      "   macro avg       0.70      0.74      0.71      5377\n",
      "weighted avg       0.80      0.78      0.79      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 809 instances\n",
      "  - relevant â†’ irrelevant: 399 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3973\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4063\n",
      "  - irrelevant: entropy = 0.3946\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.88      0.87      0.87      4132\n",
      "    relevant       0.58      0.61      0.60      1245\n",
      "\n",
      "    accuracy                           0.81      5377\n",
      "   macro avg       0.73      0.74      0.74      5377\n",
      "weighted avg       0.81      0.81      0.81      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 544 instances\n",
      "  - relevant â†’ irrelevant: 486 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3963\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4359\n",
      "  - irrelevant: entropy = 0.3843\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.90      0.79      0.85      4132\n",
      "    relevant       0.51      0.72      0.60      1245\n",
      "\n",
      "    accuracy                           0.78      5377\n",
      "   macro avg       0.71      0.76      0.72      5377\n",
      "weighted avg       0.81      0.78      0.79      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 848 instances\n",
      "  - relevant â†’ irrelevant: 349 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.4281\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4331\n",
      "  - irrelevant: entropy = 0.4267\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.90      0.83      0.86      4132\n",
      "    relevant       0.55      0.69      0.61      1245\n",
      "\n",
      "    accuracy                           0.80      5377\n",
      "   macro avg       0.73      0.76      0.74      5377\n",
      "weighted avg       0.82      0.80      0.81      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 700 instances\n",
      "  - relevant â†’ irrelevant: 385 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3674\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.4113\n",
      "  - irrelevant: entropy = 0.3541\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.90      0.83      0.86      4132\n",
      "    relevant       0.55      0.68      0.61      1245\n",
      "\n",
      "    accuracy                           0.80      5377\n",
      "   macro avg       0.72      0.76      0.74      5377\n",
      "weighted avg       0.82      0.80      0.80      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 686 instances\n",
      "  - relevant â†’ irrelevant: 401 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3628\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.3999\n",
      "  - irrelevant: entropy = 0.3517\n",
      "âŒ› Stage 1 training took: 05:58:31\n",
      "ðŸ’¾ Saving relevance_filter_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\n",
      "âœ… relevance_filter_model saved successfully.\n",
      "\n",
      "âœ… Stage 1 Training Complete.\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db18f98066bd4658b884a16e6e912fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474e833cbf0b43a887f9c9c386f499b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n",
      "ðŸš€ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1854' max='3090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1854/3090 1:30:16 < 1:00:14, 0.34 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>0.796284</td>\n",
       "      <td>0.821053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505700</td>\n",
       "      <td>0.910632</td>\n",
       "      <td>0.806478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.859763</td>\n",
       "      <td>0.808097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.81      0.82      0.82        85\n",
      "      contempt       0.88      0.85      0.86        60\n",
      "       disgust       0.88      0.81      0.84        26\n",
      "          fear       0.94      0.90      0.92        71\n",
      "     happiness       0.86      0.80      0.83       167\n",
      "       neutral       0.70      0.82      0.76       135\n",
      "   questioning       0.82      0.87      0.85        92\n",
      "       sadness       0.52      0.42      0.47        40\n",
      "      surprise       0.88      0.78      0.82       147\n",
      "neutral_speech       0.84      0.88      0.86       381\n",
      " speech_action       0.62      0.58      0.60        31\n",
      "\n",
      "      accuracy                           0.82      1235\n",
      "     macro avg       0.80      0.78      0.78      1235\n",
      "  weighted avg       0.82      0.82      0.82      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness â†’ neutral_speech: 20 instances\n",
      "  - surprise â†’ neutral: 14 instances\n",
      "  - neutral_speech â†’ sadness: 13 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.5532\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 0.8569\n",
      "  - speech_action: entropy = 0.7307\n",
      "  - neutral: entropy = 0.7038\n",
      "  - surprise: entropy = 0.6196\n",
      "  - anger: entropy = 0.5607\n",
      "  - disgust: entropy = 0.5432\n",
      "  - neutral_speech: entropy = 0.5374\n",
      "  - happiness: entropy = 0.5084\n",
      "  - questioning: entropy = 0.4833\n",
      "  - fear: entropy = 0.4111\n",
      "  - contempt: entropy = 0.2515\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.74      0.85      0.79        85\n",
      "      contempt       0.82      0.83      0.83        60\n",
      "       disgust       0.82      0.88      0.85        26\n",
      "          fear       0.95      0.89      0.92        71\n",
      "     happiness       0.86      0.79      0.82       167\n",
      "       neutral       0.71      0.64      0.67       135\n",
      "   questioning       0.81      0.89      0.85        92\n",
      "       sadness       0.63      0.30      0.41        40\n",
      "      surprise       0.80      0.75      0.77       147\n",
      "neutral_speech       0.82      0.91      0.86       381\n",
      " speech_action       0.68      0.55      0.61        31\n",
      "\n",
      "      accuracy                           0.81      1235\n",
      "     macro avg       0.79      0.75      0.76      1235\n",
      "  weighted avg       0.80      0.81      0.80      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral â†’ neutral_speech: 22 instances\n",
      "  - sadness â†’ neutral_speech: 19 instances\n",
      "  - happiness â†’ neutral_speech: 16 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.4707\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 0.7376\n",
      "  - neutral: entropy = 0.6179\n",
      "  - speech_action: entropy = 0.6059\n",
      "  - surprise: entropy = 0.5434\n",
      "  - happiness: entropy = 0.4950\n",
      "  - anger: entropy = 0.4761\n",
      "  - neutral_speech: entropy = 0.4315\n",
      "  - questioning: entropy = 0.3867\n",
      "  - fear: entropy = 0.3623\n",
      "  - contempt: entropy = 0.2581\n",
      "  - disgust: entropy = 0.2102\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.78      0.76      0.77        85\n",
      "      contempt       0.84      0.77      0.80        60\n",
      "       disgust       0.89      0.92      0.91        26\n",
      "          fear       0.91      0.89      0.90        71\n",
      "     happiness       0.84      0.81      0.82       167\n",
      "       neutral       0.74      0.79      0.77       135\n",
      "   questioning       0.83      0.78      0.80        92\n",
      "       sadness       0.52      0.38      0.43        40\n",
      "      surprise       0.77      0.85      0.81       147\n",
      "neutral_speech       0.84      0.87      0.85       381\n",
      " speech_action       0.67      0.45      0.54        31\n",
      "\n",
      "      accuracy                           0.81      1235\n",
      "     macro avg       0.78      0.75      0.76      1235\n",
      "  weighted avg       0.81      0.81      0.81      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech â†’ happiness: 15 instances\n",
      "  - sadness â†’ neutral_speech: 13 instances\n",
      "  - happiness â†’ neutral_speech: 12 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.4886\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 0.7686\n",
      "  - speech_action: entropy = 0.6447\n",
      "  - neutral: entropy = 0.6116\n",
      "  - anger: entropy = 0.5210\n",
      "  - surprise: entropy = 0.4942\n",
      "  - happiness: entropy = 0.4809\n",
      "  - neutral_speech: entropy = 0.4606\n",
      "  - questioning: entropy = 0.4535\n",
      "  - fear: entropy = 0.4500\n",
      "  - disgust: entropy = 0.3074\n",
      "  - contempt: entropy = 0.2624\n",
      "âŒ› Stage 2 training took: 01:30:17\n",
      "ðŸ’¾ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\n",
      "âœ… emotion_classifier_model saved successfully.\n",
      "\n",
      "âœ… Stage 2 Training Complete.\n",
      "\n",
      "ðŸŽ‰ Hierarchical Training Pipeline Finished Successfully.\n",
      "\n",
      "============================================================\n",
      "  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\n",
      "============================================================\n",
      "Found 26902 images to process for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Running Hierarchical Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 841/841 [33:40<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Full inference log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/V32_full_inference_log.csv\n",
      "\n",
      "Found 11382 images below 0.85 confidence for review.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting review images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11382/11382 [00:03<00:00, 3349.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Sorted review images into folders at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/review_candidates_by_predicted_class\n",
      "\n",
      "â›ï¸  Mining for hard negative confusion pairs...\n",
      "  - Found 7 hard negatives for ('contempt', 'questioning'). Saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/hard_negatives_contempt_vs_questioning.csv\n",
      "  - Found 116 hard negatives for ('contempt', 'neutral'). Saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/hard_negatives_contempt_vs_neutral.csv\n",
      "  - Found 5 hard negatives for ('fear', 'surprise'). Saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/hard_negatives_fear_vs_surprise.csv\n",
      "\n",
      "============================================================\n",
      "  DEPLOYMENT READINESS CHECK\n",
      "============================================================\n",
      "Threshold: F1-Score >= 0.8\n",
      "\n",
      "  - âŒ anger           | F1-Score: 0.77 (Below Threshold)\n",
      "  - âœ… contempt        | F1-Score: 0.80\n",
      "  - âœ… disgust         | F1-Score: 0.91\n",
      "  - âœ… fear            | F1-Score: 0.90\n",
      "  - âœ… happiness       | F1-Score: 0.82\n",
      "  - âŒ neutral         | F1-Score: 0.77 (Below Threshold)\n",
      "  - âœ… questioning     | F1-Score: 0.80\n",
      "  - âŒ sadness         | F1-Score: 0.43 (Below Threshold)\n",
      "  - âœ… surprise        | F1-Score: 0.81\n",
      "  - âœ… neutral_speech  | F1-Score: 0.85\n",
      "  - âŒ speech_action   | F1-Score: 0.54 (Below Threshold)\n",
      "\n",
      " Model is NOT ready for production.\n",
      "\n",
      "============================================================\n",
      "  CALIBRATING STAGE 2 MODEL\n",
      "============================================================\n",
      "âœ… Optimal temperature for Stage 2 model: 1.2980\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device once for the entire script run.\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # --- Step 1: Execute Training Pipeline ---\n",
    "    # The main function now returns the trained models and processor\n",
    "    model_s1, model_s2, processor = main(device)\n",
    "    \n",
    "    # --- Step 2: Run Post-Training Analysis & Curation ---\n",
    "    if RUN_INFERENCE:\n",
    "        # This function runs the full inference pass and generates logs for review.\n",
    "        # It uses the in-memory models returned from main().\n",
    "        run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "    \n",
    "    # --- Step 3: Run Final Model Checks ---\n",
    "    # Check if the model is ready for \"deployment\" based on F1 scores\n",
    "    stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "    check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "    \n",
    "    # --- Step 4: Calibrate the Stage 2 Model ---\n",
    "    logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "    labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "    \n",
    "    if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  CALIBRATING STAGE 2 MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        logits_s2 = np.load(logits_s2_path)\n",
    "        labels_s2 = np.load(labels_s2_path)\n",
    "        \n",
    "        optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "        print(f\"âœ… Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "        # plot_reliability_diagram(logits_s2, labels_s2, optimal_temp, SAVE_DIR, VERSION, \"Stage2\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Skipping calibration, logits/labels files for Stage 2 not found.\")\n",
    "\n",
    "    # --- Step 5: (Hypothetical) Run Ensemble Analysis ---\n",
    "    # This is a hypothetical example assuming a V30 model has been trained\n",
    "    v30_path = \"/path/to/your/V30_run_folder\" \n",
    "    \n",
    "    if os.path.exists(v30_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  RUNNING HIERARCHICAL ENSEMBLE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the older V30 models for the ensemble\n",
    "        s1_model_v30 = AutoModelForImageClassification.from_pretrained(os.path.join(v30_path, \"relevance_filter_model\")).to(device).eval()\n",
    "        s2_model_v30 = AutoModelForImageClassification.from_pretrained(os.path.join(v30_path, \"emotion_classifier_model\")).to(device).eval()\n",
    "        \n",
    "        # Use the in-memory V31 models (model_s1, model_s2) from the current run\n",
    "        s1_models_ensemble = [model_s1, s1_model_v30]\n",
    "        s2_models_ensemble = [model_s2, s2_model_v30]\n",
    "        \n",
    "        example_image_path = \"path/to/a/difficult/image.jpg\"\n",
    "        if os.path.exists(example_image_path):\n",
    "            prediction, confidence = hierarchical_ensemble_predict(example_image_path, processor, s1_models_ensemble, s2_models_ensemble, device)\n",
    "            print(f\"Ensemble prediction for {Path(example_image_path).name}: {prediction} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e00a50-b414-4e05-af34-ced5ab009ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote eval mismatches (indices only): /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114/v33_eval_mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, csv\n",
    "\n",
    "labels = np.load(os.path.join(SAVE_DIR, \"labels_eval_Stage2_V32.npy\"))\n",
    "logits = np.load(os.path.join(SAVE_DIR, \"logits_eval_Stage2_V32.npy\"))\n",
    "probs = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "pred = probs.argmax(axis=1)\n",
    "conf = probs.max(axis=1)\n",
    "\n",
    "out_csv = os.path.join(SAVE_DIR, \"v33_eval_mismatches.csv\")\n",
    "with open(out_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"index\", \"true_label\", \"predicted_label\", \"confidence\"])\n",
    "    for i, (t, p, c) in enumerate(zip(labels, pred, conf)):\n",
    "        if t != p:\n",
    "            w.writerow([i, id2label_s2[int(t)], id2label_s2[int(p)], float(c)])\n",
    "\n",
    "print(f\"âœ… Wrote eval mismatches (indices only): {out_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a0c25-8001-4bbe-b34f-b2d0a17e6d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
