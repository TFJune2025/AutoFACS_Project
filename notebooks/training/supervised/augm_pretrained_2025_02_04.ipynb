{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd53901-808d-444c-819b-f7b21d2e99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, applications, optimizers, callbacks\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from vit_keras import vit  # Ensure you have installed vit-keras\n",
    "\n",
    "# Global configuration\n",
    "IMG_SIZE = 224         # Use 224x224 resolution for transfer learning\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 7        # Update if you merge datasets with a different number of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f63968c-92bf-4e34-be23-ebc609125baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. DATA LOADING & PREPROCESSING FUNCTIONS\n",
    "#    (Reworking the AffectNet and FER2013 pipelines)\n",
    "# =============================================================================\n",
    "def load_and_preprocess_image(path, target_size=(IMG_SIZE, IMG_SIZE)):\n",
    "    \"\"\"\n",
    "    Loads an image from disk, converts grayscale images to RGB,\n",
    "    resizes to target_size, and applies EfficientNet preprocessing.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Unable to load image at: {path}\")\n",
    "    # If image is grayscale, convert to RGB\n",
    "    if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    # Use the EfficientNet preprocessing (assumes model input is in range [-1,1])\n",
    "    img = applications.efficientnet.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def load_dataset_from_directory(root_dir, extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "    \"\"\"\n",
    "    Loads images from a directory whose subdirectories are emotion labels.\n",
    "    Converts images to 224x224, 3-channel format.\n",
    "    Returns NumPy arrays for images and integer labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    # Assume each subdirectory is named with the emotion label\n",
    "    classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(root_dir, cls)\n",
    "        for file in os.listdir(cls_dir):\n",
    "            if file.lower().endswith(extensions):\n",
    "                try:\n",
    "                    img = load_and_preprocess_image(os.path.join(cls_dir, file))\n",
    "                    X.append(img)\n",
    "                    y.append(class_to_idx[cls])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "260f98ea-b65c-4307-b886-7f8e042f2242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAFFE dataset: Loaded 213 samples with 7 unique classes.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. LOAD & PREPROCESS JAFFE DATASET\n",
    "#    (Assumes the dataset is organized into subdirectories named after the 7 emotions)\n",
    "# =============================================================================\n",
    "\n",
    "# Global configuration for image processing\n",
    "IMG_SIZE = 224  # Target size for model input\n",
    "\n",
    "# Define the emotion classes (as expected in the dataset folders)\n",
    "CLASSES = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "# Create a mapping from emotion (folder name) to label index\n",
    "class_to_idx = {emotion: i for i, emotion in enumerate(CLASSES)}\n",
    "\n",
    "# Path to the curated JAFFE dataset\n",
    "jaffe_dir = \"/home/natalyagrokh/img_datasets/jaffe_dataset\"  # Adjust if necessary\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Iterate over each folder in the JAFFE dataset directory\n",
    "for emotion in os.listdir(jaffe_dir):\n",
    "    # Ensure the folder corresponds to one of the expected emotion classes\n",
    "    if emotion.lower() in CLASSES:\n",
    "        emotion_path = os.path.join(jaffe_dir, emotion)\n",
    "        if os.path.isdir(emotion_path):\n",
    "            for img_file in os.listdir(emotion_path):\n",
    "                # Consider common image file extensions\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    img_path = os.path.join(emotion_path, img_file)\n",
    "                    # Read the image in grayscale (JAFFE images are typically grayscale)\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is None:\n",
    "                        continue  # Skip if the image fails to load\n",
    "                    # Resize the image to the target dimensions\n",
    "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                    # Convert the single-channel image to 3 channels by duplicating the grayscale data\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                    # Apply EfficientNet preprocessing (which scales pixel values appropriately)\n",
    "                    img = applications.efficientnet.preprocess_input(img)\n",
    "                    X_list.append(img)\n",
    "                    y_list.append(class_to_idx[emotion.lower()])\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "X_jaffe = np.array(X_list)\n",
    "y_jaffe = np.array(y_list)\n",
    "\n",
    "print(f\"JAFFE dataset: Loaded {X_jaffe.shape[0]} samples with {len(np.unique(y_jaffe))} unique classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84c2206a-7dc5-4422-b35a-c8e72ab1034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 170, Validation samples: 43\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. DATA AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "# Split the JAFFE dataset into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_jaffe, \n",
    "    y_jaffe, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_jaffe\n",
    ")\n",
    "\n",
    "# Create a training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3]\n",
    ")\n",
    "\n",
    "# Create the training generator. We convert the labels to one-hot encoding.\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train,\n",
    "    tf.keras.utils.to_categorical(y_train, NUM_CLASSES),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# (Optional) Create a validation generator without augmentation (only preprocessing)\n",
    "val_datagen = ImageDataGenerator()  # No augmentation here\n",
    "val_generator = val_datagen.flow(\n",
    "    X_val,\n",
    "    tf.keras.utils.to_categorical(y_val, NUM_CLASSES),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3f684c4-b985-4027-997e-596e6b3029cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'weight_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_fn\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# --- Compile Base Model with Focal Loss and AdamW Optimizer ---\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mfocal_loss(), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m lr_schedule \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mschedules\u001b[38;5;241m.\u001b[39mExponentialDecay(\n\u001b[1;32m     41\u001b[0m     initial_learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, decay_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, decay_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/ml_expressions/lib/python3.9/site-packages/typeguard/__init__.py:1031\u001b[0m, in \u001b[0;36mtypechecked.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1031\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[43m_CallMemo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpython_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_localns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     check_argument_types(memo)\n\u001b[1;32m   1033\u001b[0m     retval \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/ml_expressions/lib/python3.9/site-packages/typeguard/__init__.py:198\u001b[0m, in \u001b[0;36m_CallMemo.__init__\u001b[0;34m(self, func, frame_locals, args, kwargs, forward_refs_policy)\u001b[0m\n\u001b[1;32m    195\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marguments \u001b[38;5;241m=\u001b[39m \u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39marguments\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m frame_locals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe must be specified if args or kwargs is None\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/ml_expressions/lib/python3.9/inspect.py:3045\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3041\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3042\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3043\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml_expressions/lib/python3.9/inspect.py:2960\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   2958\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2959\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 2960\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2962\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   2963\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'weight_decay'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. MODEL ARCHITECTURE & IMPROVEMENTS\n",
    "#    (Based on your LATEST CODE with ensemble & teacherâ€“student distillation)\n",
    "# =============================================================================\n",
    "# --- Base Model (EfficientNetB3) ---\n",
    "# Set global constants (ensure these are defined in your notebook)\n",
    "IMG_SIZE = 224         # e.g., 224\n",
    "NUM_CLASSES = 7        # e.g., 7 emotion classes\n",
    "\n",
    "# --- Base Model (EfficientNetB3) ---\n",
    "base_model = applications.EfficientNetB3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "# Freeze early layers\n",
    "for layer in base_model.layers[:150]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build classification head on top of base_model\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(512, activation='swish', kernel_regularizer=l2(0.001))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "# --- Focal Loss Function ---\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        pt = tf.exp(-ce)\n",
    "        return tf.reduce_mean(alpha * (1 - pt) ** gamma * ce)\n",
    "    return loss_fn\n",
    "\n",
    "# --- Compile Base Model with Focal Loss and AdamW Optimizer ---\n",
    "optimizer = AdamW(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss=focal_loss(), metrics=['accuracy'])\n",
    "lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3, decay_steps=1000, decay_rate=0.9\n",
    ")\n",
    "\n",
    "# --- Load Pre-Trained Models for Ensemble (Teacher) ---\n",
    "# (Assuming you have saved models with ~0.600 and ~0.622 accuracy)\n",
    "model1 = tf.keras.models.load_model('/home/natalyagrokh/img_expressions/pre-trained models/final_affectnet_model.keras', compile=False)\n",
    "model2 = tf.keras.models.load_model('/home/natalyagrokh/img_expressions/final_efficientnet_trained_model', compile=False)\n",
    "ensemble_input = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "out1 = model1(ensemble_input)\n",
    "out2 = model2(ensemble_input)\n",
    "ensemble_outputs = layers.Average()([out1, out2])\n",
    "ensemble_model = models.Model(ensemble_input, ensemble_outputs)\n",
    "teacher_model = ensemble_model  # Teacher for distillation\n",
    "\n",
    "# --- Student Model (A Smaller Architecture) ---\n",
    "def build_smaller_model():\n",
    "    base = applications.EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "    )\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "    y_sm = layers.GlobalAveragePooling2D()(base.output)\n",
    "    y_sm = layers.Dense(256, activation='relu')(y_sm)\n",
    "    y_sm = layers.Dropout(0.5)(y_sm)\n",
    "    outputs_sm = layers.Dense(NUM_CLASSES, activation='softmax')(y_sm)\n",
    "    return models.Model(base.input, outputs_sm)\n",
    "\n",
    "student_model = build_smaller_model()\n",
    "\n",
    "# Compile the student model with a combined loss:\n",
    "# 50% categorical crossentropy + 50% KL divergence from teacher predictions\n",
    "student_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: 0.5 * tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "           + 0.5 * tf.keras.losses.KLDivergence()(teacher_model(tf.keras.backend.stop_gradient(y_true)), y_pred),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- (Optional) Integrate a Vision Transformer (ViT) Branch ---\n",
    "vit_model = vit.vit_b16(\n",
    "    image_size=IMG_SIZE,\n",
    "    activation='softmax',\n",
    "    pretrained=True,\n",
    "    include_top=False\n",
    ")\n",
    "# For example, you might combine CNN and ViT features as follows:\n",
    "x_vit = base_model.output\n",
    "x_vit = vit_model(x_vit)\n",
    "x_vit = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x_vit, x_vit)\n",
    "# (This branch is optional and can be merged with your primary branch as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b841d7-723c-4e63-8b76-08895a9f5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. TRAINING & EVALUATION\n",
    "# =============================================================================\n",
    "# Define callbacks\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "checkpoint = callbacks.ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Train the base model (or student model) using the training generator and validation set\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    validation_data=(X_val, tf.keras.utils.to_categorical(y_val, NUM_CLASSES)),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(X_test, tf.keras.utils.to_categorical(y_test, NUM_CLASSES))\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112627df-6b9c-44b6-9fac-f663ed8955a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7. OPTIONAL: VISUALIZATION (e.g., Confusion Matrix, CAM)\n",
    "# =============================================================================\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "ConfusionMatrixDisplay.from_predictions(np.argmax(tf.keras.utils.to_categorical(y_test, NUM_CLASSES), axis=1),\n",
    "                                          np.argmax(y_pred, axis=1))\n",
    "plt.show()\n",
    "\n",
    "# For Class Activation Mapping (CAM), you can extract the penultimate features and dot with final dense weights.\n",
    "gap_weights = model.layers[-1].get_weights()[0]\n",
    "cam_model = models.Model(inputs=model.input, outputs=(model.layers[-3].output, model.output))\n",
    "# For a given image (img_array should be shape (1, IMG_SIZE, IMG_SIZE, 3)):\n",
    "# features, results = cam_model.predict(img_array)\n",
    "# heatmap = np.dot(features, gap_weights)\n",
    "# plt.imshow(heatmap, cmap='jet'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6332bdb-322c-4845-875c-b04389988332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions) (Local)",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
