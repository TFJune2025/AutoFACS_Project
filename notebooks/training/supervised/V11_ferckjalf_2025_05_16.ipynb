{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V11 changes:\n",
    "    # section #5 removed filename-based AU filtering\n",
    "        # added questioning perceptual clustering\n",
    "    # added new section #18 to Review & Relabel 'REVIEW' Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "import accelerate\n",
    "import dill\n",
    "import gc\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, log_loss\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import LBFGS\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalf_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"happiness\",\n",
    "    \"sadness\", \"surprise\", \"neutral\", \"questioning\"\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions\", VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto-loaded model from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V10_20250515_150651\n",
      "üñ•Ô∏è Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. Auto-Load Latest Pretrained Model and Processor\n",
    "# --------------------------\n",
    "\n",
    "# Automatically load latest model path\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions\"\n",
    "# List all version folders in descending order\n",
    "model_dirs = sorted(\n",
    "    [os.path.join(MODEL_ROOT, d) for d in os.listdir(MODEL_ROOT)\n",
    "     if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))],\n",
    "    key=lambda x: os.path.getmtime(x),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Remove the current output version (to avoid loading from empty target)\n",
    "model_dirs = [d for d in model_dirs if VERSION in d or not d.startswith(VERSION)]\n",
    "model_dirs = [d for d in model_dirs if os.path.basename(d).startswith(\"V\") and d != SAVE_DIR]\n",
    "\n",
    "# Pick the most recent complete model (not current output)\n",
    "if len(model_dirs) < 1:\n",
    "    raise FileNotFoundError(\"‚ùå No earlier model folders found.\")\n",
    "model_path = model_dirs[0]\n",
    "print(f\"‚úÖ Auto-loaded model from: {model_path}\")\n",
    "\n",
    "# Load base model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Replace classification head to match current label schema\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, len(id2label))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d22179f8c2449deb22924b36e6e4339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total examples after filtering: 22055\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Load and Prepare Dataset\n",
    "# --------------------------\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=\"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalf_dataset\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/tmp/hf_cache\"\n",
    ")\n",
    "\n",
    "counter = {\"n\": 0}\n",
    "\n",
    "def reconcile_labels(example):\n",
    "    counter[\"n\"] += 1\n",
    "    if counter[\"n\"] % 1000 == 0:\n",
    "        print(f\"Processed {counter['n']} images...\")\n",
    "\n",
    "    label = example.get(\"label\", None)\n",
    "\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = getattr(example[\"image\"], \"filename\", None)\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example\n",
    "\n",
    "# Single-threaded labeling to preserve .filename\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "print(f\"‚úÖ Total examples after filtering: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 2196 examples\n",
      "  disgust: 251 examples\n",
      "  fear: 1314 examples\n",
      "  happiness: 9172 examples\n",
      "  sadness: 1548 examples\n",
      "  surprise: 2571 examples\n",
      "  neutral: 3153 examples\n",
      "  questioning: 1850 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes: ['disgust', 'fear', 'sadness']\n",
      "\n",
      "üìÇ Image count per label folder:\n",
      "  anger: 2196 images\n",
      "  disgust: 251 images\n",
      "  fear: 1314 images\n",
      "  happiness: 9172 images\n",
      "  neutral: 3153 images\n",
      "  questioning: 1850 images\n",
      "  sadness: 1548 images\n",
      "  surprise: 2571 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Disgust hash clusters with more than 1 image:\n",
      "üîç Questioning hash clusters with more than 1 image:\n",
      "  - Cluster 9db42783: 2 images copied for review\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Disgust & Questioning Curation: Perceptual Clustering \n",
    "# --------------------------\n",
    "\n",
    "#Disgust\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Directory to inspect for disgust images\n",
    "disgust_dir = os.path.join(BASE_PATH, \"disgust\")\n",
    "disgust_images = [\n",
    "    os.path.join(disgust_dir, f) for f in os.listdir(disgust_dir)\n",
    "    if is_valid_image(f)\n",
    "]\n",
    "\n",
    "# Compute perceptual hashes for clustering\n",
    "hash_map = {}\n",
    "for path in disgust_images:\n",
    "    h = compute_hash(path)\n",
    "    if h:\n",
    "        hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "# Identify clusters with >1 similar image (potential duplicates or mislabels)\n",
    "cluster_dir = os.path.join(SAVE_DIR, \"disgust_clusters\")\n",
    "os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "print(\"üîç Disgust hash clusters with more than 1 image:\")\n",
    "for h, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        cluster_path = os.path.join(cluster_dir, h)\n",
    "        os.makedirs(cluster_path, exist_ok=True)\n",
    "        for p in paths:\n",
    "            shutil.copy(p, cluster_path)\n",
    "        print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")\n",
    "\n",
    "# Questioning\n",
    "questioning_dir = os.path.join(BASE_PATH, \"questioning\")\n",
    "questioning_images = [\n",
    "    os.path.join(questioning_dir, f) for f in os.listdir(questioning_dir)\n",
    "    if is_valid_image(f)\n",
    "]\n",
    "\n",
    "hash_map_q = {}\n",
    "for path in questioning_images:\n",
    "    h = compute_hash(path)\n",
    "    if h:\n",
    "        hash_map_q.setdefault(h, []).append(path)\n",
    "\n",
    "cluster_dir_q = os.path.join(SAVE_DIR, \"questioning_clusters\")\n",
    "os.makedirs(cluster_dir_q, exist_ok=True)\n",
    "\n",
    "print(\"üîç Questioning hash clusters with more than 1 image:\")\n",
    "for h, paths in hash_map_q.items():\n",
    "    if len(paths) > 1:\n",
    "        cluster_path = os.path.join(cluster_dir_q, h)\n",
    "        os.makedirs(cluster_path, exist_ok=True)\n",
    "        for p in paths:\n",
    "            shutil.copy(p, cluster_path)\n",
    "        print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Targeted minority augmentation will apply to: ['questioning', 'sadness', 'fear', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "label_freqs = Counter(dataset[\"label\"])\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning']\n",
    "\n",
    "# Merge and deduplicate\n",
    "minority_class_names = list(set(minority_by_name + manual_focus_classes))\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"üéØ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eea5509884041ba94c307922b3db229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# Stronger augmentation for minority classes\n",
    "minority_aug = T.Compose([\n",
    "    T.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.8),\n",
    "    T.RandomRotation(30),\n",
    "    T.ColorJitter(0.4, 0.4, 0.4, 0.2),\n",
    "    T.RandomGrayscale(p=0.3),\n",
    "    T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "#factory function that returns another function -> tranform_function\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function\n",
    "\n",
    "#returned transform_function is applied to each dataset example inside .map()\n",
    "#each time the dataset runs transform_function(example), it receives:\n",
    "#inputs = {\n",
    "#     'pixel_values': tensor,\n",
    "#     'labels': label_int\n",
    "# }\n",
    "dataset = dataset.map(make_transform_function(processor, minority_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 8. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e37d1e-a6fb-4e9f-aec5-ee945f11c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Saved label distribution snapshot: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/label_snapshots/V11_label_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Label Distribution Snapshot and Drift Monitor\n",
    "# --------------------------\n",
    "snapshot_dir = os.path.join(SAVE_DIR, \"label_snapshots\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Count current training labels\n",
    "train_label_names = [LABEL_NAMES[i] for i in train_dataset['label']]\n",
    "label_counts = pd.Series(train_label_names).value_counts().sort_index()\n",
    "label_counts.name = VERSION\n",
    "\n",
    "# Save snapshot CSV\n",
    "snapshot_path = os.path.join(snapshot_dir, f\"{VERSION}_label_distribution.csv\")\n",
    "label_counts.to_csv(snapshot_path)\n",
    "print(f\"üìä Saved label distribution snapshot: {snapshot_path}\")\n",
    "\n",
    "# Optionally compare to previous version\n",
    "previous_versions = sorted([\n",
    "    f for f in os.listdir(snapshot_dir) if f.endswith(\".csv\") and not f.startswith(VERSION)\n",
    "])\n",
    "if previous_versions:\n",
    "    latest_prev = previous_versions[-1]\n",
    "    prev_df = pd.read_csv(os.path.join(snapshot_dir, latest_prev), index_col=0)\n",
    "    diff = label_counts.subtract(prev_df.iloc[:, 0], fill_value=0)\n",
    "    print(\"üîç Label count change since last snapshot:\")\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({3: 7321, 6: 2535, 5: 2029, 0: 1785, 7: 1475, 4: 1252, 2: 1042, 1: 205})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68970ff3e4284ed188123d96a347cfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedcc8fc1a2b44dc8556a6c0b55a1416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a867ef80e9430687930c6ed71164ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57e80ab791a4b53b7bef82866509356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0f3774322e461da374870063fa76ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fa4460a7e245d28b15df9d983c7021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0343191ea240a0860477f465ba2460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1878fe1773064daa87a08f0028a21281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({5: 3000, 6: 3000, 2: 3000, 1: 3000, 7: 3000, 0: 3000, 4: 3000, 3: 3000})\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Balance Dataset\n",
    "# --------------------------\n",
    "mp.set_start_method('fork', force=True)\n",
    "\n",
    "label_target = 3000\n",
    "balanced_subsets = []\n",
    "\n",
    "# Dynamically calculate label counts\n",
    "label_counts = Counter(train_dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = train_dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    if count > label_target:\n",
    "        subset = subset.select(random.sample(range(len(subset)), label_target))\n",
    "    elif count < label_target:\n",
    "        multiplier = label_target // len(subset)\n",
    "        remainder = label_target % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "    balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 11. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # ‚úÖ (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # ‚úÖ Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # ‚úÖ Log once per epoch\n",
    "    save_safetensors=True                  # ‚úÖ Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 12. Define Compute and Confusion Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define a compute_metrics function w/ confusion matrix logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw values for further use if needed\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)    \n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify top 3 confused class pairs (excluding diagonal)\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    class_entropies = {}\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            class_entropies[class_name] = class_entropy\n",
    "    \n",
    "    for class_name, entropy_val in sorted(class_entropies.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  - {class_name}: entropy = {entropy_val:.4f}\")\n",
    "\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 4:18:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.196776</td>\n",
       "      <td>0.929721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.258620</td>\n",
       "      <td>0.932668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.314997</td>\n",
       "      <td>0.940376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.335901</td>\n",
       "      <td>0.941737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.358834</td>\n",
       "      <td>0.942643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.95      0.94       411\n",
      "     disgust       0.97      0.78      0.87        46\n",
      "        fear       0.86      0.70      0.77       272\n",
      "   happiness       0.99      0.93      0.96      1851\n",
      "     sadness       0.87      0.94      0.90       296\n",
      "    surprise       0.87      0.97      0.92       542\n",
      "     neutral       0.91      0.98      0.94       618\n",
      " questioning       0.86      0.91      0.89       375\n",
      "\n",
      "    accuracy                           0.93      4411\n",
      "   macro avg       0.91      0.90      0.90      4411\n",
      "weighted avg       0.93      0.93      0.93      4411\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness ‚Üí surprise: 60 instances\n",
      "  - fear ‚Üí questioning: 48 instances\n",
      "  - happiness ‚Üí neutral: 43 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.1029\n",
      "  - fear: entropy = 0.2641\n",
      "  - questioning: entropy = 0.1924\n",
      "  - disgust: entropy = 0.1436\n",
      "  - sadness: entropy = 0.1054\n",
      "  - anger: entropy = 0.0943\n",
      "  - neutral: entropy = 0.0801\n",
      "  - happiness: entropy = 0.0777\n",
      "  - surprise: entropy = 0.0739\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.90      0.97      0.93       411\n",
      "     disgust       0.97      0.74      0.84        46\n",
      "        fear       0.75      0.85      0.80       272\n",
      "   happiness       1.00      0.94      0.97      1851\n",
      "     sadness       0.92      0.87      0.90       296\n",
      "    surprise       0.94      0.97      0.95       542\n",
      "     neutral       0.89      0.98      0.93       618\n",
      " questioning       0.89      0.85      0.87       375\n",
      "\n",
      "    accuracy                           0.93      4411\n",
      "   macro avg       0.91      0.90      0.90      4411\n",
      "weighted avg       0.94      0.93      0.93      4411\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness ‚Üí neutral: 60 instances\n",
      "  - questioning ‚Üí fear: 50 instances\n",
      "  - happiness ‚Üí anger: 27 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0489\n",
      "  - questioning: entropy = 0.0959\n",
      "  - fear: entropy = 0.0892\n",
      "  - sadness: entropy = 0.0723\n",
      "  - disgust: entropy = 0.0557\n",
      "  - surprise: entropy = 0.0485\n",
      "  - happiness: entropy = 0.0416\n",
      "  - anger: entropy = 0.0370\n",
      "  - neutral: entropy = 0.0207\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.95      0.94      0.95       411\n",
      "     disgust       0.90      0.78      0.84        46\n",
      "        fear       0.86      0.75      0.80       272\n",
      "   happiness       0.99      0.96      0.97      1851\n",
      "     sadness       0.90      0.93      0.92       296\n",
      "    surprise       0.91      0.98      0.94       542\n",
      "     neutral       0.92      0.97      0.95       618\n",
      " questioning       0.85      0.92      0.88       375\n",
      "\n",
      "    accuracy                           0.94      4411\n",
      "   macro avg       0.91      0.90      0.91      4411\n",
      "weighted avg       0.94      0.94      0.94      4411\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí questioning: 48 instances\n",
      "  - happiness ‚Üí neutral: 41 instances\n",
      "  - happiness ‚Üí surprise: 35 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0275\n",
      "  - disgust: entropy = 0.0662\n",
      "  - fear: entropy = 0.0469\n",
      "  - anger: entropy = 0.0414\n",
      "  - questioning: entropy = 0.0403\n",
      "  - sadness: entropy = 0.0353\n",
      "  - neutral: entropy = 0.0230\n",
      "  - surprise: entropy = 0.0209\n",
      "  - happiness: entropy = 0.0202\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.94      0.97      0.96       411\n",
      "     disgust       0.92      0.72      0.80        46\n",
      "        fear       0.83      0.74      0.79       272\n",
      "   happiness       0.99      0.96      0.98      1851\n",
      "     sadness       0.89      0.94      0.91       296\n",
      "    surprise       0.94      0.97      0.95       542\n",
      "     neutral       0.93      0.96      0.95       618\n",
      " questioning       0.85      0.90      0.88       375\n",
      "\n",
      "    accuracy                           0.94      4411\n",
      "   macro avg       0.91      0.90      0.90      4411\n",
      "weighted avg       0.94      0.94      0.94      4411\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí questioning: 45 instances\n",
      "  - happiness ‚Üí neutral: 33 instances\n",
      "  - questioning ‚Üí fear: 27 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0216\n",
      "  - disgust: entropy = 0.0557\n",
      "  - fear: entropy = 0.0525\n",
      "  - anger: entropy = 0.0282\n",
      "  - questioning: entropy = 0.0281\n",
      "  - sadness: entropy = 0.0265\n",
      "  - neutral: entropy = 0.0183\n",
      "  - surprise: entropy = 0.0182\n",
      "  - happiness: entropy = 0.0149\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.93      0.98      0.96       411\n",
      "     disgust       0.92      0.74      0.82        46\n",
      "        fear       0.82      0.77      0.80       272\n",
      "   happiness       0.99      0.96      0.98      1851\n",
      "     sadness       0.90      0.92      0.91       296\n",
      "    surprise       0.94      0.97      0.96       542\n",
      "     neutral       0.93      0.97      0.95       618\n",
      " questioning       0.86      0.90      0.88       375\n",
      "\n",
      "    accuracy                           0.94      4411\n",
      "   macro avg       0.91      0.90      0.91      4411\n",
      "weighted avg       0.94      0.94      0.94      4411\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí questioning: 40 instances\n",
      "  - happiness ‚Üí neutral: 38 instances\n",
      "  - questioning ‚Üí fear: 29 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0198\n",
      "  - disgust: entropy = 0.0647\n",
      "  - fear: entropy = 0.0468\n",
      "  - sadness: entropy = 0.0303\n",
      "  - questioning: entropy = 0.0217\n",
      "  - anger: entropy = 0.0212\n",
      "  - neutral: entropy = 0.0177\n",
      "  - happiness: entropy = 0.0148\n",
      "  - surprise: entropy = 0.0139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.030657632279396056, metrics={'train_runtime': 15540.9366, 'train_samples_per_second': 7.722, 'train_steps_per_second': 0.965, 'total_flos': 9.29953881980928e+18, 'train_loss': 0.030657632279396056, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 13. Trainer with Class-Weighted Loss\n",
    "# --------------------------\n",
    "\n",
    "# ‚öñÔ∏è Compute dynamic class weights from training labels/set\n",
    "label_freqs = Counter(train_dataset['label'])\n",
    "total = sum(label_freqs.values())\n",
    "class_weights = torch.tensor([total / label_freqs[i] for i in range(len(label_freqs))], dtype=torch.float).to(device)\n",
    "\n",
    "# üî• Define Focal Loss to focus on hard-to-classify examples\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal.mean() if self.reduction == 'mean' else focal.sum()\n",
    "\n",
    "# ‚ö†Ô∏è Confidence penalty discourages overconfident incorrect predictions\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Use focal loss instead of standard cross-entropy\n",
    "        focal_loss_fn = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "        # Combine with confidence penalty (entropy-based)\n",
    "        loss = focal_loss_fn(logits, labels) + confidence_penalty(logits, beta=0.05)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# trainer initialization\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # Rescue & Save from Last Checkpoint (after training)\n",
    "# # --------------------------\n",
    "# #in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"‚úÖ Processor manually re-saved.\")\n",
    "\n",
    "# # Use parent directory of SAVE_DIR to locate latest V* folder\n",
    "# parent_dir = os.path.dirname(SAVE_DIR)\n",
    "# v_folders = [\n",
    "#     d for d in os.listdir(parent_dir)\n",
    "#     if os.path.isdir(os.path.join(parent_dir, d)) and d.startswith(\"V\")\n",
    "# ]\n",
    "\n",
    "# def extract_timestamp(name):\n",
    "#     try:\n",
    "#         _, date_str, time_str = name.split(\"_\")\n",
    "#         return datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "#     except Exception:\n",
    "#         return datetime.min\n",
    "\n",
    "# latest_version_folder = max(v_folders, key=extract_timestamp)\n",
    "# latest_version_path = os.path.join(parent_dir, latest_version_folder)\n",
    "# print(f\"üóÇÔ∏è Using latest version folder: {latest_version_path}\")\n",
    "\n",
    "# # Locate latest checkpoint within that version folder\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(latest_version_path, d)\n",
    "#     for d in os.listdir(latest_version_path)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(latest_version_path, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"‚ùå No checkpoint found in latest version folder.\")\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # Load model and processor from latest checkpoint and save them\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(latest_version_path)\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12eaaf8a-eb3d-43b5-a99c-b513d20621c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452\n",
      "‚úÖ State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/final_model.pth\n",
      "‚úÖ Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452\n",
      "‚úÖ Trainer backup saved.\n",
      "‚úÖ Memory cleanup complete after save.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 14. Save Final Independent Model (Safe Save Mode)\n",
    "# --------------------------\n",
    "\n",
    "model = model.to(\"cpu\")  # move to CPU first\n",
    "\n",
    "# Save processor\n",
    "processor.save_pretrained(SAVE_DIR)\n",
    "print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "\n",
    "# Save state dict\n",
    "final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "# Save full model\n",
    "model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "# Save trainer state (if defined)\n",
    "if 'trainer' in globals():\n",
    "    try:\n",
    "        trainer.save_model(os.path.join(SAVE_DIR, \"backup_trainer_model\"))\n",
    "        print(\"‚úÖ Trainer backup saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "    \n",
    "# Free memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Memory cleanup complete after save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 15. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"‚úÖ Model reloaded for inference.\")\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345/345 [07:50<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "üìù Saved REVIEW file paths to V11_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/V11_distribution_plot_20250516_120452.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 16. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"üìù Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452\n",
      "üìÇ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/logits_eval_V11.npy\n",
      "üìÇ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/labels_eval_V11.npy\n",
      "‚úÖ Optimal temperature: 1.6804\n",
      "‚úÖ Calibrated Log Loss: 0.2413\n",
      "üìä Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/V11_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"üìÅ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# Apply calibration from saved logits and labels\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"‚ùå Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n  - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Loading logits from: {logits_path}\")\n",
    "    print(f\"üìÇ Loading labels from: {labels_path}\")\n",
    "    \n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()\n",
    "\n",
    "# Reliability diagram\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {output_path}\")\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adaeec19-d4c7-43a6-80cb-86aafd573989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed tagging + copying REVIEW predictions to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/review_predictions_by_class\n",
      "üìÑ CSV log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V11_20250516_120452/V11_review_predictions_with_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Review & Relabel 'REVIEW' Predictions\n",
    "# --------------------------\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "\n",
    "image_paths = [\n",
    "    p for p in Path(IMAGE_DIR).rglob(\"*\")\n",
    "    if p.is_file() and p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "]\n",
    "\n",
    "review_log = []\n",
    "\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            conf, pred_idx = torch.max(probs, dim=-1)\n",
    "\n",
    "        conf_val = conf.item()\n",
    "        pred_label = id2label[pred_idx.item()]\n",
    "        tag = \"REVIEW\" if conf_val < REVIEW_THRESHOLD else pred_label\n",
    "\n",
    "        review_log.append({\n",
    "            \"image_path\": str(img_path),\n",
    "            \"predicted_label\": pred_label,\n",
    "            \"confidence\": round(conf_val, 4),\n",
    "            \"tag\": tag\n",
    "        })\n",
    "\n",
    "        if tag == \"REVIEW\":\n",
    "            target_dir = os.path.join(REVIEW_BY_CLASS_DIR, pred_label)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            shutil.copy(str(img_path), target_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error with image: {img_path} | {e}\")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(review_log).to_csv(REVIEW_CSV_LOG, index=False)\n",
    "print(f\"‚úÖ Completed tagging + copying REVIEW predictions to: {REVIEW_BY_CLASS_DIR}\")\n",
    "print(f\"üìÑ CSV log saved to: {REVIEW_CSV_LOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
