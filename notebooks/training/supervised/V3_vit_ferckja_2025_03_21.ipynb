{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343668aa-787e-4b54-a635-7522fac1e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #In lambdalabs jupyter lab instance, run these:\n",
    "# pip install transformers\n",
    "# pip install tf-keras\n",
    "# pip install --upgrade \"numpy<2\"\n",
    "# pip install datasets\n",
    "# pip install --upgrade datasets pillow\n",
    "# pip install --upgrade \"accelerate>=0.26.0\"\n",
    "# #then check dependency warnings\n",
    "# pip check\n",
    "# #if any issues run\n",
    "# pip install debugpy\n",
    "# pip install --upgrade argcomplete\n",
    "# sudo apt-get install python3-cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8c5ec5-8e88-476d-b78c-a4091f45ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-21 14:23:33.029039: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-21 14:23:33.048527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742567013.067686    2767 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742567013.073957    2767 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742567013.091754    2767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742567013.091776    2767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742567013.091777    2767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742567013.091779    2767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-21 14:23:33.097294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b43b469-39a1-4623-954a-00414237f47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Reload Pretrained Model and Processor\n",
    "# --------------------------\n",
    "checkpoint = \"trpakov/vit-face-expression\"\n",
    "processor = AutoImageProcessor.from_pretrained(checkpoint,use_fast=True)\n",
    "model = AutoModelForImageClassification.from_pretrained(checkpoint)\n",
    "# Load fine-tuned model weights\n",
    "model.load_state_dict(torch.load(\"/home/ubuntu/MLexpressionsStorage/final_model_V1.pth\"))\n",
    "#Puts the model into evaluation model->disables dropout, batch norm to ensure consistent results\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c061e7a1-8c51-4302-8192-9f6ace8e2ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 37081/37081 [00:01<00:00, 26916.95files/s] \n",
      "Computing checksums: 100%|██████████| 37081/37081 [00:29<00:00, 1278.60it/s]\n",
      "Generating train split: 37081 examples [00:02, 15930.68 examples/s]\n",
      "Map: 100%|██████████| 37081/37081 [00:03<00:00, 11617.98 examples/s]\n",
      "Filter: 100%|██████████| 37081/37081 [03:14<00:00, 191.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. Prepare Dataset\n",
    "# --------------------------\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/home/ubuntu/MLexpressionsStorage/img_datasets/combo_ferckja_dataset\", split=\"train\")\n",
    "\n",
    "label_mapping = {\n",
    "    'anger': 'Angry', 'contempt': 'Disgust', 'disgust': 'Disgust',\n",
    "    'fear': 'Fear', 'happiness': 'Happy', 'sadness': 'Sad',\n",
    "    'surprise': 'Surprise', 'neutral': 'Neutral'\n",
    "}\n",
    "num_mapping = {\n",
    "    'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
    "    'Sad': 4, 'Surprise': 5, 'Neutral': 6\n",
    "}\n",
    "\n",
    "def reconcile_labels(example):\n",
    "    # Convert label from integer to string if needed\n",
    "    original_label = dataset.features['label'].int2str(example['label']).lower()\n",
    "    # Map to pre-trained model's label set to get correct label\n",
    "    mapped_label = label_mapping.get(original_label)\n",
    "    #Converts label into a numerical value using num_mapping\n",
    "    example['label'] = num_mapping.get(mapped_label, -1)\n",
    "    return example\n",
    "\n",
    "# Apply label reconciliation and filter out unrecognized labels\n",
    "dataset = dataset.map(reconcile_labels)\n",
    "dataset = dataset.filter(lambda x: x['label'] != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c566c34f-88f2-4247-a702-bf979150c89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 37081/37081 [04:53<00:00, 126.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Data Augmentation and Processing\n",
    "# --------------------------\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "def transform(example):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # Ensure image is properly loaded\n",
    "    if not isinstance(image, Image.Image):\n",
    "        image = Image.open(image)\n",
    "\n",
    "    # Convert to RGB if it's in a different mode\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # Apply data augmentation\n",
    "    image = data_augment(image)\n",
    "\n",
    "    # Convert image to tensor format for model input, ensuring \n",
    "    # that dataset format aligns with ViT model's expected input\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    inputs[\"labels\"] = example[\"label\"]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# Apply transformations and remove unnecessary columns\n",
    "dataset = dataset.map(partial(transform), remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e5fc044-7286-42c4-b1f2-4076ec64a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Train-Validation Split\n",
    "# --------------------------\n",
    "splits = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = splits['train']\n",
    "eval_dataset = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87509c3-0b20-4a68-bd95-fca1d57d6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 5. Training Arguments\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_retrained_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir=\"./logs\",\n",
    "    #label smoothing prevents overconfidence\n",
    "    label_smoothing_factor=0.1,\n",
    "    #cosine lr scheduler helps avoid overfitting\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #warmup ratio prevents instability @ start\n",
    "    warmup_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1728d57-8ba8-4431-8bad-665408ef7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6. Evaluation Metric\n",
    "# --------------------------\n",
    "#function computes accurace using armax() to get predicted labels\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cbefc7c-edc3-4dde-860d-7dde1d21e7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2320' max='2320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2320/2320 3:39:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.809995</td>\n",
       "      <td>0.832008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.848600</td>\n",
       "      <td>0.813848</td>\n",
       "      <td>0.835513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>0.832012</td>\n",
       "      <td>0.842659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.820087</td>\n",
       "      <td>0.848321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.814358</td>\n",
       "      <td>0.851827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2320, training_loss=0.5851076454951846, metrics={'train_runtime': 13166.4743, 'train_samples_per_second': 11.265, 'train_steps_per_second': 0.176, 'total_flos': 1.1494126967676273e+19, 'train_loss': 0.5851076454951846, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Trainer\n",
    "# --------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #early stopping if validation loss doesn't imporve for 2 epochs\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0656b018-8853-4302-8588-c7ac811e1fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/MLexpressionsStorage/vit_final_independent_V3/preprocessor_config.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Save Final Independent Model\n",
    "# --------------------------\n",
    "model.save_pretrained(\"/home/ubuntu/MLexpressionsStorage/vit_final_independent_V3\")\n",
    "processor.save_pretrained(\"/home/ubuntu/MLexpressionsStorage/vit_final_independent_V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2074b27-e69d-430a-a1d8-8c33c52174a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/ubuntu/MLexpressionsStorage/final_model_V3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3ae13-c963-458f-a1fb-27dcaa749988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
