{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V29 changes:\n",
    "    # overview: selectively turn off label smoothing for contempt,disgust \n",
    "    # section #2 - updated CustomTrainer,TargetedSmoothedCrossEntropyLoss\n",
    "    # section #3 - loading V28 checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, precision_recall_fscore_support\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    GaussianBlur,\n",
    "    RandAugment,\n",
    "    RandomAffine,\n",
    "    RandomApply,\n",
    "    RandomPerspective,\n",
    "    RandomAdjustSharpness,\n",
    "    ToPILImage,\n",
    "    ToTensor\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    'anger', 'disgust', 'fear', 'happiness', 'neutral',\n",
    "    'questioning', 'sadness', 'surprise', 'contempt', 'unknown'\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "HARD_CLASS_NAMES = ['contempt', 'disgust', 'fear', 'questioning']\n",
    "hard_class_ids = [label2id[n] for n in HARD_CLASS_NAMES]\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\", VERSION_TAG)\n",
    "LOGITS_PATH = os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\")\n",
    "LABELS_PATH = os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Utility Functions (Metrics & Calibration)\n",
    "# --------------------------\n",
    "\n",
    "# ------------------------------------------\n",
    "# Part A: Data Preparation & Augmentation\n",
    "# ------------------------------------------\n",
    "\n",
    "# üó∫Ô∏è Injects 'image_path' to dataset BEFORE any map/filter\n",
    "def add_image_path(example):\n",
    "    # Handle DatasetsImage and PIL.Image types robustly.\n",
    "    img_obj = example[\"image\"]\n",
    "    path = getattr(img_obj, \"filename\", None)\n",
    "    if path is None:\n",
    "        # Fallback for rare cases where the path is not in the image object.\n",
    "        if \"file\" in example:\n",
    "            path = os.path.join(BASE_PATH, example[\"file\"])\n",
    "        else:\n",
    "            path = \"\"\n",
    "    example[\"image_path\"] = path\n",
    "    return example\n",
    "\n",
    "# üè∑Ô∏è Standardizes labels from various sources (int, str, filepath) to a consistent integer ID.\n",
    "def reconcile_labels(example):\n",
    "    label = example.get(\"label\", None)\n",
    "    # Determine the original label string from different possible input formats.\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = example[\"image_path\"]\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "    \n",
    "    # Map the string label to its corresponding integer ID.\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example\n",
    "\n",
    "# üîç Computes a perceptual hash (pHash) for an image to find visually similar duplicates.\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# üì¶ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self, processor, augment_dict):\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Apply augmentations and process images\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            # Select the correct augmentation pipeline\n",
    "            aug_pipeline = self.augment_dict.get(label, data_augment)\n",
    "            # Ensure image is in RGB format\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "            augmented_image = aug_pipeline(rgb_image)\n",
    "            processed_images.append(augmented_image)\n",
    "\n",
    "        # Create the 'pixel_values' batch.\n",
    "        # Padding and truncation arguments are removed as they are not needed when\n",
    "        # augmentations already resize the images to a uniform size.\n",
    "        batch = self.processor(\n",
    "            images=processed_images,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Add labels to the batch\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "        \n",
    "# ------------------------------------------\n",
    "# Part B: Model & Training Components\n",
    "# ------------------------------------------\n",
    "      \n",
    "# üèãÔ∏è Defines a custom Trainer that uses targeted loss function\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize our new targeted loss function.\n",
    "        self.loss_fct = TargetedSmoothedCrossEntropyLoss(smoothing=0.05)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "\n",
    "# üîÑ Implements Cross-Entropy Loss with Targeted Label Smoothing for multiple classes\n",
    "# Smoothing turned OFF for 'contempt' & 'disgust' to encourage confident predictions\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        # Define the list of classes for which smoothing will be turned OFF.\n",
    "        self.target_class_ids = [label2id['contempt'], label2id['disgust']]\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            # 1. Start with standard smoothed labels for all samples.\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            \n",
    "            # 2. Create a mask to find all samples belonging to our target classes.\n",
    "            # torch.isin is an efficient way to check for multiple IDs at once.\n",
    "            target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "            \n",
    "            # 3. For the targeted samples, overwrite with a \"sharp\" one-hot encoding.\n",
    "            if target_mask.any():\n",
    "                sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                smooth_labels[target_mask] = sharp_labels\n",
    "        \n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# ‚ö†Ô∏è Confidence Penalty to Reduce Overconfidence\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    # Entropy is a measure of uncertainty; penalizing low entropy encourages less confident predictions.\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "    \n",
    "# üìä Compute Metrics with Confusion Matrix Logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(labels, preds, target_names=LABEL_NAMES, output_dict=True)\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw logits/labels for calibration or further analysis\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in LABEL_NAMES]\n",
    "    recalls = [report[name][\"recall\"] for name in LABEL_NAMES]\n",
    "    precisions = [report[name][\"precision\"] for name in LABEL_NAMES]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    # Sort for display only; CSV row stays in canonical label order\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "    epoch = getattr(trainer.state, \"epoch\", None) if \"trainer\" in globals() else None\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(LABEL_NAMES, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(LABEL_NAMES, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(LABEL_NAMES, precisions)},\n",
    "        **{f\"entropy_{n}\": [e] for n, e in entropy_per_class}\n",
    "    })\n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # Generate and print confusion matrix heatmap\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    print(\"\\nüîç Class entropies (sorted):\")\n",
    "    for class_name, entropy in sorted_entropy:\n",
    "        print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# üíæ Saves the model, processor, and trainer state \n",
    "def save_model_and_processor(model, processor, save_dir, trainer=None):\n",
    "    print(f\"Saving model and processor to: {save_dir}\")\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Save processor\n",
    "    processor.save_pretrained(save_dir)\n",
    "    print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "    \n",
    "    # Save full model\n",
    "    model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "    print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save state dict\n",
    "    final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "    # Save trainer state\n",
    "    if trainer is not None:\n",
    "        try:\n",
    "            trainer.save_model(os.path.join(save_dir, \"backup_trainer_model\"))\n",
    "            print(\"‚úÖ Trainer backup saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass  # Not all systems have CUDA\n",
    "    print(\"‚úÖ Memory cleanup complete after save.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Part C: General & Debugging Utilities\n",
    "# ------------------------------------------\n",
    "\n",
    "def analyze_dataset_structure(dataset_to_analyze, id2label, base_path):\n",
    "    # Print label schema from the dataset being analyzed.\n",
    "    print(\"Label schema (from dataset):\", dataset_to_analyze.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object.\n",
    "    label_counts = Counter(dataset_to_analyze[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect and print minority classes for informational purposes.\n",
    "    N = 3\n",
    "    minority_classes_ids = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    minority_names = [id2label[i] for i in minority_classes_ids]\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes (for info): {minority_names}\")\n",
    "\n",
    "    # Count images per directory from the source folder for reference.\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per source folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    # The function now only returns the dictionary that is used later.\n",
    "    return folder_image_counts\n",
    "\n",
    "# üìÇ Finds the most recent file (e.g., 'audit.csv') from the latest V* run directory.\n",
    "def find_latest_run_artifact(root_dir, filename):\n",
    "    # Find all previous version directories.\n",
    "    all_run_dirs = [\n",
    "        d for d in os.listdir(root_dir)\n",
    "        if d.startswith(\"V\") and os.path.isdir(os.path.join(root_dir, d))\n",
    "    ]\n",
    "    if not all_run_dirs:\n",
    "        return None\n",
    "\n",
    "    # Sort by version number to reliably find the latest run.\n",
    "    try:\n",
    "        latest_run_dir_name = sorted(all_run_dirs, key=extract_version_from_path, reverse=True)[0]\n",
    "        artifact_path = os.path.join(root_dir, latest_run_dir_name, filename)\n",
    "\n",
    "        if os.path.exists(artifact_path):\n",
    "            print(f\"‚úÖ Found artifact '{filename}' in latest run: {latest_run_dir_name}\")\n",
    "            return artifact_path\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Artifact '{filename}' not found in {latest_run_dir_name}.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not process previous run directories to find '{filename}'. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# üîë Creates a unique 'label/filename.jpg' key from a full image path for reliable matching.\n",
    "def get_relative_path_key(path):\n",
    "    try:\n",
    "        return os.path.join(os.path.basename(os.path.dirname(path)), os.path.basename(path))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "# üî¢ Extracts version number from a directory name\n",
    "def extract_version_from_path(path):\n",
    "    import re\n",
    "    match = re.search(r\"V(\\d+)\", os.path.basename(path))\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# üö¶ Prints the label distribution for a given dataset, \n",
    "    #useful for debugging and sanity checks.\n",
    "def check_label_integrity(dataset, LABEL_NAMES, label2id):\n",
    "    # Count all mapped labels in the dataset.\n",
    "    label_counts = Counter(dataset['label'])\n",
    "    print(\"\\nüö® Label distribution after mapping (before split):\")\n",
    "    for label_id in range(len(LABEL_NAMES)):\n",
    "        label_name = LABEL_NAMES[label_id]\n",
    "        print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "\n",
    "    # Specifically highlight 'surprise'\n",
    "    surprise_id = label2id['surprise']\n",
    "    if label_counts.get(surprise_id, 0) == 0:\n",
    "        print(\"‚ùóWARNING: No 'surprise' images found after mapping!\")\n",
    "    elif label_counts[surprise_id] < 50:  # arbitrary threshold\n",
    "        print(f\"‚ö†Ô∏è Only {label_counts[surprise_id]} 'surprise' images found! Check curation or mapping.\")\n",
    "\n",
    "# üö¶ Prints the label distribution for a dictionary of datasets (e.g., train, eval).\n",
    "def check_all_label_integrity(datasets_dict, LABEL_NAMES, label2id):\n",
    "    for name, dataset in datasets_dict.items():\n",
    "        print(f\"\\nüö® Label distribution for: {name}\")\n",
    "        label_counts = Counter(dataset['label'])\n",
    "        \n",
    "        for label_id in range(len(LABEL_NAMES)):\n",
    "            label_name = LABEL_NAMES[label_id]\n",
    "            print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "        \n",
    "        surprise_id = label2id['surprise']\n",
    "        if label_counts.get(surprise_id, 0) == 0:\n",
    "            print(\"‚ùóWARNING: No 'surprise' images found in this split!\")\n",
    "        elif label_counts[surprise_id] < 50:\n",
    "            print(f\"‚ö†Ô∏è Only {label_counts[surprise_id]} 'surprise' images in {name}! Check curation or mapping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Explicitly loading V28checkpoint from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V28_20250709_153248\n",
      "‚úÖ Classifier head reset for new training.\n",
      "\n",
      "üñ•Ô∏è Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Auto-Load V20 Golden Checkpoint\n",
    "# --------------------------\n",
    "# Manually set the path to your best-performing model\n",
    "model_path = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V28_20250709_153248\"\n",
    "print(f\"‚úÖ Explicitly loading V28checkpoint from: {model_path}\")\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Reset the classifier head for new training\n",
    "model.classifier = nn.Linear(model.config.hidden_size, len(LABEL_NAMES))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "print(\"‚úÖ Classifier head reset for new training.\")\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"\\nüñ•Ô∏è Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Counting valid image files on disk for verification...\n",
      "‚úÖ Found 17452 image files in /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\n",
      "‚úÖ Datasets caching disabled for this run to ensure fresh data load.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0b19795beb42ad889e2b1e1ed5b150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17461 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d2e6efb9ce4e0e8246b76cbb6d3d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Add file path to each record:   0%|          | 0/17451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30b17dab88941ada6d8f7490585467c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Re-labeling dataset (preserving image_path):   0%|          | 0/17451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4a238f9e5b490ca9d068c7014ebc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total examples after filtering: 17451\n",
      "Sample with path: /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset/anger/Abel_Pacheco_0002.jpg\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Load and Prepare Dataset (with filename preservation)\n",
    "# ==============================\n",
    "\n",
    "# --- Dynamic File Count ---\n",
    "print(\"üîç Counting valid image files on disk for verification...\")\n",
    "# This will recursively find all valid image files in your dataset directory\n",
    "expected_file_count = len(\n",
    "    [p for p in Path(BASE_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    ")\n",
    "print(f\"‚úÖ Found {expected_file_count} image files in {BASE_PATH}\")\n",
    "\n",
    "# Disable caching BEFORE loading\n",
    "datasets.disable_caching()\n",
    "print(\"‚úÖ Datasets caching disabled for this run to ensure fresh data load.\")\n",
    "\n",
    "# Step 1: Load dataset and capture filepaths\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=BASE_PATH,\n",
    "    split=\"train\" # No need to specify cache_dir when caching is off\n",
    ")\n",
    "\n",
    "# Only run ONCE and only here, so \"image_path\" is never dropped later!\n",
    "dataset = dataset.map(add_image_path, desc=\"Add file path to each record\")\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset (preserving image_path)\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "# ** Robust Verification **\n",
    "final_count = len(dataset)\n",
    "print(f\"‚úÖ Total examples after filtering: {final_count}\")\n",
    "print(\"Sample with path:\", dataset[0][\"image_path\"])\n",
    "\n",
    "# Assertion checks whether loaded count is very close to the disk count\n",
    "# Small tolerance accounts for any files that fail to load or be filtered\n",
    "assert abs(final_count - expected_file_count) < 10, \\\n",
    "    f\"Dataset size mismatch! Found {expected_file_count} files but loaded {final_count}.\"\n",
    "\n",
    "assert dataset[0].get(\"image_path\", None), \"image_path missing from first record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2969f417-bded-4680-b5ef-9fd528be21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 15 hard-negative images to be excluded from training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9e2a99505442a8a190a7d9300db468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering out hard negatives:   0%|          | 0/17451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Curated dataset created for V25. Removed 4 images.\n",
      "   - Original size: 17451 -> New size: 17447\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. V25 Data Curation: Remove Hard Negatives\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Define the path to the hard negatives file generated by the V24 run.\n",
    "V24_RUN_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V24_20250704_081028\"\n",
    "hard_negatives_path = os.path.join(V24_RUN_DIR, 'review_hardneg_contempt_questioning.txt')\n",
    "\n",
    "if not os.path.exists(hard_negatives_path):\n",
    "    raise FileNotFoundError(f\"CRITICAL: The hard negatives file is missing: {hard_negatives_path}\")\n",
    "\n",
    "# 2. Load the list of image paths to be excluded into a set for fast lookup.\n",
    "with open(hard_negatives_path, 'r') as f:\n",
    "    # os.path.normpath ensures path formats are consistent (e.g., handles slashes).\n",
    "    exclusion_list = {os.path.normpath(line.strip()) for line in f if line.strip()}\n",
    "print(f\"‚úÖ Loaded {len(exclusion_list)} hard-negative images to be excluded from training.\")\n",
    "\n",
    "# 3. Filter the main dataset.\n",
    "#    The 'dataset' variable holds the data loaded in Section #4.\n",
    "initial_count = len(dataset)\n",
    "curated_dataset = dataset.filter(\n",
    "    lambda example: os.path.normpath(example['image_path']) not in exclusion_list,\n",
    "    desc=\"Filtering out hard negatives\"\n",
    ")\n",
    "removed_count = initial_count - len(curated_dataset)\n",
    "print(f\"‚úÖ Curated dataset created for V25. Removed {removed_count} images.\")\n",
    "print(f\"   - Original size: {initial_count} -> New size: {len(curated_dataset)}\")\n",
    "\n",
    "# All subsequent sections must now use 'curated_dataset'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise', 'unknown'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 2302 examples\n",
      "  disgust: 309 examples\n",
      "  fear: 1432 examples\n",
      "  happiness: 2892 examples\n",
      "  neutral: 3333 examples\n",
      "  questioning: 1895 examples\n",
      "  sadness: 1706 examples\n",
      "  surprise: 2783 examples\n",
      "  contempt: 409 examples\n",
      "  unknown: 386 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes (for info): ['contempt', 'disgust', 'unknown']\n",
      "\n",
      "üìÇ Image count per source folder:\n",
      "  anger: 2302 images\n",
      "  contempt: 412 images\n",
      "  disgust: 309 images\n",
      "  fear: 1432 images\n",
      "  happiness: 2892 images\n",
      "  neutral: 3333 images\n",
      "  questioning: 1896 images\n",
      "  sadness: 1706 images\n",
      "  surprise: 2783 images\n",
      "  unknown: 386 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "\n",
    "# Call the function and assign its single, useful return value.\n",
    "folder_image_counts = analyze_dataset_structure(curated_dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Disgust hash clusters with more than 1 image:\n",
      "üîç Sadness hash clusters with more than 1 image:\n",
      "  - Cluster 958c52e1: 2 images copied for review\n",
      "  - Cluster ee9a8d33: 2 images copied for review\n",
      "  - Cluster d0890396: 2 images copied for review\n",
      "  - Cluster bb0d06f2: 2 images copied for review\n",
      "  - Cluster d7f00fa2: 2 images copied for review\n",
      "üîç Fear hash clusters with more than 1 image:\n",
      "  - Cluster 9ae56592: 2 images copied for review\n",
      "  - Cluster 91c8ee81: 2 images copied for review\n",
      "  - Cluster dae5a596: 2 images copied for review\n",
      "üîç Questioning hash clusters with more than 1 image:\n",
      "  - Cluster da014886: 2 images copied for review\n",
      "  - Cluster 9db42783: 2 images copied for review\n",
      "üîç Contempt hash clusters with more than 1 image:\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Perceptual Clustering for Ambiguous/Confused Classes\n",
    "# --------------------------\n",
    "\n",
    "CLUSTER_TARGETS = [\"disgust\", \"sadness\", \"fear\", \"questioning\", \"contempt\"]\n",
    "\n",
    "for class_name in CLUSTER_TARGETS:\n",
    "    class_dir = os.path.join(BASE_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"‚ö†Ô∏è Class dir not found: {class_dir} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    class_images = [\n",
    "        os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
    "        if is_valid_image(f)\n",
    "    ]\n",
    "    hash_map = {}\n",
    "    for path in class_images:\n",
    "        h = compute_hash(path)\n",
    "        if h:\n",
    "            hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "    cluster_dir = os.path.join(SAVE_DIR, f\"{class_name}_clusters\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"üîç {class_name.capitalize()} hash clusters with more than 1 image:\")\n",
    "    for h, paths in hash_map.items():\n",
    "        if len(paths) > 1:\n",
    "            cluster_path = os.path.join(cluster_dir, h)\n",
    "            os.makedirs(cluster_path, exist_ok=True)\n",
    "            for p in paths:\n",
    "                shutil.copy(p, cluster_path)\n",
    "            print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Targeted minority augmentation will apply to: ['contempt', 'questioning', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "label_freqs = Counter(curated_dataset[\"label\"])\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "minority_by_name = [n for n in minority_by_name if n != \"unknown\"]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning', 'contempt']\n",
    "\n",
    "# Merge and deduplicate\n",
    "minority_class_names = list(set(minority_by_name + manual_focus_classes))\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"üéØ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmentation pipelines defined. They will be applied by the data collator during training.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# V25 Strategy: We define the augmentation pipelines here, but they will be\n",
    "# applied on-the-fly by the DataCollator during training. This is a more\n",
    "# robust method that avoids issues with the data balancing steps.\n",
    "\n",
    "# Baseline augmentation for majority classes.\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# Stronger RandAugment for targeted minority classes.\n",
    "minority_aug = T.Compose([\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Augmentation pipelines defined. They will be applied by the data collator during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({4: 3333, 3: 2892, 7: 2783, 0: 2302, 5: 1895, 6: 1706, 2: 1432, 8: 409, 9: 386, 1: 309})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30130fc8c69e4830ae6759db677c7c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883454b8886a440ab708564f71ed6140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce3107b2653472890ad553560ede86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e9d374150144a796a8a9cf316648f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902431d73783435d84c2d3205c339811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4fb5dd73c64251b033cbd257615162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6849dae59d3432987022fa56d30ac9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b052500ffacf46239ead580fdfa36b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c131b74bd4fa407f8b63b3fe54ce6976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43609816532e4122a976afd7b3a33b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({4: 3333, 3: 2892, 7: 2783, 0: 2302, 6: 2250, 1: 2250, 5: 2250, 2: 2250, 8: 2250, 9: 386})\n",
      "\n",
      "üö® Label distribution for: full curated dataset\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3333\n",
      "  questioning : 1895\n",
      "  sadness     : 1706\n",
      "  surprise    : 2783\n",
      "  contempt    : 409\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set (post-balance)\n",
      "  anger       : 2302\n",
      "  disgust     : 2250\n",
      "  fear        : 2250\n",
      "  happiness   : 2892\n",
      "  neutral     : 3333\n",
      "  questioning : 2250\n",
      "  sadness     : 2250\n",
      "  surprise    : 2783\n",
      "  contempt    : 2250\n",
      "  unknown     : 386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Balance Dataset (with NO oversampling for 'unknown')\n",
    "# --------------------------\n",
    "MINORITY_CAP = 2250\n",
    "balanced_subsets = []\n",
    "label_counts = Counter(curated_dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = curated_dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    class_name = LABEL_NAMES[label]\n",
    "    if class_name == \"unknown\":\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count < MINORITY_CAP:\n",
    "        multiplier = MINORITY_CAP // len(subset)\n",
    "        remainder = MINORITY_CAP % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        balanced_subsets.append(subset)\n",
    "    else:\n",
    "        # Append full set (no downsampling for majority classes)\n",
    "        balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))\n",
    "\n",
    "hard_classes = ['contempt', 'disgust', 'questioning', 'surprise', 'fear']\n",
    "hard_class_ids = [label2id[c] for c in hard_classes]\n",
    "\n",
    "# Calculate weights: Give hard classes 2x, others 1x\n",
    "weights = [2.0 if l in hard_class_ids else 1.0 for l in train_dataset[\"label\"]]\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full curated dataset\": curated_dataset,\n",
    "        \"train set (post-balance)\": train_dataset,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")\n",
    "\n",
    "# cleaning memory\n",
    "del balanced_subsets\n",
    "del subset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Curated data split into 13957 training and 3490 validation samples.\n",
      "\n",
      "--- Starting V29 Training with on-the-fly processing ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8725' max='8725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8725/8725 54:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.535300</td>\n",
       "      <td>0.374020</td>\n",
       "      <td>0.975358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.377649</td>\n",
       "      <td>0.974212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.376376</td>\n",
       "      <td>0.976504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.358693</td>\n",
       "      <td>0.979943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.363032</td>\n",
       "      <td>0.978223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.98      0.99      0.98       471\n",
      "     disgust       0.99      0.93      0.96        71\n",
      "        fear       0.95      0.94      0.94       289\n",
      "   happiness       0.99      0.99      0.99       542\n",
      "     neutral       0.99      0.98      0.98       697\n",
      " questioning       0.95      0.98      0.96       367\n",
      "     sadness       0.98      0.98      0.98       350\n",
      "    surprise       0.98      0.99      0.98       542\n",
      "    contempt       0.84      0.83      0.84        84\n",
      "     unknown       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.98      3490\n",
      "   macro avg       0.96      0.96      0.96      3490\n",
      "weighted avg       0.98      0.98      0.98      3490\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 14 instances\n",
      "  - fear ‚Üí surprise: 7 instances\n",
      "  - disgust ‚Üí contempt: 5 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3440\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.4112\n",
      "  - questioning: entropy = 0.4031\n",
      "  - fear: entropy = 0.3814\n",
      "  - unknown: entropy = 0.3660\n",
      "  - surprise: entropy = 0.3588\n",
      "  - sadness: entropy = 0.3374\n",
      "  - neutral: entropy = 0.3258\n",
      "  - anger: entropy = 0.3160\n",
      "  - happiness: entropy = 0.3124\n",
      "  - disgust: entropy = 0.3083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.97      0.98      0.98       471\n",
      "     disgust       0.94      0.92      0.93        71\n",
      "        fear       0.95      0.93      0.94       289\n",
      "   happiness       0.99      1.00      0.99       542\n",
      "     neutral       1.00      0.98      0.99       697\n",
      " questioning       0.96      0.96      0.96       367\n",
      "     sadness       0.98      0.97      0.98       350\n",
      "    surprise       0.97      0.99      0.98       542\n",
      "    contempt       0.82      0.81      0.81        84\n",
      "     unknown       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.97      3490\n",
      "   macro avg       0.96      0.95      0.96      3490\n",
      "weighted avg       0.97      0.97      0.97      3490\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 11 instances\n",
      "  - fear ‚Üí surprise: 9 instances\n",
      "  - questioning ‚Üí contempt: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3333\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - fear: entropy = 0.3687\n",
      "  - questioning: entropy = 0.3580\n",
      "  - surprise: entropy = 0.3507\n",
      "  - contempt: entropy = 0.3467\n",
      "  - anger: entropy = 0.3332\n",
      "  - sadness: entropy = 0.3262\n",
      "  - neutral: entropy = 0.3191\n",
      "  - happiness: entropy = 0.3168\n",
      "  - unknown: entropy = 0.3136\n",
      "  - disgust: entropy = 0.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.98      0.99      0.98       471\n",
      "     disgust       0.93      0.89      0.91        71\n",
      "        fear       0.97      0.94      0.95       289\n",
      "   happiness       0.99      1.00      0.99       542\n",
      "     neutral       0.99      0.99      0.99       697\n",
      " questioning       0.95      0.96      0.96       367\n",
      "     sadness       0.99      0.99      0.99       350\n",
      "    surprise       0.98      0.99      0.99       542\n",
      "    contempt       0.79      0.77      0.78        84\n",
      "     unknown       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.98      3490\n",
      "   macro avg       0.96      0.95      0.95      3490\n",
      "weighted avg       0.98      0.98      0.98      3490\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 15 instances\n",
      "  - disgust ‚Üí contempt: 7 instances\n",
      "  - fear ‚Üí surprise: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3319\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - fear: entropy = 0.3845\n",
      "  - contempt: entropy = 0.3434\n",
      "  - questioning: entropy = 0.3429\n",
      "  - surprise: entropy = 0.3391\n",
      "  - neutral: entropy = 0.3322\n",
      "  - anger: entropy = 0.3306\n",
      "  - sadness: entropy = 0.3282\n",
      "  - happiness: entropy = 0.3161\n",
      "  - unknown: entropy = 0.3018\n",
      "  - disgust: entropy = 0.1711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.98      0.99      0.99       471\n",
      "     disgust       0.99      0.93      0.96        71\n",
      "        fear       0.96      0.94      0.95       289\n",
      "   happiness       0.99      1.00      0.99       542\n",
      "     neutral       0.99      0.99      0.99       697\n",
      " questioning       0.96      0.98      0.97       367\n",
      "     sadness       0.98      0.98      0.98       350\n",
      "    surprise       0.98      0.99      0.98       542\n",
      "    contempt       0.88      0.88      0.88        84\n",
      "     unknown       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.98      3490\n",
      "   macro avg       0.97      0.97      0.97      3490\n",
      "weighted avg       0.98      0.98      0.98      3490\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 9 instances\n",
      "  - fear ‚Üí surprise: 8 instances\n",
      "  - surprise ‚Üí fear: 5 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3340\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - fear: entropy = 0.3809\n",
      "  - questioning: entropy = 0.3718\n",
      "  - sadness: entropy = 0.3419\n",
      "  - surprise: entropy = 0.3395\n",
      "  - contempt: entropy = 0.3297\n",
      "  - neutral: entropy = 0.3244\n",
      "  - anger: entropy = 0.3242\n",
      "  - happiness: entropy = 0.3233\n",
      "  - unknown: entropy = 0.3085\n",
      "  - disgust: entropy = 0.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.99       471\n",
      "     disgust       0.96      0.93      0.94        71\n",
      "        fear       0.97      0.93      0.95       289\n",
      "   happiness       0.99      1.00      0.99       542\n",
      "     neutral       0.99      0.99      0.99       697\n",
      " questioning       0.95      0.98      0.97       367\n",
      "     sadness       0.98      0.99      0.98       350\n",
      "    surprise       0.98      0.99      0.99       542\n",
      "    contempt       0.89      0.79      0.84        84\n",
      "     unknown       0.99      1.00      0.99        77\n",
      "\n",
      "    accuracy                           0.98      3490\n",
      "   macro avg       0.97      0.96      0.96      3490\n",
      "weighted avg       0.98      0.98      0.98      3490\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 12 instances\n",
      "  - fear ‚Üí surprise: 9 instances\n",
      "  - fear ‚Üí anger: 4 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3346\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - fear: entropy = 0.3794\n",
      "  - questioning: entropy = 0.3751\n",
      "  - contempt: entropy = 0.3503\n",
      "  - anger: entropy = 0.3417\n",
      "  - sadness: entropy = 0.3304\n",
      "  - neutral: entropy = 0.3292\n",
      "  - surprise: entropy = 0.3266\n",
      "  - happiness: entropy = 0.3180\n",
      "  - unknown: entropy = 0.3037\n",
      "  - disgust: entropy = 0.1742\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Saving Final Model ---\n",
      "Saving model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\n",
      "‚úÖ Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\n",
      "‚úÖ Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\n",
      "‚úÖ State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/final_model.pth\n",
      "‚úÖ Memory cleanup complete after save.\n",
      "--- Model saved to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 ---\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11. Optimizer, Scheduler, and Training\n",
    "# --------------------------\n",
    "\n",
    "# --- Part A: Train/Validation Split on CURATED Data ---\n",
    "split_dataset = curated_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(f\"‚úÖ Curated data split into {len(train_dataset)} training and {len(eval_dataset)} validation samples.\")\n",
    "\n",
    "\n",
    "# --- Part B: Define Training Arguments ---\n",
    "# Reusing arguments from your script, ensuring best model is loaded.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),\n",
    "    logging_strategy=\"epoch\",\n",
    "    remove_unused_columns=False  # <-- Add this line\n",
    ")\n",
    "\n",
    "# This part is also correct.\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "# --- Part C: Instantiate the Data Collator ---\n",
    "# This uses augmentation pipelines defined in Section #8.\n",
    "minority_augment_map = {label_id: minority_aug for label_id in minority_classes}\n",
    "data_collator = DataCollatorWithAugmentation(\n",
    "    processor=processor,\n",
    "    augment_dict=minority_augment_map\n",
    ")\n",
    "\n",
    "# --- Part D: Discriminative Learning Rate Optimizer Setup ---\n",
    "# Define different learning rates for the head and the backbone\n",
    "head_lr = 5e-5      # High learning rate for the classifier head\n",
    "backbone_lr = 2e-7  # Very low learning rate for the fine-tuned backbone layers\n",
    "\n",
    "# First, ensure all layers are frozen by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze the classifier head to be trained    \n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "# This gives the model more capacity to adapt its feature extraction.\n",
    "for name, param in model.vit.encoder.layer[-4:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create parameter groups for the optimizer\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.classifier.parameters(), 'lr': head_lr},\n",
    "    {'params': model.vit.encoder.layer[-4:].parameters(), 'lr': backbone_lr}\n",
    "]\n",
    "\n",
    "# Create the AdamW optimizer with the specified parameter groups.\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "\n",
    "# --- Part E: Trainer Initialization and Execution ---\n",
    "# Initialize your custom CustomLossTrainer with all components.\n",
    "trainer = CustomLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "# --- Part F: Train the Model and Finalize ---\n",
    "print(f\"\\n--- Starting {VERSION} Training with on-the-fly processing ---\")\n",
    "trainer.train()\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "# Save the final, best-performing model from the run.\n",
    "print(\"\\n--- Saving Final Model ---\")\n",
    "save_model_and_processor(trainer.model, processor, SAVE_DIR)\n",
    "print(f\"--- Model saved to {SAVE_DIR} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# 12. Inference & Analysis Utilities \n",
    "# ===================================\n",
    "\n",
    "# ------------------------------------------\n",
    "# Part A: Core Prediction Functions\n",
    "# ------------------------------------------\n",
    "\n",
    "# üöÄ Runs inference on a folder of images in batches and assigns labels or 'REVIEW'.\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path)) # <-- THIS LINE WAS MISSING\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# ------------------------------------------\n",
    "# Part B: Post-Inference Analysis & Visualization\n",
    "# ------------------------------------------\n",
    "\n",
    "# ‚õèÔ∏è Mines the prediction CSV to find \"hard negative\" images from specific confusing class pairs.\n",
    "def parse_review_confusions(csv_path, confusion_pairs):\n",
    "    import csv\n",
    "    flagged_imgs = {pair: [] for pair in confusion_pairs}\n",
    "    with open(csv_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            pred = row[\"predicted_label\"]\n",
    "            true = os.path.basename(os.path.dirname(row[\"image_path\"]))\n",
    "            conf = float(row[\"confidence\"])\n",
    "            for a, b in confusion_pairs:\n",
    "                if ((pred == a and true == b) or (pred == b and true == a)) and conf < 0.8:\n",
    "                    flagged_imgs[(a, b)].append(row[\"image_path\"])\n",
    "    return flagged_imgs\n",
    "\n",
    "# üìä Creates distribution bar plot of predicted label.\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# üìà Plot Reliability Diagram (Calibration Curve)\n",
    "    # visualizes how well model confidence matches actual accuracy\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {output_path}\")\n",
    "\n",
    "# ü§ù Takes a list of models and predicts a label for a single image by averaging their softmax probabilities.\n",
    "def ensemble_predict(models, processor, image_path, device=\"cpu\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    softmaxes = []\n",
    "    individual_preds = []\n",
    "\n",
    "    # Get predictions from each model in the ensemble.\n",
    "    for m in models:\n",
    "        with torch.no_grad():\n",
    "            logits = m(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            individual_preds.append(id2label[torch.argmax(probs, dim=-1).item()])\n",
    "            softmaxes.append(probs.cpu().numpy())\n",
    "\n",
    "    # Average the probabilities and return the final prediction.\n",
    "    avg_probs = np.mean(softmaxes, axis=0)\n",
    "    ensemble_pred_idx = np.argmax(avg_probs)\n",
    "    ensemble_conf = avg_probs[0, ensemble_pred_idx]\n",
    "\n",
    "    return id2label[ensemble_pred_idx], ensemble_conf, individual_preds\n",
    "    \n",
    "# ------------------------------------------\n",
    "# Part C: Model Calibration Utilities\n",
    "# ------------------------------------------\n",
    "\n",
    "# üå°Ô∏è Apply Temperature Scaling for Calibration\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"‚ùå Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Loading logits from: {logits_path}\")\n",
    "    print(f\"üìÇ Loading labels from: {labels_path}\")\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model reloaded for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 273/273 [06:17<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "üìù Saved REVIEW file paths to V29_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/V29_distribution_plot_20250710_082807.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 13. Entry Point for Inference\n",
    "# -----------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"‚úÖ Model reloaded for inference.\")\n",
    "\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"üìù Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\n",
      "üìÇ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/logits_eval_V29.npy\n",
      "üìÇ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/labels_eval_V29.npy\n",
      "‚úÖ Optimal temperature: 1.1055\n",
      "‚úÖ Calibrated Log Loss: 0.1494\n",
      "üìä Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/V29_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 14. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"üìÅ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adaeec19-d4c7-43a6-80cb-86aafd573989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed tagging + copying REVIEW predictions to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/review_predictions_by_class\n",
      "üìÑ CSV log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/V29_review_predictions_with_preds.csv\n",
      "‚úÖ Review assignments (with audit) complete.\n",
      "Assignment summary: Counter({'neutral': 3193, 'happiness': 2928, 'surprise': 2839, 'anger': 2344, 'sadness': 1628, 'fear': 1318, 'unknown': 1066, 'questioning': 863, 'REVIEW': 590, 'contempt': 281, 'disgust': 261})\n",
      "‚úÖ Saved 265 clusters to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/review_predictions_clustered\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 15. Review & Relabel 'REVIEW' Predictions (with Audit Logging & Clustering)\n",
    "# --------------------------\n",
    "MINORITY_LABELS = [\"disgust\", \"contempt\", \"fear\", \"questioning\"]\n",
    "MINORITY_ENTROPY_THRESH = 0.6\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "REVIEW_CLUSTER_DIR = os.path.join(SAVE_DIR, \"review_predictions_clustered\")\n",
    "\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "os.makedirs(REVIEW_CLUSTER_DIR, exist_ok=True)\n",
    "\n",
    "# ---- If you HAVE NOT already generated review CSV (inference stage) ----\n",
    "if not os.path.exists(REVIEW_CSV_LOG):\n",
    "    review_log = []\n",
    "    image_paths = [\n",
    "        p for p in Path(IMAGE_DIR).rglob(\"*\")\n",
    "        if p.is_file() and p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "    ]\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                conf, pred_idx = torch.max(probs, dim=-1)\n",
    "            conf_val = conf.item()\n",
    "            pred_label = id2label[pred_idx.item()]\n",
    "            \n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1).item()\n",
    "\n",
    "            if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "                tag = \"unknown\"\n",
    "            elif conf_val < REVIEW_THRESHOLD:\n",
    "                tag = \"REVIEW\"\n",
    "            else:\n",
    "                tag = pred_label\n",
    "            \n",
    "            review_log.append({\n",
    "                \"image_path\": str(img_path),\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"confidence\": round(conf_val, 4),\n",
    "                \"entropy\": round(entropy, 4),\n",
    "                \"tag\": tag\n",
    "            })\n",
    "            \n",
    "            # For backward compatibility, still copy to REVIEW_BY_CLASS_DIR if tag is not \"unknown\"\n",
    "            if tag not in [\"unknown\"]:\n",
    "                target_dir = os.path.join(REVIEW_BY_CLASS_DIR, tag)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                shutil.copy(str(img_path), target_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with image: {img_path} | {e}\")\n",
    "            \n",
    "    pd.DataFrame(review_log).to_csv(REVIEW_CSV_LOG, index=False)\n",
    "    print(f\"‚úÖ Completed tagging + copying REVIEW predictions to: {REVIEW_BY_CLASS_DIR}\")\n",
    "    print(f\"üìÑ CSV log saved to: {REVIEW_CSV_LOG}\")\n",
    "\n",
    "# ---- If you already HAVE a review CSV (assignment/audit stage) ----\n",
    "df = pd.read_csv(REVIEW_CSV_LOG)\n",
    "review_assignment_log = []\n",
    "for _, row in df.iterrows():\n",
    "    path = row[\"image_path\"]\n",
    "    pred_label = row[\"predicted_label\"]\n",
    "    conf = float(row[\"confidence\"])\n",
    "    true_label = os.path.basename(os.path.dirname(path))\n",
    "    \n",
    "    entropy = float(row.get(\"entropy\", 0))  # default to 0 if not present\n",
    "    if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "        assigned = \"unknown\"\n",
    "    elif conf < REVIEW_THRESHOLD:\n",
    "        assigned = \"REVIEW\"\n",
    "    else:\n",
    "        assigned = pred_label\n",
    "    \n",
    "    dest_dir = os.path.join(REVIEW_BY_CLASS_DIR, assigned)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    shutil.copy(path, dest_dir)\n",
    "    review_assignment_log.append([path, true_label, pred_label, conf, assigned, entropy])\n",
    "\n",
    "log_df = pd.DataFrame(\n",
    "    review_assignment_log,\n",
    "    columns=[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\", \"entropy\"]\n",
    ")\n",
    "\n",
    "log_df.to_csv(os.path.join(SAVE_DIR, \"review_assignment_audit.csv\"), index=False)\n",
    "print(\"‚úÖ Review assignments (with audit) complete.\")\n",
    "\n",
    "print(\"Assignment summary:\", Counter(log_df[\"assigned_folder\"]))\n",
    "\n",
    "# ---- Perceptual hash clustering of review pool ----\n",
    "def phash_distance(hash1, hash2):\n",
    "    return hash1 - hash2\n",
    "\n",
    "PHASH_CLUSTER_THRESHOLD = 6\n",
    "image_paths = [row[0] for row in review_assignment_log if row[4] != \"unknown\"]  # assigned to a class\n",
    "\n",
    "hashes = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"L\").resize((64, 64))\n",
    "        hashes.append(hex_to_hash(str(phash(img))))\n",
    "    except Exception as e:\n",
    "        print(f\"phash error: {img_path} | {e}\")\n",
    "\n",
    "clusters = []\n",
    "used = set()\n",
    "for i, h1 in enumerate(hashes):\n",
    "    if i in used:\n",
    "        continue\n",
    "    cluster = [image_paths[i]]\n",
    "    used.add(i)\n",
    "    for j, h2 in enumerate(hashes):\n",
    "        if j <= i or j in used:\n",
    "            continue\n",
    "        if phash_distance(h1, h2) <= PHASH_CLUSTER_THRESHOLD:\n",
    "            cluster.append(image_paths[j])\n",
    "            used.add(j)\n",
    "    if len(cluster) > 1:\n",
    "        clusters.append(cluster)\n",
    "\n",
    "if not clusters:\n",
    "    print(f\"‚ö†Ô∏è No clusters found for review. {REVIEW_CLUSTER_DIR} will remain empty.\")\n",
    "else:\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        out_dir = os.path.join(REVIEW_CLUSTER_DIR, f\"cluster_{idx}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for p in cluster:\n",
    "            shutil.copy(p, out_dir)\n",
    "    print(f\"‚úÖ Saved {len(clusters)} clusters to {REVIEW_CLUSTER_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc464dc0-3204-453c-9a4c-4e63700ad6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flagged 1 hard negatives for ('contempt', 'questioning'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/review_hardneg_contempt_questioning.txt\n",
      "\n",
      "Flagged 14 hard negatives for ('fear', 'surprise'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/review_hardneg_fear_surprise.txt\n",
      "üîç Found 17311 total predictions (CSV) and 1185 REVIEW-tagged paths.\n",
      "üìÇ Grouped 1174 REVIEW images into folders by predicted label in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807/review_predictions_by_class\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 16. REVIEW Pool Diagnostics & Hard Confusion Mining\n",
    "# --------------------------\n",
    "\n",
    "# A. Flag hard confusion pairs for manual review\n",
    "REVIEW_CONFUSION_PAIRS = [(\"contempt\", \"questioning\"), (\"fear\", \"surprise\")]\n",
    "\n",
    "confusion_candidates = parse_review_confusions(REVIEW_CSV_LOG, REVIEW_CONFUSION_PAIRS)\n",
    "for pair, imgs in confusion_candidates.items():\n",
    "    print(f\"\\nFlagged {len(imgs)} hard negatives for {pair}:\")\n",
    "    out_path = os.path.join(SAVE_DIR, f\"review_hardneg_{pair[0]}_{pair[1]}.txt\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(imgs))\n",
    "    print(f\"  Saved list: {out_path}\")\n",
    "\n",
    "# B. Organize REVIEW-tagged images by predicted class (for curation)\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "review_txt_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "\n",
    "if os.path.exists(review_txt_path) and os.path.exists(csv_path):\n",
    "    with open(review_txt_path, \"r\") as f:\n",
    "        review_paths = {line.strip() for line in f.readlines()}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    count = 0\n",
    "\n",
    "    print(f\"üîç Found {len(df)} total predictions (CSV) and {len(review_paths)} REVIEW-tagged paths.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = row[\"image_path\"]\n",
    "        label = row[\"predicted_label\"]\n",
    "        conf = row[\"confidence\"]\n",
    "\n",
    "        if path in review_paths and label != \"REVIEW\":\n",
    "            dest_dir = os.path.join(REVIEW_SORT_DIR, label)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy(path, dest_dir)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"üìÇ Grouped {count} REVIEW images into folders by predicted label in: {REVIEW_SORT_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing review candidates file or prediction CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label name/id mapping:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: happiness\n",
      "4: neutral\n",
      "5: questioning\n",
      "6: sadness\n",
      "7: surprise\n",
      "8: contempt\n",
      "9: unknown\n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.9415        contempt  \n",
      "1      0.9549        contempt  \n",
      "2      0.9265         neutral  \n",
      "3      0.9661        contempt  \n",
      "4      0.9330        contempt  \n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.9415        contempt  \n",
      "1      0.9549        contempt  \n",
      "2      0.9265         neutral  \n",
      "3      0.9661        contempt  \n",
      "4      0.9330        contempt  \n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Visualization & Error Tracking\n",
    "# --------------------------\n",
    "\n",
    "print(\"Label name/id mapping:\")\n",
    "for idx, name in enumerate(LABEL_NAMES):\n",
    "    print(f\"{idx}: {name}\")\n",
    "\n",
    "# Defensive: Check that metrics file exists before plotting\n",
    "per_class_csv = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "if not os.path.exists(per_class_csv):\n",
    "    print(f\"‚ö†Ô∏è Metrics file {per_class_csv} not found.\")\n",
    "else:\n",
    "    metrics_df = pd.read_csv(per_class_csv)\n",
    "    last_row = metrics_df.iloc[-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    f1s = [last_row[f\"f1_{n}\"] for n in LABEL_NAMES]\n",
    "    ax.bar(LABEL_NAMES, f1s)\n",
    "    ax.set_title(\"Per-Class F1 (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_f1.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Bar plot of per-class entropy\n",
    "    entropies = [last_row[f\"entropy_{n}\"] for n in LABEL_NAMES]\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.bar(LABEL_NAMES, entropies)\n",
    "    ax.set_title(\"Per-Class Mean Entropy (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_entropy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram for REVIEW pool\n",
    "    review_counts = Counter()\n",
    "    if os.path.exists(REVIEW_SORT_DIR):\n",
    "        for label_dir in os.listdir(REVIEW_SORT_DIR):\n",
    "            count = len(os.listdir(os.path.join(REVIEW_SORT_DIR, label_dir)))\n",
    "            review_counts[label_dir] = count\n",
    "        plt.bar(review_counts.keys(), review_counts.values())\n",
    "        plt.title(\"REVIEW Pool Distribution by Predicted Class\")\n",
    "        plt.savefig(os.path.join(SAVE_DIR, \"review_pool_distribution.png\"))\n",
    "        plt.close()\n",
    "        # Flag if >70% in one class\n",
    "        total = sum(review_counts.values())\n",
    "        for label, count in review_counts.items():\n",
    "            if total > 0 and count / total > 0.7:\n",
    "                print(f\"‚ö†Ô∏è REVIEW pool highly imbalanced: {count/total:.1%} in '{label}'\")\n",
    "\n",
    "    # Audit print block (as before)\n",
    "    print(\"Sample review predictions (audit):\")\n",
    "    if 'log_df' in locals():\n",
    "        print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "    elif 'df' in locals():\n",
    "        print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "    else:\n",
    "        print(\"No review/audit DataFrame found for printing.\")\n",
    "\n",
    "# ‚úÖ AUDIT BLOCK\n",
    "print(\"Sample review predictions (audit):\")\n",
    "if 'log_df' in locals():\n",
    "    print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "elif 'df' in locals():\n",
    "    print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "else:\n",
    "    print(\"No review/audit DataFrame found for printing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55576ae4-91ae-4d68-b538-2b57a0b46089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All classes ready for deployment: F1 >= 0.8 and entropy <= 0.4\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Deployment Readiness Assertions and Flags\n",
    "# --------------------------\n",
    "\n",
    "# Load metrics\n",
    "metrics_df = pd.read_csv(os.path.join(SAVE_DIR, \"per_class_metrics.csv\"))\n",
    "last = metrics_df.iloc[-1]\n",
    "warn = False\n",
    "\n",
    "for cname in LABEL_NAMES:\n",
    "    f1 = last[f\"f1_{cname}\"]\n",
    "    entropy = last[f\"entropy_{cname}\"]\n",
    "    if f1 < 0.8:\n",
    "        print(f\"üö® F1 < 0.8 for class '{cname}': {f1:.2f}\")\n",
    "        warn = True\n",
    "    if entropy > 0.4:\n",
    "        print(f\"üö® Entropy > 0.4 for class '{cname}': {entropy:.2f}\")\n",
    "        warn = True\n",
    "\n",
    "if not warn:\n",
    "    print(\"‚úÖ All classes ready for deployment: F1 >= 0.8 and entropy <= 0.4\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some classes not deployment-ready! Address above issues before production.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e53267-5a68-47f7-b0c9-babcecdcfe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up Model Ensembling Analysis ---\n",
      "‚úÖ Dynamically selected models for ensembling:\n",
      "   - Model 1 (New):  V29_20250710_082807\n",
      "   - Model 2 (Prev): V28_20250709_153248\n",
      "\n",
      "--- Running Ensemble Analysis on Hard Cases ---\n",
      "\n",
      "Image: img_2617.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (New) Pred: neutral\n",
      "  - Model 2 (Prev) Pred: neutral\n",
      "  - ENSEMBLE Pred:   neutral (Confidence: 0.93)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'neutral'.\n",
      "\n",
      "Image: Iain_Richmond_0001.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (New) Pred: neutral\n",
      "  - Model 2 (Prev) Pred: neutral\n",
      "  - ENSEMBLE Pred:   neutral (Confidence: 0.94)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'neutral'.\n",
      "\n",
      "Image: Richard_Gephardt_0001.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (New) Pred: happiness\n",
      "  - Model 2 (Prev) Pred: happiness\n",
      "  - ENSEMBLE Pred:   happiness (Confidence: 0.80)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'happiness'.\n",
      "\n",
      "Image: img_1685.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (New) Pred: anger\n",
      "  - Model 2 (Prev) Pred: anger\n",
      "  - ENSEMBLE Pred:   anger (Confidence: 0.91)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'anger'.\n",
      "\n",
      "Image: Bill_Frist_0008.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (New) Pred: anger\n",
      "  - Model 2 (Prev) Pred: anger\n",
      "  - ENSEMBLE Pred:   anger (Confidence: 0.95)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'anger'.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 19. Model Ensembling Analysis (Fully Dynamic)\n",
    "# --------------------------\n",
    "\n",
    "print(\"--- Setting up Model Ensembling Analysis ---\")\n",
    "\n",
    "# Find all valid model directories that contain saved model weights\n",
    "all_model_dirs = [\n",
    "    os.path.join(MODEL_ROOT, d)\n",
    "    for d in os.listdir(MODEL_ROOT)\n",
    "    if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))\n",
    "       and (os.path.exists(os.path.join(MODEL_ROOT, d, \"model.safetensors\")) or \n",
    "            os.path.exists(os.path.join(MODEL_ROOT, d, \"pytorch_model.bin\")))\n",
    "]\n",
    "\n",
    "# Sort by version number and select the two most recent, including the one just trained\n",
    "# This now correctly includes the current SAVE_DIR as a candidate\n",
    "sorted_models = sorted(all_model_dirs, key=extract_version_from_path, reverse=True)\n",
    "\n",
    "if len(sorted_models) < 2:\n",
    "    print(\"‚ö†Ô∏è Found fewer than two model versions (including current). Skipping ensembling.\")\n",
    "else:\n",
    "    model_path_1 = sorted_models[0] # The model just trained\n",
    "    model_path_2 = sorted_models[1] # The previous model\n",
    "\n",
    "    print(f\"‚úÖ Dynamically selected models for ensembling:\")\n",
    "    print(f\"   - Model 1 (New):  {os.path.basename(model_path_1)}\")\n",
    "    print(f\"   - Model 2 (Prev): {os.path.basename(model_path_2)}\")\n",
    "\n",
    "    # Load the models\n",
    "    model_1 = AutoModelForImageClassification.from_pretrained(model_path_1).to(device).eval()\n",
    "    model_2 = AutoModelForImageClassification.from_pretrained(model_path_2).to(device).eval()\n",
    "    ensemble_models = [model_1, model_2]\n",
    "    \n",
    "    # --- Part B: Run Analysis on Misclassified Images ---\n",
    "    audit_csv_path = os.path.join(SAVE_DIR, \"review_assignment_audit.csv\")\n",
    "    if os.path.exists(audit_csv_path):\n",
    "        print(\"\\n--- Running Ensemble Analysis on Hard Cases ---\")\n",
    "        audit_df = pd.read_csv(audit_csv_path)\n",
    "        \n",
    "        # Filter for images that were misclassified by the newest model\n",
    "        misclassified_df = audit_df[audit_df['true_label'] != audit_df['pred_label']]\n",
    "        \n",
    "        if misclassified_df.empty:\n",
    "            print(\"‚úÖ No misclassified images found in the audit file. Nothing to analyze.\")\n",
    "        else:\n",
    "            for _, row in misclassified_df.head(5).iterrows():\n",
    "                image_path = row['image_path']\n",
    "                true_label = row['true_label']\n",
    "                \n",
    "                # This now calls the utility function defined in Cell In[4]\n",
    "                ensemble_pred, ensemble_conf, individual_preds = ensemble_predict(\n",
    "                    ensemble_models, processor, image_path, device=device\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nImage: {os.path.basename(image_path)}\")\n",
    "                print(f\"  - True Label:      {true_label}\")\n",
    "                print(f\"  - Model 1 (New) Pred: {individual_preds[0]}\")\n",
    "                print(f\"  - Model 2 (Prev) Pred: {individual_preds[1]}\")\n",
    "                print(f\"  - ENSEMBLE Pred:   {ensemble_pred} (Confidence: {ensemble_conf:.2f})\")\n",
    "                \n",
    "                if ensemble_pred == true_label:\n",
    "                    print(\"  - ‚úÖ SUCCESS: Ensemble corrected the misclassification!\")\n",
    "                else:\n",
    "                    print(f\"  - ‚ùå FAILURE: Ensemble also misclassified as '{ensemble_pred}'.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Could not find audit CSV at {audit_csv_path}. Skipping ensemble analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41407e4-724f-4f2a-8fc9-691b4f9191fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
