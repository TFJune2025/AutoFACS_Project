{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "053c00e3-60a1-4d59-b34e-0df8862c8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code results in extremely low accuracy of .2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f377b7-c4f5-4564-87ad-2c0c186c0187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:09:56.918747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:09:56.957023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:09:56.960418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:09:56.964163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# Enable mixed precision for faster training\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b8ee3b-a834-414a-ae76-d1f1f2979c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = 224  # EfficientNetV2 prefers a larger image size\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "AUGMENTED_DATASET_DIR = \"/home/natalyagrokh/img_datasets/combo_ferck_dataset_1\"  # Augmented dataset directory\n",
    "NUM_CLASSES = 8  # Number of unique emotions after merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20dda2b-bf5a-4ddc-8d6e-03ed938902bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean hidden or unexpected directories\n",
    "def clean_hidden_directories(directory):\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if class_name.startswith('.') or not os.path.isdir(class_path):\n",
    "            print(f\"Removing unexpected directory: {class_path}\")\n",
    "            os.rmdir(class_path)  # Remove empty hidden folders\n",
    "\n",
    "clean_hidden_directories(AUGMENTED_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004a028e-0877-4ff3-a894-9503ef1b8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Class Distribution: {'disgust': 2896, 'contempt': 216, 'fear': 5196, 'sadness': 6161, 'surprise': 4251, 'anger': 5088, 'neutral': 6198, 'happiness': 9196}\n",
      "Class to Index Mapping: {'disgust': 0, 'contempt': 1, 'fear': 2, 'sadness': 3, 'surprise': 4, 'anger': 5, 'neutral': 6, 'happiness': 7}\n",
      "Class Weights: {0: 1.692075276243094, 1: 22.68634259259259, 2: 0.9430812163202463, 3: 0.7953660120110372, 4: 1.1527287697012467, 5: 0.9630994496855346, 6: 0.7906179412713779, 7: 0.5328675511091779}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate Class Weights\n",
    "def calculate_class_distribution(dataset_dir):\n",
    "    distribution = {}\n",
    "    for class_name in os.listdir(dataset_dir):\n",
    "        # Skip hidden folders (like .ipynb_checkpoints)\n",
    "        if class_name.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            num_images = len([\n",
    "                img for img in os.listdir(class_path)\n",
    "                if img.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "            ])\n",
    "            distribution[class_name] = num_images\n",
    "    return distribution\n",
    "\n",
    "# Get the class distribution from the dataset directory\n",
    "new_distribution = calculate_class_distribution(AUGMENTED_DATASET_DIR)\n",
    "print(\"New Class Distribution:\", new_distribution)\n",
    "\n",
    "# Generate the labels and counts\n",
    "labels = list(new_distribution.keys())  # Class names\n",
    "counts = np.array(list(new_distribution.values()))  # Number of images per class\n",
    "\n",
    "# Create a mapping of class names to indices\n",
    "class_to_index = {label: i for i, label in enumerate(labels)}\n",
    "print(\"Class to Index Mapping:\", class_to_index)\n",
    "\n",
    "# Create the `y` array (class labels repeated by the number of images in each class)\n",
    "y = []\n",
    "for class_name, count in new_distribution.items():\n",
    "    y.extend([class_to_index[class_name]] * count)\n",
    "y = np.array(y)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y),  # Unique class indices\n",
    "    y=y  # Repeated class labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5d7be0-9ad8-48ed-8533-583701046438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39202 files belonging to 8 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:10:04.109103: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-27 21:10:04.109645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:10:04.114608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:10:04.117918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:10:04.436025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:10:04.438324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:10:04.440375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-27 21:10:04.442309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Dataset\n",
    "dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    AUGMENTED_DATASET_DIR,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Split into train and test datasets (80-20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb93275-cd65-4cd9-b93d-460cf334b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/ml_expressions/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Preprocess and Augment Training Data\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.grayscale_to_rgb(image)  # Convert grayscale to RGB for EfficientNet\n",
    "    return image, label  # Keep labels as integers\n",
    "\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomBrightness(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_image).map(\n",
    "    lambda x, y: (data_augmentation(x), y)\n",
    ")\n",
    "test_dataset = test_dataset.map(preprocess_image)\n",
    "\n",
    "# Prefetch for efficiency\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6c2ceb-2715-4668-ba39-752263867263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the Model\n",
    "base_model = EfficientNetV2B0(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.4),  # Regularization\n",
    "    Dense(NUM_CLASSES, activation=\"softmax\", dtype=\"float32\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e8be8b-8e1a-48ad-9b81-e8dfd5cacc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Model\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),  # Use sparse categorical loss\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856c08f5-de2a-4894-8bed-bff9a65b8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fine-Tune the Model\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:  # Freeze all but the last 50 layers\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9904b3-c0a2-4f5d-94b8-40a306ca45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=7,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4225e8be-022f-4c29-831d-deb5b499ab53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 21:10:36.391987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2025-01-27 21:10:37.128694: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x557d97d44c10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-01-27 21:10:37.128750: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-01-27 21:10:37.138821: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-27 21:10:37.343919: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/490 [==============================] - 331s 657ms/step - loss: 2.2538 - accuracy: 0.1885 - val_loss: 2.0827 - val_accuracy: 0.2360 - lr: 9.9486e-04\n",
      "Epoch 2/50\n",
      "490/490 [==============================] - 313s 639ms/step - loss: 2.2504 - accuracy: 0.1953 - val_loss: 2.1131 - val_accuracy: 0.2344 - lr: 9.8974e-04\n",
      "Epoch 3/50\n",
      "490/490 [==============================] - 311s 635ms/step - loss: 2.2475 - accuracy: 0.1967 - val_loss: 2.1597 - val_accuracy: 0.2351 - lr: 9.8464e-04\n",
      "Epoch 4/50\n",
      "490/490 [==============================] - ETA: 0s - loss: 2.2436 - accuracy: 0.1927  \n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0004897856852039695.\n",
      "490/490 [==============================] - 313s 638ms/step - loss: 2.2436 - accuracy: 0.1927 - val_loss: 2.1391 - val_accuracy: 0.1325 - lr: 9.7957e-04\n",
      "Epoch 5/50\n",
      "490/490 [==============================] - 317s 646ms/step - loss: 2.2451 - accuracy: 0.1916 - val_loss: 2.0907 - val_accuracy: 0.2346 - lr: 9.7453e-04\n",
      "Epoch 6/50\n",
      "490/490 [==============================] - 318s 650ms/step - loss: 2.2515 - accuracy: 0.1965 - val_loss: 2.1103 - val_accuracy: 0.2345 - lr: 9.6951e-04\n",
      "Epoch 7/50\n",
      "490/490 [==============================] - 320s 651ms/step - loss: 2.2502 - accuracy: 0.1954 - val_loss: 2.0479 - val_accuracy: 0.2346 - lr: 9.6452e-04\n",
      "Epoch 8/50\n",
      "490/490 [==============================] - 319s 650ms/step - loss: 2.2480 - accuracy: 0.1954 - val_loss: 2.1095 - val_accuracy: 0.2373 - lr: 9.5955e-04\n",
      "Epoch 9/50\n",
      "490/490 [==============================] - 319s 650ms/step - loss: 2.2382 - accuracy: 0.1989 - val_loss: 2.0455 - val_accuracy: 0.2350 - lr: 9.5461e-04\n",
      "Epoch 10/50\n",
      "490/490 [==============================] - 320s 653ms/step - loss: 2.2422 - accuracy: 0.2008 - val_loss: 2.0532 - val_accuracy: 0.2350 - lr: 9.4969e-04\n",
      "Epoch 11/50\n",
      "490/490 [==============================] - 316s 644ms/step - loss: 2.2441 - accuracy: 0.1971 - val_loss: 2.0726 - val_accuracy: 0.2354 - lr: 9.4480e-04\n",
      "Epoch 12/50\n",
      "490/490 [==============================] - ETA: 0s - loss: 2.2422 - accuracy: 0.1974  \n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00046996897435747087.\n",
      "490/490 [==============================] - 318s 649ms/step - loss: 2.2422 - accuracy: 0.1974 - val_loss: 2.1062 - val_accuracy: 0.2348 - lr: 9.3994e-04\n",
      "Epoch 13/50\n",
      "490/490 [==============================] - 311s 635ms/step - loss: 2.2420 - accuracy: 0.2001 - val_loss: 2.0878 - val_accuracy: 0.2340 - lr: 9.3510e-04\n",
      "Epoch 14/50\n",
      "490/490 [==============================] - 323s 659ms/step - loss: 2.2416 - accuracy: 0.1967 - val_loss: 2.0826 - val_accuracy: 0.2351 - lr: 9.3028e-04\n",
      "Epoch 15/50\n",
      "490/490 [==============================] - ETA: 0s - loss: 2.2413 - accuracy: 0.1971  \n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00046274616033770144.\n",
      "490/490 [==============================] - 318s 648ms/step - loss: 2.2413 - accuracy: 0.1971 - val_loss: 2.0801 - val_accuracy: 0.2346 - lr: 9.2549e-04\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train the Model with Class Weights\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    class_weight=class_weights  # Pass the computed weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de56f6-3254-426a-9398-0b20a11390a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save the Model\n",
    "model.save(\"efficientnet_emotion_model_augmented_with_weights.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions) (Local)",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
