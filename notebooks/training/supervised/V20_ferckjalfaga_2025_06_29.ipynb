{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V20 changes:\n",
    "    # section #2 - moved WeightedTrainer, modified_train from #15\n",
    "    # section #15 - updated entirely\n",
    "        # implement discriminative learning rates (also known as differential learning rates). \n",
    "        # unfroze classifier head & final 2 layers of ViT backbone\n",
    "        # give classifier head high learning rate \n",
    "        # gave deeper backbone layers very low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, precision_recall_fscore_support\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    GaussianBlur,\n",
    "    RandAugment,\n",
    "    RandomAffine,\n",
    "    RandomApply,\n",
    "    RandomPerspective,\n",
    "    RandomAdjustSharpness,\n",
    "    ToPILImage,\n",
    "    ToTensor\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    'anger', 'disgust', 'fear', 'happiness', 'neutral',\n",
    "    'questioning', 'sadness', 'surprise', 'contempt', 'unknown'\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "HARD_CLASS_NAMES = ['contempt', 'disgust', 'fear', 'questioning']\n",
    "hard_class_ids = [label2id[n] for n in HARD_CLASS_NAMES]\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\", VERSION_TAG)\n",
    "LOGITS_PATH = os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\")\n",
    "LABELS_PATH = os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Utility Functions (Metrics & Calibration)\n",
    "# --------------------------\n",
    "\n",
    "# üîç Compute perceptual hash for image similarity clustering (used in REVIEW and Disgust curation)\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Inject image_path BEFORE any map/filter\n",
    "def add_image_path(example):\n",
    "    # Handle DatasetsImage and PIL.Image types robustly\n",
    "    img_obj = example[\"image\"]\n",
    "    path = getattr(img_obj, \"filename\", None)\n",
    "    if path is None:\n",
    "        # Fallback: Try reconstructing path from folder and file (rarely needed)\n",
    "        if \"file\" in example:\n",
    "            path = os.path.join(BASE_PATH, example[\"file\"])\n",
    "        else:\n",
    "            path = \"\"\n",
    "    example[\"image_path\"] = path\n",
    "    return example\n",
    "\n",
    "\n",
    "# Reconcile labels as before, but preserve all fields\n",
    "def reconcile_labels(example):\n",
    "    label = example.get(\"label\", None)\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = example[\"image_path\"]\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example    \n",
    "\n",
    "\n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Use smoothed CE + confidence penalty\n",
    "        smooth_ce_loss = SmoothedCrossEntropyLoss(smoothing=0.05)\n",
    "        loss = smooth_ce_loss(logits, labels) + confidence_penalty(logits, beta=0.05)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)  # instead of eval_loss\n",
    "    return result\n",
    "    \n",
    "\n",
    "# üîÑ Smoothed Cross Entropy Loss (Œµ = 0.05)\n",
    "class SmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ‚ö†Ô∏è Confidence Penalty to Reduce Overconfidence\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "\n",
    "# üìä Compute Metrics with Confusion Matrix Logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(labels, preds, target_names=LABEL_NAMES, output_dict=True)\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw logits/labels for calibration or further analysis\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in LABEL_NAMES]\n",
    "    recalls = [report[name][\"recall\"] for name in LABEL_NAMES]\n",
    "    precisions = [report[name][\"precision\"] for name in LABEL_NAMES]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    # Sort for display only; CSV row stays in canonical label order\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "    epoch = getattr(trainer.state, \"epoch\", None) if \"trainer\" in globals() else None\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(LABEL_NAMES, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(LABEL_NAMES, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(LABEL_NAMES, precisions)},\n",
    "        **{f\"entropy_{n}\": [e] for n, e in entropy_per_class}\n",
    "    })\n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # Generate and print confusion matrix heatmap\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    print(\"\\nüîç Class entropies (sorted):\")\n",
    "    for class_name, entropy in sorted_entropy:\n",
    "        print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# üå°Ô∏è Apply Temperature Scaling for Calibration\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"‚ùå Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Loading logits from: {logits_path}\")\n",
    "    print(f\"üìÇ Loading labels from: {labels_path}\")\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()\n",
    "\n",
    "\n",
    "# üìà Plot Reliability Diagram (Calibration Curve)\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {output_path}\")\n",
    "\n",
    "# saving model and processor\n",
    "def save_model_and_processor(model, processor, save_dir, trainer=None):\n",
    "    print(f\"Saving model and processor to: {save_dir}\")\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Save processor\n",
    "    processor.save_pretrained(save_dir)\n",
    "    print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "    \n",
    "    # Save full model\n",
    "    model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "    print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save state dict\n",
    "    final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "    # Save trainer state\n",
    "    if trainer is not None:\n",
    "        try:\n",
    "            trainer.save_model(os.path.join(save_dir, \"backup_trainer_model\"))\n",
    "            print(\"‚úÖ Trainer backup saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass  # Not all systems have CUDA\n",
    "    print(\"‚úÖ Memory cleanup complete after save.\")\n",
    "\n",
    "\n",
    "# üö¶ Prints label distribution for a dataset\n",
    "    #only calling for ad hoc debugging, experiments, sanity checks \n",
    "def check_label_integrity(dataset, LABEL_NAMES, label2id):\n",
    "    # Count all mapped labels\n",
    "    label_counts = Counter(dataset['label'])\n",
    "    print(\"\\nüö® Label distribution after mapping (before split):\")\n",
    "    for label_id in range(len(LABEL_NAMES)):\n",
    "        label_name = LABEL_NAMES[label_id]\n",
    "        print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "\n",
    "    # Specifically highlight 'surprise'\n",
    "    surprise_id = label2id['surprise']\n",
    "    if label_counts.get(surprise_id, 0) == 0:\n",
    "        print(\"‚ùóWARNING: No 'surprise' images found after mapping!\")\n",
    "    elif label_counts[surprise_id] < 50:  # arbitrary threshold\n",
    "        print(f\"‚ö†Ô∏è Only {label_counts[surprise_id]} 'surprise' images found! Check curation or mapping.\")\n",
    "\n",
    "\n",
    "# üö¶ Prints label distribution for multiple datasets\n",
    "def check_all_label_integrity(datasets_dict, LABEL_NAMES, label2id):\n",
    "    for name, dataset in datasets_dict.items():\n",
    "        print(f\"\\nüö® Label distribution for: {name}\")\n",
    "        label_counts = Counter(dataset['label'])\n",
    "        for label_id in range(len(LABEL_NAMES)):\n",
    "            label_name = LABEL_NAMES[label_id]\n",
    "            print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "        surprise_id = label2id['surprise']\n",
    "        if label_counts.get(surprise_id, 0) == 0:\n",
    "            print(\"‚ùóWARNING: No 'surprise' images found in this split!\")\n",
    "        elif label_counts[surprise_id] < 50:\n",
    "            print(f\"‚ö†Ô∏è Only {label_counts[surprise_id]} 'surprise' images in {name}! Check curation or mapping.\")\n",
    "\n",
    "\n",
    "# --- Stronger Augmentation Utility ---\n",
    "def make_transform_function(processor, hard_class_ids):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = strong_aug if label in hard_class_ids else data_augment\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function  \n",
    "\n",
    "\n",
    "# Returns a boolean tensor: True if the prediction is low-confidence\n",
    "def is_uncertain(probs, threshold=0.85, entropy_thresh=1.3):\n",
    "    conf, _ = torch.max(probs, dim=-1)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1)\n",
    "    return (conf < threshold) | (entropy > entropy_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available model directories (sorted by version):\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V19_20250628_131955\n",
      "   Files: ['review_hardneg_contempt_questioning.txt', 'model.safetensors', 'per_class_entropy.png', 'review_predictions_clustered', 'V19_reliability_diagram_calibrated.png', '.DS_Store', 'contempt_clusters', 'label_snapshots', 'V19_augmentation_snapshot.csv', 'fear_clusters', 'config.json', 'sadness_clusters', 'labels_eval_V19.npy', 'disgust_clusters', 'review_predictions_by_class', 'review_hardneg_fear_surprise.txt', 'per_class_f1.png', 'review_pool_distribution.png', 'checkpoint-11472', 'V19_distribution_plot_20250628_131955.png', 'logs', 'per_class_metrics.csv', 'V19_calibrated_temperature.pt', 'review_assignment_audit.csv', 'V19_ferckjalfaga_2025_06_28.ipynb', 'V19_review_candidates.txt', '.ipynb_checkpoints', 'logits_eval_V19.npy', 'checkpoint-2868', 'questioning_clusters', 'V19_review_predictions_with_preds.csv', 'confusion_matrix_epoch_V19.png', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V18_20250623_092248\n",
      "   Files: ['review_hardneg_contempt_questioning.txt', 'model.safetensors', 'per_class_entropy.png', 'review_predictions_clustered', '.DS_Store', 'contempt_clusters', 'label_snapshots', 'fear_clusters', 'config.json', 'sadness_clusters', 'labels_eval_V18.npy', 'V18_review_candidates.txt', 'V18_calibrated_temperature.pt', 'V18_reliability_diagram_calibrated.png', 'disgust_clusters', 'review_predictions_by_class', 'review_hardneg_fear_surprise.txt', 'per_class_f1.png', 'V18_ferckjalfag_2025_06_23.ipynb', 'review_pool_distribution.png', 'checkpoint-11472', 'logs', 'per_class_metrics.csv', 'V18_review_predictions_with_preds.csv', 'review_assignment_audit.csv', '.ipynb_checkpoints', 'V18_augmentation_snapshot.csv', 'logits_eval_V18.npy', 'checkpoint-2868', 'confusion_matrix_epoch_V18.png', 'questioning_clusters', 'final_model.pth', 'preprocessor_config.json', 'V18_distribution_plot_20250623_092248.png']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V17_20250620_111524\n",
      "   Files: ['confusion_matrix_epoch_V17.png', 'review_hardneg_contempt_questioning.txt', 'model.safetensors', 'per_class_entropy.png', 'review_predictions_clustered', '.DS_Store', 'logits_eval_V17.npy', 'contempt_clusters', 'label_snapshots', 'fear_clusters', 'config.json', 'sadness_clusters', 'disgust_clusters', 'review_predictions_by_class', 'V17_calibrated_temperature.pt', 'review_hardneg_fear_surprise.txt', 'per_class_f1.png', 'review_pool_distribution.png', 'V17_review_predictions_with_preds.csv', 'logs', 'per_class_metrics.csv', 'V17_review_candidates.txt', 'labels_eval_V17.npy', 'review_assignment_audit.csv', '.ipynb_checkpoints', 'V17_distribution_plot_20250620_111524.png', 'V17_ferckjalfag_2025_06_20.ipynb', 'V17_reliability_diagram_calibrated.png', 'questioning_clusters', 'final_model.pth', 'V17_augmentation_snapshot.csv', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V16_20250618_144335\n",
      "   Files: ['review_hardneg_contempt_questioning.txt', 'model.safetensors', 'confusion_matrix_epoch_V16.png', 'per_class_entropy.png', 'V16_augmentation_snapshot.csv', 'review_predictions_clustered', '.DS_Store', 'logits_eval_V16.npy', 'contempt_clusters', 'label_snapshots', 'V16_review_candidates.txt', 'fear_clusters', 'config.json', 'sadness_clusters', 'disgust_clusters', 'review_predictions_by_class', 'review_hardneg_fear_surprise.txt', 'per_class_f1.png', 'review_pool_distribution.png', 'V16_distribution_plot_20250618_144335.png', 'V16_calibrated_temperature.pt', 'logs', 'per_class_metrics.csv', 'V16_reliability_diagram_calibrated.png', 'review_assignment_audit.csv', 'labels_eval_V16.npy', '.ipynb_checkpoints', 'V16_review_predictions_with_preds.csv', 'questioning_clusters', 'V16_ferckjalfag_2025_06_17.ipynb', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V15_20250616_154815\n",
      "   Files: ['model.safetensors', 'V15_distribution_plot_20250616_154815.png', 'review_predictions_clustered', 'confusion_matrix_epoch_V15.png', '.DS_Store', 'V15_calibrated_temperature.pt', 'contempt_clusters', 'label_snapshots', 'logits_eval_V15.npy', 'fear_clusters', 'config.json', 'sadness_clusters', 'disgust_clusters', 'review_predictions_by_class', 'V15_reliability_diagram_calibrated.png', 'V15_review_candidates.txt', 'V15_ferckjalfag_2025_06_16.ipynb', 'logs', 'per_class_metrics.csv', 'labels_eval_V15.npy', 'V15_augmentation_snapshot.csv', 'V15_review_predictions_with_preds.csv', '.ipynb_checkpoints', 'questioning_clusters', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959\n",
      "   Files: ['model.safetensors', 'V14_review_candidates.txt', 'confusion_matrix_epoch_V14.png', 'review_predictions_clustered', 'V14_reliability_diagram_calibrated.png', '.DS_Store', 'contempt_clusters', 'label_snapshots', 'logits_eval_V14.npy', 'fear_clusters', 'config.json', 'sadness_clusters', 'V14_augmentation_snapshot.csv', 'disgust_clusters', 'review_predictions_by_class', 'V14_ferckjalfag_2025_06_14.ipynb', 'logs', 'labels_eval_V14.npy', '.ipynb_checkpoints', 'V14_calibrated_temperature.pt', 'questioning_clusters', 'V14_review_predictions_with_preds.csv', 'final_model.pth', 'preprocessor_config.json', 'V14_distribution_plot_20250614_190959.png']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V13_20250527_161430\n",
      "   Files: ['model.safetensors', 'logits_eval_V13.npy', 'review_predictions_clustered', 'backup_trainer_model', '.DS_Store', 'label_snapshots', 'confusion_matrix_epoch_V13.png', 'V13_review_predictions_with_preds.csv', 'config.json', 'sadness_clusters', 'V13_ferckjalf_2025_05_27.ipynb', 'V13_reliability_diagram_calibrated.png', 'disgust_clusters', 'review_predictions_by_class', 'temperature_V13.txt', 'V13_augmentation_snapshot.csv', 'labels_eval_V13.npy', 'logs', 'V13_distribution_plot_20250527_161430.png', '.ipynb_checkpoints', 'questioning_clusters', 'V13_calibrated_temperature.pt', 'final_model.pth', 'preprocessor_config.json', 'V13_review_candidates.txt']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V12_20250519_171503\n",
      "   Files: ['V12_review_predictions_with_preds.csv', 'model.safetensors', 'V12_calibrated_temperature.pt', 'logits_eval_V12.npy', 'V12_reliability_diagram_calibrated.png', 'backup_trainer_model', '.DS_Store', 'confusion_matrix_epoch_V12.png', 'label_snapshots', 'config.json', 'disgust_clusters', 'review_predictions_by_class', 'V12_ferckjalf_2025_05_19.ipynb', 'V12_review_candidates.txt', 'labels_eval_V12.npy', 'V12_augmentation_snapshot.csv', 'logs', 'V12_distribution_plot_20250519_171503.png', 'questioning_clusters', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V11_20250516_120452\n",
      "   Files: ['logits_eval_V11.npy', 'model.safetensors', 'V14_20250531_160421_experimental', 'confusion_matrix_epoch_V11.png', 'backup_trainer_model', '.DS_Store', 'label_snapshots', 'config.json', 'disgust_clusters', 'review_predictions_by_class', 'V11_distribution_plot_20250516_120452.png', 'V11_ferckjalf_2025_05_16.ipynb', 'V11_review_predictions_with_preds.csv', 'labels_eval_V11.npy', 'logs', 'V11_calibrated_temperature.pt', '.ipynb_checkpoints', 'V11_review_candidates.txt', 'V11_reliability_diagram_calibrated.png', 'questioning_clusters', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V10_20250515_150651\n",
      "   Files: ['logits_eval_V10.npy', 'model.safetensors', 'V10_distribution_plot_20250515_150651.png', 'backup_trainer_model', '.DS_Store', 'confusion_matrix_epoch_V10.png', 'label_snapshots', 'V10_review_candidates.txt', 'config.json', 'V10_ferckjalf_2025_05_15.ipynb', 'V10_calibrated_temperature.pt', 'disgust_clusters', 'V15_20250613_140110', 'V14_20250612_094038', 'logs', 'labels_eval_V10.npy', 'V10_reliability_diagram_calibrated.png', '.ipynb_checkpoints', 'disgust_with_AUs', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V9_20250513_152634\n",
      "   Files: ['model.safetensors', 'backup_trainer_model', '.DS_Store', 'logits_eval_V9.npy', 'config.json', 'V9_ferckjalf_2025_05_13.ipynb', 'confusion_matrix_epoch_V9.png', 'labels_eval_V9.npy', 'V9_calibrated_temperature.pt', 'V9_review_candidates.txt', 'logs', '.ipynb_checkpoints', 'V9_distribution_plot_20250514_131858.png', 'V9_reliability_diagram_calibrated.png', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V8_20250503_190001\n",
      "   Files: ['model.safetensors', 'V8_distribution_plot_20250503_163313.png', '.DS_Store', 'config.json', 'V8_distribution_plot_20250503_190001.png', 'confusion_matrix_epoch_V8.png', 'logs', '.ipynb_checkpoints', 'V8_ferckjalf_2025_04_26.ipynb', 'final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V7_vit_final_independent\n",
      "   Files: ['events.out.tfevents.1742583470.104-171-202-99.12653.0', 'distribution_plot_20250326_035318.png', 'model.safetensors', '.DS_Store', 'config.json', 'events.out.tfevents.1742410645.104-171-202-163.7809.0', 'V7_ferckjalf_2025_04_19.ipynb', 'events.out.tfevents.1744483563.104-171-203-193.3100.0', 'V7_distribution_plot_20250420_031249.png', 'events.out.tfevents.1745095524.104-171-202-110.17789.0', 'events.out.tfevents.1745091345.104-171-202-110.14599.0', 'events.out.tfevents.1742487297.104-171-203-147.4942.0', 'V7_final_model.pth', 'events.out.tfevents.1742939255.104-171-203-66.28305.1', 'events.out.tfevents.1742739108.104-171-202-168.4431.0', '.ipynb_checkpoints', 'events.out.tfevents.1742938997.104-171-203-66.28305.0', 'events.out.tfevents.1742568732.104-171-202-99.2767.0', 'events.out.tfevents.1742668544.104-171-203-203.4192.0', 'V7_confusion_matrix_epoch.png', 'events.out.tfevents.1742668324.104-171-203-203.3879.0', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V6_vit_final_independent\n",
      "   Files: ['model.safetensors', 'V6_ferckja_2025_04_12.ipynb', 'config.json', 'V6_distribution_plot_20250413_004102.png', 'V6_ferckja_2025_02_28.ipynb', 'V6_final_model.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V5_vit_final_independent\n",
      "   Files: ['model.safetensors', 'V5_distribution_plot_20250326_035318.png', 'V5_ferckja_2025_03_25.ipynb', 'config.json', 'V5_final_model.pth', 'final_model_V5.pth', 'events.out.tfevents.1742586282.104-171-202-99.26463.0', 'prediction_distribution.png', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V4_vit_final_independent\n",
      "   Files: ['model.safetensors', 'V4_final_model.pth', 'config.json', 'v4_prediction_distribution.png', 'V4_ferckja_2025_02_28.ipynb', 'final_model_V4.pth', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V3_vit_final_independent\n",
      "   Files: ['model.safetensors', 'V3_final_model.pth', 'config.json', 'V3_ferckja_2025_03_21.ipynb', 'preprocessor_config.json']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V2_vit_final_independent\n",
      "   Files: ['V2_final_model.pth', 'V2_ferckja_2025_03_20.ipynb']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V1_vit_final_independent\n",
      "   Files: ['V1_ferckja_2025_02_28.ipynb', 'V1_final_model.pth']\n",
      " - /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V0_vit_final_independent\n",
      "   Files: ['V0_ferckja_2025_02_28.ipynb', '.ipynb_checkpoints']\n",
      "‚úÖ Auto-loaded model from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V19_20250628_131955\n",
      "Model classifier bias after head (re-)init: Parameter containing:\n",
      "tensor([-2.0012e-02, -4.5068e-04,  3.2776e-02, -1.0479e-03, -7.7234e-03,\n",
      "        -2.2387e-02, -1.7475e-02,  9.0398e-03,  3.0066e-02,  8.9271e-05],\n",
      "       requires_grad=True)\n",
      "Classifier bias requires_grad after head (re-)init: True\n",
      "üñ•Ô∏è Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Auto-Load Latest Pretrained Model and Processor\n",
    "# --------------------------\n",
    "\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "def extract_version(dirname):\n",
    "    # Extracts the version number as an integer for sorting, e.g., V15_... ‚Üí 15\n",
    "    match = re.match(r\"V(\\d+)\", os.path.basename(dirname))\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "model_dirs = [\n",
    "    os.path.join(MODEL_ROOT, d)\n",
    "    for d in os.listdir(MODEL_ROOT)\n",
    "    if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))\n",
    "]\n",
    "\n",
    "# Exclude SAVE_DIR (current output) by absolute path\n",
    "model_dirs = [d for d in model_dirs if os.path.abspath(d) != os.path.abspath(SAVE_DIR)]\n",
    "\n",
    "# Sort by version number, descending (highest first)\n",
    "model_dirs = sorted(model_dirs, key=extract_version, reverse=True)\n",
    "\n",
    "print(\"Available model directories (sorted by version):\")\n",
    "for d in model_dirs:\n",
    "    print(\" -\", d)\n",
    "    print(\"   Files:\", os.listdir(d))\n",
    "\n",
    "if len(model_dirs) < 1:\n",
    "    raise FileNotFoundError(\"‚ùå No earlier model folders found.\")\n",
    "\n",
    "model_path = model_dirs[0]\n",
    "print(f\"‚úÖ Auto-loaded model from: {model_path}\")\n",
    "\n",
    "# Load base model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Classifier head with single Linear layer\n",
    "model.classifier = nn.Linear(model.classifier.in_features, len(id2label))\n",
    "print(\"Model classifier bias after head (re-)init:\", model.classifier.bias)\n",
    "print(\"Classifier bias requires_grad after head (re-)init:\", model.classifier.bias.requires_grad)\n",
    "\n",
    "# Replace classification head to match current label schema\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Counting valid image files on disk for verification...\n",
      "‚úÖ Found 17501 image files in /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\n",
      "‚úÖ Datasets caching disabled for this run to ensure fresh data load.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bec6e96d47448cbb95906833babddab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38e9ccc30d54659a534a9ca23917e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Add file path to each record:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c638864255e74917920b14ecf1ee32aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Re-labeling dataset (preserving image_path):   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd605597c5cb49e9ac44bcf72f57b0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total examples after filtering: 17500\n",
      "Sample with path: /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset/anger/Abel_Pacheco_0002.jpg\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Load and Prepare Dataset (with filename preservation)\n",
    "# ==============================\n",
    "\n",
    "# --- Dynamic File Count ---\n",
    "print(\"üîç Counting valid image files on disk for verification...\")\n",
    "# This will recursively find all valid image files in your dataset directory\n",
    "expected_file_count = len(\n",
    "    [p for p in Path(BASE_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    ")\n",
    "print(f\"‚úÖ Found {expected_file_count} image files in {BASE_PATH}\")\n",
    "\n",
    "# Disable caching BEFORE loading\n",
    "datasets.disable_caching()\n",
    "print(\"‚úÖ Datasets caching disabled for this run to ensure fresh data load.\")\n",
    "\n",
    "# Step 1: Load dataset and capture filepaths\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=BASE_PATH,\n",
    "    split=\"train\" # No need to specify cache_dir when caching is off\n",
    ")\n",
    "\n",
    "# Only run ONCE and only here, so \"image_path\" is never dropped later!\n",
    "dataset = dataset.map(add_image_path, desc=\"Add file path to each record\")\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset (preserving image_path)\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "# ** Robust Verification **\n",
    "final_count = len(dataset)\n",
    "print(f\"‚úÖ Total examples after filtering: {final_count}\")\n",
    "print(\"Sample with path:\", dataset[0][\"image_path\"])\n",
    "\n",
    "# Assertion checks whether loaded count is very close to the disk count\n",
    "# Small tolerance accounts for any files that fail to load or be filtered\n",
    "assert abs(final_count - expected_file_count) < 10, \\\n",
    "    f\"Dataset size mismatch! Found {expected_file_count} files but loaded {final_count}.\"\n",
    "\n",
    "assert dataset[0].get(\"image_path\", None), \"image_path missing from first record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise', 'unknown'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 2302 examples\n",
      "  disgust: 309 examples\n",
      "  fear: 1432 examples\n",
      "  happiness: 2892 examples\n",
      "  neutral: 3334 examples\n",
      "  questioning: 1939 examples\n",
      "  sadness: 1706 examples\n",
      "  surprise: 2779 examples\n",
      "  contempt: 421 examples\n",
      "  unknown: 386 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes: ['contempt', 'disgust', 'unknown']\n",
      "\n",
      "üìÇ Image count per label folder:\n",
      "  anger: 2302 images\n",
      "  contempt: 421 images\n",
      "  disgust: 309 images\n",
      "  fear: 1432 images\n",
      "  happiness: 2892 images\n",
      "  neutral: 3334 images\n",
      "  questioning: 1939 images\n",
      "  sadness: 1706 images\n",
      "  surprise: 2779 images\n",
      "  unknown: 386 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Disgust hash clusters with more than 1 image:\n",
      "üîç Sadness hash clusters with more than 1 image:\n",
      "  - Cluster 958c52e1: 2 images copied for review\n",
      "  - Cluster ee9a8d33: 2 images copied for review\n",
      "  - Cluster d0890396: 2 images copied for review\n",
      "  - Cluster bb0d06f2: 2 images copied for review\n",
      "  - Cluster d7f00fa2: 2 images copied for review\n",
      "üîç Fear hash clusters with more than 1 image:\n",
      "  - Cluster 9ae56592: 2 images copied for review\n",
      "  - Cluster 91c8ee81: 2 images copied for review\n",
      "  - Cluster dae5a596: 2 images copied for review\n",
      "üîç Questioning hash clusters with more than 1 image:\n",
      "  - Cluster da014886: 2 images copied for review\n",
      "  - Cluster 9db42783: 2 images copied for review\n",
      "üîç Contempt hash clusters with more than 1 image:\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Perceptual Clustering for Ambiguous/Confused Classes\n",
    "# --------------------------\n",
    "\n",
    "CLUSTER_TARGETS = [\"disgust\", \"sadness\", \"fear\", \"questioning\", \"contempt\"]\n",
    "\n",
    "for class_name in CLUSTER_TARGETS:\n",
    "    class_dir = os.path.join(BASE_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"‚ö†Ô∏è Class dir not found: {class_dir} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    class_images = [\n",
    "        os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
    "        if is_valid_image(f)\n",
    "    ]\n",
    "    hash_map = {}\n",
    "    for path in class_images:\n",
    "        h = compute_hash(path)\n",
    "        if h:\n",
    "            hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "    cluster_dir = os.path.join(SAVE_DIR, f\"{class_name}_clusters\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"üîç {class_name.capitalize()} hash clusters with more than 1 image:\")\n",
    "    for h, paths in hash_map.items():\n",
    "        if len(paths) > 1:\n",
    "            cluster_path = os.path.join(cluster_dir, h)\n",
    "            os.makedirs(cluster_path, exist_ok=True)\n",
    "            for p in paths:\n",
    "                shutil.copy(p, cluster_path)\n",
    "            print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Targeted minority augmentation will apply to: ['disgust', 'questioning', 'contempt']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "label_freqs = Counter(dataset[\"label\"])\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "minority_by_name = [n for n in minority_by_name if n != \"unknown\"]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning', 'contempt']\n",
    "\n",
    "# Merge and deduplicate\n",
    "minority_class_names = list(set(minority_by_name + manual_focus_classes))\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"üéØ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c70dc430e0e45a795aa4e46a4cde37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmentation counts: {'anger': 2302, 'contempt': 421, 'disgust': 309, 'fear': 1432, 'happiness': 2892, 'neutral': 3334, 'questioning': 1939, 'sadness': 1706, 'surprise': 2779, 'unknown': 386}\n",
      "‚úÖ Saved augmentation snapshot to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/V20_augmentation_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# RandAugment for specific minority classes only\n",
    "minority_classes_names = minority_class_names\n",
    "minority_classes = [label2id[label] for label in minority_classes_names]\n",
    "\n",
    "minority_aug = T.Compose([\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "])\n",
    "\n",
    "# Augmentation counter tracking\n",
    "aug_count = Counter()\n",
    "\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "        aug_count[label] += 1\n",
    "\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function\n",
    "\n",
    "# After mapping finishes:\n",
    "dataset = dataset.map(make_transform_function(processor, minority_classes))\n",
    "formatted_counts = {LABEL_NAMES[k]: v for k, v in aug_count.items()}\n",
    "print(f\"‚úÖ Augmentation counts: {formatted_counts}\")\n",
    "\n",
    "# Explicitly log dataset snapshots (class distribution) to a \n",
    "# CSV or JSON after each run for easy future diffing and tracking\n",
    "snapshot_path = os.path.join(SAVE_DIR, f\"{VERSION}_augmentation_snapshot.csv\")\n",
    "aug_snapshot = pd.DataFrame.from_dict(dict(aug_count), orient='index', columns=['count'])\n",
    "aug_snapshot.to_csv(snapshot_path)\n",
    "\n",
    "print(f\"‚úÖ Saved augmentation snapshot to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö® Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set\n",
      "  anger       : 1825\n",
      "  disgust     : 244\n",
      "  fear        : 1154\n",
      "  happiness   : 2318\n",
      "  neutral     : 2702\n",
      "  questioning : 1545\n",
      "  sadness     : 1377\n",
      "  surprise    : 2185\n",
      "  contempt    : 347\n",
      "  unknown     : 303\n",
      "\n",
      "üö® Label distribution for: val set\n",
      "  anger       : 477\n",
      "  disgust     : 65\n",
      "  fear        : 278\n",
      "  happiness   : 574\n",
      "  neutral     : 632\n",
      "  questioning : 394\n",
      "  sadness     : 329\n",
      "  surprise    : 594\n",
      "  contempt    : 74\n",
      "  unknown     : 83\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e37d1e-a6fb-4e9f-aec5-ee945f11c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Saved label distribution snapshot: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/label_snapshots/V20_label_distribution.csv\n",
      "\n",
      "üö® Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set\n",
      "  anger       : 1825\n",
      "  disgust     : 244\n",
      "  fear        : 1154\n",
      "  happiness   : 2318\n",
      "  neutral     : 2702\n",
      "  questioning : 1545\n",
      "  sadness     : 1377\n",
      "  surprise    : 2185\n",
      "  contempt    : 347\n",
      "  unknown     : 303\n",
      "\n",
      "üö® Label distribution for: val set\n",
      "  anger       : 477\n",
      "  disgust     : 65\n",
      "  fear        : 278\n",
      "  happiness   : 574\n",
      "  neutral     : 632\n",
      "  questioning : 394\n",
      "  sadness     : 329\n",
      "  surprise    : 594\n",
      "  contempt    : 74\n",
      "  unknown     : 83\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Label Distribution Snapshot and Drift Monitor\n",
    "# --------------------------\n",
    "snapshot_dir = os.path.join(SAVE_DIR, \"label_snapshots\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Count current training labels\n",
    "train_label_names = [LABEL_NAMES[i] for i in train_dataset['label']]\n",
    "label_counts = pd.Series(train_label_names).value_counts().sort_index()\n",
    "label_counts.name = VERSION\n",
    "\n",
    "# Save snapshot CSV\n",
    "snapshot_path = os.path.join(snapshot_dir, f\"{VERSION}_label_distribution.csv\")\n",
    "label_counts.to_csv(snapshot_path)\n",
    "print(f\"üìä Saved label distribution snapshot: {snapshot_path}\")\n",
    "\n",
    "# Optionally compare to previous version\n",
    "previous_versions = sorted([\n",
    "    f for f in os.listdir(snapshot_dir) if f.endswith(\".csv\") and not f.startswith(VERSION)\n",
    "])\n",
    "if previous_versions:\n",
    "    latest_prev = previous_versions[-1]\n",
    "    prev_df = pd.read_csv(os.path.join(snapshot_dir, latest_prev), index_col=0)\n",
    "    diff = label_counts.subtract(prev_df.iloc[:, 0], fill_value=0)\n",
    "    print(\"üîç Label count change since last snapshot:\")\n",
    "    print(diff)\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({4: 3334, 3: 2892, 7: 2779, 0: 2302, 5: 1939, 6: 1706, 2: 1432, 8: 421, 9: 386, 1: 309})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd804cf3817743bc8fc93b1228e7bd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ef175a44834a27b6a780ff9699779d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b2d254e14740f49f79ae1188e83608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a65d5fff6064d6da9b1117723821ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25abf7b5a2c14608bf0f880fbb1fab1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e78f6705514ab083e5ece18d94dff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217a8aecb7404510bce74877f7c398fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a097ad660a0640ba904a243f76d6eaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9320f82c3c43248dba1111e5c8fcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f023c7d7df240aba5eb65881768ccd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({4: 3334, 3: 2892, 7: 2779, 0: 2302, 2: 2250, 5: 2250, 1: 2250, 6: 2250, 8: 2250, 9: 386})\n",
      "\n",
      "üö® Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set\n",
      "  anger       : 2302\n",
      "  disgust     : 2250\n",
      "  fear        : 2250\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 2250\n",
      "  sadness     : 2250\n",
      "  surprise    : 2779\n",
      "  contempt    : 2250\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: val set\n",
      "  anger       : 477\n",
      "  disgust     : 65\n",
      "  fear        : 278\n",
      "  happiness   : 574\n",
      "  neutral     : 632\n",
      "  questioning : 394\n",
      "  sadness     : 329\n",
      "  surprise    : 594\n",
      "  contempt    : 74\n",
      "  unknown     : 83\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11. Balance Dataset (with NO oversampling for 'unknown')\n",
    "# --------------------------\n",
    "MINORITY_CAP = 2250\n",
    "balanced_subsets = []\n",
    "label_counts = Counter(dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    class_name = LABEL_NAMES[label]\n",
    "    if class_name == \"unknown\":\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count < MINORITY_CAP:\n",
    "        multiplier = MINORITY_CAP // len(subset)\n",
    "        remainder = MINORITY_CAP % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        balanced_subsets.append(subset)\n",
    "    else:\n",
    "        # Append full set (no downsampling for majority classes)\n",
    "        balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))\n",
    "\n",
    "hard_classes = ['contempt', 'disgust', 'questioning', 'surprise', 'fear']\n",
    "hard_class_ids = [label2id[c] for c in hard_classes]\n",
    "\n",
    "# Calculate weights: Give hard classes 2x, others 1x\n",
    "weights = [2.0 if l in hard_class_ids else 1.0 for l in train_dataset[\"label\"]]\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9367aa62-f6a3-410e-9ce5-3e297a4cb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum split: 16664 easy, 0 hard\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 12. Curriculum Learning: Staged Hard/Easy Sample Scheduling (with preserved image_path)\n",
    "# ==========================\n",
    "\n",
    "# (A) Use audit_csv_path, or rebuild audit if needed\n",
    "audit_csv_path = os.path.join(SAVE_DIR, \"review_assignment_audit.csv\")\n",
    "if not os.path.exists(audit_csv_path):\n",
    "    # Rebuild (all records have image_path now!)\n",
    "    pd.DataFrame([{\"image_path\": ex[\"image_path\"], \"label\": ex[\"label\"]}\n",
    "                  for ex in dataset]).to_csv(audit_csv_path, index=False)\n",
    "    \n",
    "audit_df = pd.read_csv(audit_csv_path)\n",
    "\n",
    "# (B) Map image_path (full path or filename) to index\n",
    "dataset_path_to_idx = {\n",
    "    os.path.basename(ex[\"image_path\"]): i for i, ex in enumerate(dataset)\n",
    "}\n",
    "\n",
    "easy_idxs, hard_idxs = [], []\n",
    "for _, row in audit_df.iterrows():\n",
    "    path_val = row[\"image_path\"]\n",
    "    if not isinstance(path_val, str) or not path_val:\n",
    "        continue\n",
    "    basename = os.path.basename(path_val)\n",
    "    idx = dataset_path_to_idx.get(basename)\n",
    "    if idx is not None:\n",
    "        # For now, assign all as easy unless you have other columns\n",
    "        easy_idxs.append(idx)\n",
    "    else:\n",
    "        print(f\"Image in audit CSV not found in current dataset: {row['image_path']}\")\n",
    "\n",
    "easy_dataset = dataset.select(easy_idxs)\n",
    "hard_dataset = dataset.select(hard_idxs)\n",
    "print(f\"Curriculum split: {len(easy_dataset)} easy, {len(hard_dataset)} hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 13. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # ‚úÖ (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # ‚úÖ Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # ‚úÖ Log once per epoch\n",
    "    save_safetensors=True                  # ‚úÖ Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 14. Define Compute Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up optimizer with discriminative learning rates ---\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11472' max='14340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11472/14340 5:51:40 < 1:27:56, 0.54 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.545800</td>\n",
       "      <td>0.393153</td>\n",
       "      <td>0.977143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.434800</td>\n",
       "      <td>0.391516</td>\n",
       "      <td>0.977143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.429800</td>\n",
       "      <td>0.389998</td>\n",
       "      <td>0.977429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.389894</td>\n",
       "      <td>0.976571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.98       477\n",
      "     disgust       0.92      0.89      0.91        65\n",
      "        fear       0.93      0.95      0.94       278\n",
      "   happiness       0.99      1.00      1.00       574\n",
      "     neutral       0.99      0.99      0.99       632\n",
      " questioning       0.95      0.98      0.96       394\n",
      "     sadness       1.00      0.97      0.98       329\n",
      "    surprise       0.98      0.99      0.98       594\n",
      "    contempt       0.86      0.77      0.81        74\n",
      "     unknown       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           0.98      3500\n",
      "   macro avg       0.96      0.95      0.96      3500\n",
      "weighted avg       0.98      0.98      0.98      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 15 instances\n",
      "  - fear ‚Üí surprise: 7 instances\n",
      "  - surprise ‚Üí fear: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3170\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4777\n",
      "  - contempt: entropy = 0.4501\n",
      "  - disgust: entropy = 0.3494\n",
      "  - fear: entropy = 0.3442\n",
      "  - surprise: entropy = 0.3026\n",
      "  - happiness: entropy = 0.2935\n",
      "  - anger: entropy = 0.2917\n",
      "  - neutral: entropy = 0.2708\n",
      "  - sadness: entropy = 0.2700\n",
      "  - unknown: entropy = 0.2695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.98       477\n",
      "     disgust       0.92      0.92      0.92        65\n",
      "        fear       0.92      0.96      0.94       278\n",
      "   happiness       0.99      1.00      1.00       574\n",
      "     neutral       0.99      0.99      0.99       632\n",
      " questioning       0.96      0.97      0.96       394\n",
      "     sadness       1.00      0.97      0.98       329\n",
      "    surprise       0.99      0.98      0.98       594\n",
      "    contempt       0.87      0.78      0.82        74\n",
      "     unknown       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           0.98      3500\n",
      "   macro avg       0.96      0.96      0.96      3500\n",
      "weighted avg       0.98      0.98      0.98      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 14 instances\n",
      "  - surprise ‚Üí fear: 10 instances\n",
      "  - fear ‚Üí surprise: 6 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3161\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4687\n",
      "  - contempt: entropy = 0.4523\n",
      "  - fear: entropy = 0.3409\n",
      "  - disgust: entropy = 0.3249\n",
      "  - surprise: entropy = 0.3084\n",
      "  - anger: entropy = 0.2977\n",
      "  - neutral: entropy = 0.2866\n",
      "  - sadness: entropy = 0.2862\n",
      "  - happiness: entropy = 0.2627\n",
      "  - unknown: entropy = 0.2552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.98       477\n",
      "     disgust       0.92      0.94      0.93        65\n",
      "        fear       0.93      0.95      0.94       278\n",
      "   happiness       0.99      1.00      1.00       574\n",
      "     neutral       0.99      0.99      0.99       632\n",
      " questioning       0.96      0.97      0.96       394\n",
      "     sadness       1.00      0.97      0.98       329\n",
      "    surprise       0.98      0.98      0.98       594\n",
      "    contempt       0.87      0.78      0.82        74\n",
      "     unknown       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           0.98      3500\n",
      "   macro avg       0.96      0.96      0.96      3500\n",
      "weighted avg       0.98      0.98      0.98      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 14 instances\n",
      "  - surprise ‚Üí fear: 8 instances\n",
      "  - fear ‚Üí surprise: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3167\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4768\n",
      "  - contempt: entropy = 0.4658\n",
      "  - fear: entropy = 0.3578\n",
      "  - disgust: entropy = 0.3369\n",
      "  - anger: entropy = 0.2952\n",
      "  - surprise: entropy = 0.2895\n",
      "  - neutral: entropy = 0.2879\n",
      "  - sadness: entropy = 0.2818\n",
      "  - happiness: entropy = 0.2726\n",
      "  - unknown: entropy = 0.2518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.98       477\n",
      "     disgust       0.92      0.94      0.93        65\n",
      "        fear       0.92      0.96      0.94       278\n",
      "   happiness       0.99      1.00      1.00       574\n",
      "     neutral       0.99      0.99      0.99       632\n",
      " questioning       0.95      0.97      0.96       394\n",
      "     sadness       1.00      0.97      0.98       329\n",
      "    surprise       0.99      0.98      0.98       594\n",
      "    contempt       0.85      0.78      0.82        74\n",
      "     unknown       1.00      1.00      1.00        83\n",
      "\n",
      "    accuracy                           0.98      3500\n",
      "   macro avg       0.96      0.96      0.96      3500\n",
      "weighted avg       0.98      0.98      0.98      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 14 instances\n",
      "  - surprise ‚Üí fear: 10 instances\n",
      "  - questioning ‚Üí contempt: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3148\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4791\n",
      "  - contempt: entropy = 0.4562\n",
      "  - disgust: entropy = 0.3416\n",
      "  - fear: entropy = 0.3375\n",
      "  - anger: entropy = 0.3057\n",
      "  - surprise: entropy = 0.2950\n",
      "  - sadness: entropy = 0.2819\n",
      "  - neutral: entropy = 0.2762\n",
      "  - happiness: entropy = 0.2676\n",
      "  - unknown: entropy = 0.2563\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Final Diagnostics ---\n",
      "Classifier head weights (after train): Parameter containing:\n",
      "tensor([[ 0.0402,  0.0066, -0.0083,  ..., -0.0015, -0.0325, -0.0073],\n",
      "        [ 0.0093,  0.0425,  0.0462,  ..., -0.0046,  0.0006,  0.0021],\n",
      "        [-0.0044, -0.0025,  0.0231,  ..., -0.0146,  0.0081, -0.0081],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0062, -0.0105,  ...,  0.0334, -0.0081, -0.0319],\n",
      "        [-0.0120,  0.0432, -0.0044,  ...,  0.0316,  0.0321, -0.0299],\n",
      "        [-0.0187,  0.0152,  0.0117,  ..., -0.0304,  0.0137, -0.0470]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Classifier head bias (after train): Parameter containing:\n",
      "tensor([-0.0221,  0.0044,  0.0265, -0.0082, -0.0072, -0.0290, -0.0142,  0.0072,\n",
      "         0.0447, -0.0110], device='mps:0', requires_grad=True)\n",
      "Classifier bias requires_grad (after train): True\n",
      "Classifier weight in optimizer param group: True\n",
      "Classifier bias in optimizer param group: True\n",
      "Saving model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\n",
      "‚úÖ Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\n",
      "‚úÖ Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\n",
      "‚úÖ State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/final_model.pth\n",
      "‚úÖ Memory cleanup complete after save.\n",
      "--- Model saved to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956 ---\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 15. Optimizer, Scheduler, and Training (V20 Revision)\n",
    "# --------------------------\n",
    "\n",
    "# --- Part A: Discriminative Learning Rate Optimizer Setup ---\n",
    "print(\"--- Setting up optimizer with discriminative learning rates ---\")\n",
    "\n",
    "# Define different learning rates for the new head and the backbone layers\n",
    "head_lr = 5e-5      # Higher learning rate for the classifier head\n",
    "backbone_lr = 2e-7  # Much lower learning rate for the last two backbone layers\n",
    "\n",
    "# First, ensure all layers are frozen by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classifier head to be trained\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last 2 layers of the ViT encoder for fine-tuning.\n",
    "for name, param in model.vit.encoder.layer[-2:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create parameter groups for the optimizer to handle different learning rates\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.classifier.parameters(), 'lr': head_lr},\n",
    "    {'params': model.vit.encoder.layer[-2:].parameters(), 'lr': backbone_lr}\n",
    "]\n",
    "\n",
    "# Create the AdamW optimizer with the specified parameter groups.\n",
    "# We will pass this to the Trainer.\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01) # Using your weight_decay value\n",
    "\n",
    "# --- Part B: Trainer Initialization ---\n",
    "# Modify training args for learning rate scheduling and early stopping\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.metric_for_best_model = \"eval_loss\"\n",
    "training_args.evaluation_strategy = \"epoch\"\n",
    "training_args.save_strategy = \"epoch\"\n",
    "\n",
    "# Add EarlyStoppingCallback\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "# Initialize WeightedTrainer, passing our new custom optimizer.\n",
    "# We pass `None` for the scheduler, as you define a custom one later.\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    "    optimizers=(optimizer, None),  # Pass the newly created optimizer here\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "# --- Part C: Custom Scheduler Setup ---\n",
    "# This section is preserved from your V19 notebook.\n",
    "# It now correctly uses the new optimizer that we passed to the trainer.\n",
    "scheduler = CosineAnnealingWarmRestarts(trainer.optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "# Add scheduler step logic inside the training loop by monkey-patching the train method\n",
    "original_train = trainer.train\n",
    "\n",
    "trainer.train = modified_train # Overwrite the trainer's train method\n",
    "\n",
    "# --- Part E: Train the Model and Finalize ---\n",
    "# Fine-tune model\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "trainer.train()\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "\n",
    "# === Diagnostics: Model Head and Optimizer State ===\n",
    "print(\"\\n--- Final Diagnostics ---\")\n",
    "print(\"Classifier head weights (after train):\", model.classifier.weight)\n",
    "print(\"Classifier head bias (after train):\", model.classifier.bias)\n",
    "print(\"Classifier bias requires_grad (after train):\", model.classifier.bias.requires_grad)\n",
    "\n",
    "# Confirm all head parameters are present in the optimizer\n",
    "opt_params = set([p for group in trainer.optimizer.param_groups for p in group['params']])\n",
    "print(\"Classifier weight in optimizer param group:\", model.classifier.weight in opt_params)\n",
    "print(\"Classifier bias in optimizer param group:\", model.classifier.bias in opt_params)\n",
    "\n",
    "# Enforce assertions to guarantee the head was trained\n",
    "assert model.classifier.weight in opt_params, \"CRITICAL ERROR: Classifier weight NOT in optimizer!\"\n",
    "assert model.classifier.bias in opt_params, \"CRITICAL ERROR: Classifier bias NOT in optimizer!\"\n",
    "\n",
    "# Save the final model and processor\n",
    "save_model_and_processor(model, processor, SAVE_DIR)\n",
    "print(f\"--- Model saved to {SAVE_DIR} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # 16. Rescue & Save from Last Checkpoint (after training)\n",
    "# # --------------------------\n",
    "# #in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"‚úÖ Processor manually re-saved.\")\n",
    "\n",
    "# # Use parent directory of SAVE_DIR to locate latest V* folder\n",
    "# parent_dir = os.path.dirname(SAVE_DIR)\n",
    "# v_folders = [\n",
    "#     d for d in os.listdir(parent_dir)\n",
    "#     if os.path.isdir(os.path.join(parent_dir, d)) and d.startswith(\"V\")\n",
    "# ]\n",
    "\n",
    "# def extract_timestamp(name):\n",
    "#     try:\n",
    "#         _, date_str, time_str = name.split(\"_\")\n",
    "#         return datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "#     except Exception:\n",
    "#         return datetime.min\n",
    "\n",
    "# latest_version_folder = max(v_folders, key=extract_timestamp)\n",
    "# latest_version_path = os.path.join(parent_dir, latest_version_folder)\n",
    "# print(f\"üóÇÔ∏è Using latest version folder: {latest_version_path}\")\n",
    "\n",
    "# # Locate latest checkpoint within that version folder\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(latest_version_path, d)\n",
    "#     for d in os.listdir(latest_version_path)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(latest_version_path, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"‚ùå No checkpoint found in latest version folder.\")\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # Load model and processor from latest checkpoint and save them\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(latest_version_path)\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"‚úÖ Model reloaded for inference.\")\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [06:23<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "üìù Saved REVIEW file paths to V20_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/V20_distribution_plot_20250629_134956.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"üìù Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\n",
      "üìÇ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/logits_eval_V20.npy\n",
      "üìÇ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/labels_eval_V20.npy\n",
      "‚úÖ Optimal temperature: 1.1138\n",
      "‚úÖ Calibrated Log Loss: 0.1423\n",
      "üìä Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/V20_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 19. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"üìÅ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaeec19-d4c7-43a6-80cb-86aafd573989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed tagging + copying REVIEW predictions to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/review_predictions_by_class\n",
      "üìÑ CSV log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/V20_review_predictions_with_preds.csv\n",
      "‚úÖ Review assignments (with audit) complete.\n",
      "Assignment summary: Counter({'neutral': 3189, 'happiness': 2944, 'surprise': 2900, 'anger': 2333, 'sadness': 1610, 'fear': 1368, 'unknown': 1205, 'questioning': 684, 'REVIEW': 596, 'contempt': 288, 'disgust': 243})\n",
      "‚úÖ Saved 267 clusters to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/review_predictions_clustered\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 20. Review & Relabel 'REVIEW' Predictions (with Audit Logging & Clustering)\n",
    "# --------------------------\n",
    "MINORITY_LABELS = [\"disgust\", \"contempt\", \"fear\", \"questioning\"]\n",
    "MINORITY_ENTROPY_THRESH = 0.6\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "REVIEW_CLUSTER_DIR = os.path.join(SAVE_DIR, \"review_predictions_clustered\")\n",
    "\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "os.makedirs(REVIEW_CLUSTER_DIR, exist_ok=True)\n",
    "\n",
    "# ---- If you HAVE NOT already generated review CSV (inference stage) ----\n",
    "if not os.path.exists(REVIEW_CSV_LOG):\n",
    "    review_log = []\n",
    "    image_paths = [\n",
    "        p for p in Path(IMAGE_DIR).rglob(\"*\")\n",
    "        if p.is_file() and p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "    ]\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                conf, pred_idx = torch.max(probs, dim=-1)\n",
    "            conf_val = conf.item()\n",
    "            pred_label = id2label[pred_idx.item()]\n",
    "            \n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1).item()\n",
    "\n",
    "            if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "                tag = \"unknown\"\n",
    "            elif conf_val < REVIEW_THRESHOLD:\n",
    "                tag = \"REVIEW\"\n",
    "            else:\n",
    "                tag = pred_label\n",
    "            \n",
    "            review_log.append({\n",
    "                \"image_path\": str(img_path),\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"confidence\": round(conf_val, 4),\n",
    "                \"entropy\": round(entropy, 4),\n",
    "                \"tag\": tag\n",
    "            })\n",
    "            \n",
    "            # For backward compatibility, still copy to REVIEW_BY_CLASS_DIR if tag is not \"unknown\"\n",
    "            if tag not in [\"unknown\"]:\n",
    "                target_dir = os.path.join(REVIEW_BY_CLASS_DIR, tag)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                shutil.copy(str(img_path), target_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with image: {img_path} | {e}\")\n",
    "            \n",
    "    pd.DataFrame(review_log).to_csv(REVIEW_CSV_LOG, index=False)\n",
    "    print(f\"‚úÖ Completed tagging + copying REVIEW predictions to: {REVIEW_BY_CLASS_DIR}\")\n",
    "    print(f\"üìÑ CSV log saved to: {REVIEW_CSV_LOG}\")\n",
    "\n",
    "# ---- If you already HAVE a review CSV (assignment/audit stage) ----\n",
    "df = pd.read_csv(REVIEW_CSV_LOG)\n",
    "review_assignment_log = []\n",
    "for _, row in df.iterrows():\n",
    "    path = row[\"image_path\"]\n",
    "    pred_label = row[\"predicted_label\"]\n",
    "    conf = float(row[\"confidence\"])\n",
    "    true_label = os.path.basename(os.path.dirname(path))\n",
    "    \n",
    "    entropy = float(row.get(\"entropy\", 0))  # default to 0 if not present\n",
    "    if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "        assigned = \"unknown\"\n",
    "    elif conf < REVIEW_THRESHOLD:\n",
    "        assigned = \"REVIEW\"\n",
    "    else:\n",
    "        assigned = pred_label\n",
    "    \n",
    "    dest_dir = os.path.join(REVIEW_BY_CLASS_DIR, assigned)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    shutil.copy(path, dest_dir)\n",
    "    review_assignment_log.append([path, true_label, pred_label, conf, assigned, entropy])\n",
    "\n",
    "log_df = pd.DataFrame(\n",
    "    review_assignment_log,\n",
    "    columns=[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\", \"entropy\"]\n",
    ")\n",
    "\n",
    "log_df.to_csv(os.path.join(SAVE_DIR, \"review_assignment_audit.csv\"), index=False)\n",
    "print(\"‚úÖ Review assignments (with audit) complete.\")\n",
    "\n",
    "print(\"Assignment summary:\", Counter(log_df[\"assigned_folder\"]))\n",
    "\n",
    "# ---- Perceptual hash clustering of review pool ----\n",
    "def phash_distance(hash1, hash2):\n",
    "    return hash1 - hash2\n",
    "\n",
    "PHASH_CLUSTER_THRESHOLD = 6\n",
    "image_paths = [row[0] for row in review_assignment_log if row[4] != \"unknown\"]  # assigned to a class\n",
    "\n",
    "hashes = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"L\").resize((64, 64))\n",
    "        hashes.append(hex_to_hash(str(phash(img))))\n",
    "    except Exception as e:\n",
    "        print(f\"phash error: {img_path} | {e}\")\n",
    "\n",
    "clusters = []\n",
    "used = set()\n",
    "for i, h1 in enumerate(hashes):\n",
    "    if i in used:\n",
    "        continue\n",
    "    cluster = [image_paths[i]]\n",
    "    used.add(i)\n",
    "    for j, h2 in enumerate(hashes):\n",
    "        if j <= i or j in used:\n",
    "            continue\n",
    "        if phash_distance(h1, h2) <= PHASH_CLUSTER_THRESHOLD:\n",
    "            cluster.append(image_paths[j])\n",
    "            used.add(j)\n",
    "    if len(cluster) > 1:\n",
    "        clusters.append(cluster)\n",
    "\n",
    "if not clusters:\n",
    "    print(f\"‚ö†Ô∏è No clusters found for review. {REVIEW_CLUSTER_DIR} will remain empty.\")\n",
    "else:\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        out_dir = os.path.join(REVIEW_CLUSTER_DIR, f\"cluster_{idx}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for p in cluster:\n",
    "            shutil.copy(p, out_dir)\n",
    "    print(f\"‚úÖ Saved {len(clusters)} clusters to {REVIEW_CLUSTER_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc464dc0-3204-453c-9a4c-4e63700ad6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flagged 11 hard negatives for ('contempt', 'questioning'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/review_hardneg_contempt_questioning.txt\n",
      "\n",
      "Flagged 14 hard negatives for ('fear', 'surprise'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/review_hardneg_fear_surprise.txt\n",
      "üîç Found 17360 total predictions (CSV) and 1309 REVIEW-tagged paths.\n",
      "üìÇ Grouped 1298 REVIEW images into folders by predicted label in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956/review_predictions_by_class\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 21. REVIEW Pool Diagnostics & Hard Confusion Mining\n",
    "# --------------------------\n",
    "\n",
    "# A. Flag hard confusion pairs for manual review\n",
    "REVIEW_CONFUSION_PAIRS = [(\"contempt\", \"questioning\"), (\"fear\", \"surprise\")]\n",
    "\n",
    "def parse_review_confusions(csv_path, confusion_pairs):\n",
    "    import csv\n",
    "    flagged_imgs = {pair: [] for pair in confusion_pairs}\n",
    "    with open(csv_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            pred = row[\"predicted_label\"]\n",
    "            true = os.path.basename(os.path.dirname(row[\"image_path\"]))\n",
    "            conf = float(row[\"confidence\"])\n",
    "            for a, b in confusion_pairs:\n",
    "                if ((pred == a and true == b) or (pred == b and true == a)) and conf < 0.8:\n",
    "                    flagged_imgs[(a, b)].append(row[\"image_path\"])\n",
    "    return flagged_imgs\n",
    "\n",
    "confusion_candidates = parse_review_confusions(REVIEW_CSV_LOG, REVIEW_CONFUSION_PAIRS)\n",
    "for pair, imgs in confusion_candidates.items():\n",
    "    print(f\"\\nFlagged {len(imgs)} hard negatives for {pair}:\")\n",
    "    out_path = os.path.join(SAVE_DIR, f\"review_hardneg_{pair[0]}_{pair[1]}.txt\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(imgs))\n",
    "    print(f\"  Saved list: {out_path}\")\n",
    "\n",
    "# B. Organize REVIEW-tagged images by predicted class (for curation)\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "review_txt_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "\n",
    "if os.path.exists(review_txt_path) and os.path.exists(csv_path):\n",
    "    with open(review_txt_path, \"r\") as f:\n",
    "        review_paths = {line.strip() for line in f.readlines()}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    count = 0\n",
    "\n",
    "    print(f\"üîç Found {len(df)} total predictions (CSV) and {len(review_paths)} REVIEW-tagged paths.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = row[\"image_path\"]\n",
    "        label = row[\"predicted_label\"]\n",
    "        conf = row[\"confidence\"]\n",
    "\n",
    "        if path in review_paths and label != \"REVIEW\":\n",
    "            dest_dir = os.path.join(REVIEW_SORT_DIR, label)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy(path, dest_dir)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"üìÇ Grouped {count} REVIEW images into folders by predicted label in: {REVIEW_SORT_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing review candidates file or prediction CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label name/id mapping:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: happiness\n",
      "4: neutral\n",
      "5: questioning\n",
      "6: sadness\n",
      "7: surprise\n",
      "8: contempt\n",
      "9: unknown\n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.9031        contempt  \n",
      "1      0.9504        contempt  \n",
      "2      0.9480         neutral  \n",
      "3      0.9472        contempt  \n",
      "4      0.9445        contempt  \n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.9031        contempt  \n",
      "1      0.9504        contempt  \n",
      "2      0.9480         neutral  \n",
      "3      0.9472        contempt  \n",
      "4      0.9445        contempt  \n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 22. Visualization & Error Tracking\n",
    "# --------------------------\n",
    "\n",
    "print(\"Label name/id mapping:\")\n",
    "for idx, name in enumerate(LABEL_NAMES):\n",
    "    print(f\"{idx}: {name}\")\n",
    "\n",
    "# Defensive: Check that metrics file exists before plotting\n",
    "per_class_csv = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "if not os.path.exists(per_class_csv):\n",
    "    print(f\"‚ö†Ô∏è Metrics file {per_class_csv} not found.\")\n",
    "else:\n",
    "    metrics_df = pd.read_csv(per_class_csv)\n",
    "    last_row = metrics_df.iloc[-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    f1s = [last_row[f\"f1_{n}\"] for n in LABEL_NAMES]\n",
    "    ax.bar(LABEL_NAMES, f1s)\n",
    "    ax.set_title(\"Per-Class F1 (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_f1.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Bar plot of per-class entropy\n",
    "    entropies = [last_row[f\"entropy_{n}\"] for n in LABEL_NAMES]\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.bar(LABEL_NAMES, entropies)\n",
    "    ax.set_title(\"Per-Class Mean Entropy (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_entropy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram for REVIEW pool\n",
    "    review_counts = Counter()\n",
    "    if os.path.exists(REVIEW_SORT_DIR):\n",
    "        for label_dir in os.listdir(REVIEW_SORT_DIR):\n",
    "            count = len(os.listdir(os.path.join(REVIEW_SORT_DIR, label_dir)))\n",
    "            review_counts[label_dir] = count\n",
    "        plt.bar(review_counts.keys(), review_counts.values())\n",
    "        plt.title(\"REVIEW Pool Distribution by Predicted Class\")\n",
    "        plt.savefig(os.path.join(SAVE_DIR, \"review_pool_distribution.png\"))\n",
    "        plt.close()\n",
    "        # Flag if >70% in one class\n",
    "        total = sum(review_counts.values())\n",
    "        for label, count in review_counts.items():\n",
    "            if total > 0 and count / total > 0.7:\n",
    "                print(f\"‚ö†Ô∏è REVIEW pool highly imbalanced: {count/total:.1%} in '{label}'\")\n",
    "\n",
    "    # Audit print block (as before)\n",
    "    print(\"Sample review predictions (audit):\")\n",
    "    if 'log_df' in locals():\n",
    "        print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "    elif 'df' in locals():\n",
    "        print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "    else:\n",
    "        print(\"No review/audit DataFrame found for printing.\")\n",
    "\n",
    "# ‚úÖ AUDIT BLOCK\n",
    "print(\"Sample review predictions (audit):\")\n",
    "if 'log_df' in locals():\n",
    "    print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "elif 'df' in locals():\n",
    "    print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "else:\n",
    "    print(\"No review/audit DataFrame found for printing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55576ae4-91ae-4d68-b538-2b57a0b46089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Entropy > 0.4 for class 'questioning': 0.48\n",
      "üö® Entropy > 0.4 for class 'contempt': 0.46\n",
      "‚ö†Ô∏è Some classes not deployment-ready! Address above issues before production.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 23. Deployment Readiness Assertions and Flags\n",
    "# --------------------------\n",
    "\n",
    "# Load metrics\n",
    "metrics_df = pd.read_csv(os.path.join(SAVE_DIR, \"per_class_metrics.csv\"))\n",
    "last = metrics_df.iloc[-1]\n",
    "warn = False\n",
    "\n",
    "for cname in LABEL_NAMES:\n",
    "    f1 = last[f\"f1_{cname}\"]\n",
    "    entropy = last[f\"entropy_{cname}\"]\n",
    "    if f1 < 0.8:\n",
    "        print(f\"üö® F1 < 0.8 for class '{cname}': {f1:.2f}\")\n",
    "        warn = True\n",
    "    if entropy > 0.4:\n",
    "        print(f\"üö® Entropy > 0.4 for class '{cname}': {entropy:.2f}\")\n",
    "        warn = True\n",
    "\n",
    "if not warn:\n",
    "    print(\"‚úÖ All classes ready for deployment: F1 >= 0.8 and entropy <= 0.4\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some classes not deployment-ready! Address above issues before production.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e53267-5a68-47f7-b0c9-babcecdcfe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 24. Model Ensembling: Averaging V15 and V16 Predictions\n",
    "# --------------------------\n",
    "\n",
    "V19_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V19_20250628_131955\"\n",
    "V20_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\"\n",
    "\n",
    "def ensemble_predict(models, processor, image_path, device=\"cpu\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    softmaxes = []\n",
    "    for m in models:\n",
    "        with torch.no_grad():\n",
    "            logits = m(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            softmaxes.append(probs)\n",
    "    avg_probs = np.mean(softmaxes, axis=0)\n",
    "    pred_idx = np.argmax(avg_probs)\n",
    "    conf = avg_probs[0, pred_idx]\n",
    "    return pred_idx, conf, avg_probs[0]\n",
    "\n",
    "# Load both models\n",
    "model_v19 = AutoModelForImageClassification.from_pretrained(V19_PATH).to(device).eval()\n",
    "model_v20 = AutoModelForImageClassification.from_pretrained(V20_PATH).to(device).eval()\n",
    "ensemble_models = [model_v19, model_v20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3daf8-cf6f-448c-b984-2acb11d6634d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20790b01-6bc6-46e7-bfc3-23ad8a270a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
