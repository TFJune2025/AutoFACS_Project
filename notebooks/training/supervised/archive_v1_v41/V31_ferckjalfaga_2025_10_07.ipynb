{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# V31 changes:\n",
    "    # overview: Re-integrated advanced diagnostics from V28 and automated the end-to-end\n",
    "        # workflow. Added early stopping, dynamic checkpoint loading, and a full suite\n",
    "        # of post-hoc analysis tools for model validation, calibration, and curation.\n",
    "    # section #1 - Replaced the hard-coded PRETRAINED_CHECKPOINT_PATH with a new\n",
    "        # `find_latest_checkpoint` function that dynamically finds and loads the\n",
    "        # most recent model run, automating iterative training.\n",
    "    # section #3 - Overhauled `compute_metrics_with_confusion` to restore advanced\n",
    "        # diagnostics: per-epoch CSV logging (F1, precision, recall), calculation of\n",
    "        # top confusion pairs, and class-based prediction entropy.\n",
    "    # section #3 - Added `check_deployment_readiness` function to automate final\n",
    "        # validation against a production F1-score threshold.\n",
    "    # section #4 - Added a sanity check to `main()` to verify the checkpoint path exists.\n",
    "    # section #4 - Integrated `EarlyStoppingCallback` to prevent overfitting and find\n",
    "        # the optimal number of training epochs automatically.\n",
    "    # section #4 - Modified the `main` function to return trained models and the processor,\n",
    "        # enabling an efficient in-memory workflow for post-training analysis.\n",
    "    # section #4 - deleted device = torch.device(\"cpu\") since it's defined\n",
    "        # in main execution block (if __name__...)\n",
    "    # section #4 - added Discriminative Learning Rate Implementation in training_args_s1\n",
    "    # section #4 - added 'sadness' and 'speech_action; to minoirity_classes_s2\n",
    "    # section #4 - created global trainer_s1, trainer_s2 to bypass the \n",
    "        # NameError: name 'trainer_s1' is not defined as a result for Hugging Face Trainer \n",
    "        # running its evaluation at the end of an epoch calling compute_metrics_with_...\n",
    "    # section #6 - Added a new `run_post_training_analysis` function to consolidate\n",
    "        # the entire post-hoc workflow, including inference, low-confidence review\n",
    "        # candidate generation, and hard-negative mining.\n",
    "    # section #7 & #8 - Re-integrated `apply_temperature_scaling` for model calibration\n",
    "        # and `hierarchical_ensemble_predict` for advanced ensembling analysis,\n",
    "        # adapting both for the two-stage architecture.\n",
    "    # section #9 - Consolidated all scattered execution blocks into a single, unified\n",
    "        # `if __name__ == \"__main__\":` entry point. This new block orchestrates the\n",
    "        # entire train-then-analyze pipeline in a logical sequence.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dynamically loading latest checkpoint: V29_20250710_082807\n",
      "ðŸ“ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- ðŸ“‚ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- âš™ï¸ Run Configuration ---\n",
    "# Set to True to run the hierarchical inference pipeline on the full dataset after training is complete.\n",
    "RUN_INFERENCE = True\n",
    "# Set to True on the first run to copy and organize files. Set to False on subsequent runs to save time.\n",
    "PREPARE_DATASETS = True\n",
    "\n",
    "# Finds the most recent V* model directory based on modification time.\n",
    "def find_latest_checkpoint(root_dir):\n",
    "    all_run_dirs = [\n",
    "        os.path.join(root_dir, d)\n",
    "        for d in os.listdir(root_dir)\n",
    "        if d.startswith(\"V\") and os.path.isdir(os.path.join(root_dir, d))\n",
    "    ]\n",
    "    if not all_run_dirs:\n",
    "        return None\n",
    "\n",
    "    # Sort directories by modification time, newest first\n",
    "    sorted_dirs = sorted(all_run_dirs, key=os.path.getmtime, reverse=True)\n",
    "\n",
    "    # The newest directory is the current run's empty folder.\n",
    "    # We need the second newest, which is the latest *completed* run.\n",
    "    if len(sorted_dirs) > 1:\n",
    "        return sorted_dirs[1] # <-- Return the second item in the list\n",
    "    else:\n",
    "        # If there's only one (or zero), no previous checkpoint exists\n",
    "        return None\n",
    "\n",
    "# --- ðŸ¤– Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Dynamically find the latest checkpoint to train from\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR)\n",
    "\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = latest_checkpoint\n",
    "    print(f\"âœ… Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    # If no checkpoint is found, fall back to the base model from Hugging Face\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(f\"âš ï¸ No local checkpoint found. Starting from base model: {BASE_MODEL_NAME}\")\n",
    "    \n",
    "# --- ðŸ·ï¸ Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# --- ðŸ–¼ï¸ File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- ðŸ”¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"ðŸ—‚ï¸ Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\nâœ… Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# ðŸ“¦ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self, processor, augment_dict):\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = T.Compose([\n",
    "            T.RandomResizedCrop(size=(224, 224)), # <-- Use this instead of T.Resize\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "        ])\n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            # Select the correct augmentation pipeline, default to base_augment\n",
    "            aug_pipeline = self.augment_dict.get(label, self.base_augment)\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "            augmented_image = aug_pipeline(rgb_image)\n",
    "            processed_images.append(augmented_image)\n",
    "\n",
    "        batch = self.processor(\n",
    "            images=processed_images,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# ðŸ‹ï¸ Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ðŸ”„ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# ðŸ“Š Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(eval_pred, label_names, stage_name=\"\"):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw logits/labels for later analysis like temperature scaling\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in label_names]\n",
    "    recalls = [report[name][\"recall\"] for name in label_names]\n",
    "    precisions = [report[name][\"precision\"] for name in label_names]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    \n",
    "    # Create a dictionary for entropies in the correct order for the CSV\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    # Access the trainer instance through its global-like availability during compute_metrics call\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names}\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "    # --- End Re-integration ---\n",
    "\n",
    "    # Generate and save a heatmap of the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((label_names[i], label_names[j]), cm[i][j])\n",
    "        for i in range(len(label_names))\n",
    "        for j in range(len(label_names)) if i != j and cm[i][j] > 0\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} â†’ {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute and print entropy metrics\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nðŸ§  Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nðŸ” Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "    # --- End Re-integration ---\n",
    "    \n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# ðŸ’¾ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"ðŸ’¾ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"âœ… {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"âš ï¸ Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - âŒ {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - âœ… {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"âœ… Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        PRETRAINED_CHECKPOINT_PATH,\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"âš–ï¸ Stage 1 Class Weights: {class_weights_s1}\")\n",
    "\n",
    "    # --- Define Early Stopping ---\n",
    "    # Stops training if validation loss doesn't improve for 2 consecutive epochs\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Discriminative Learning Rate for Stage 1 ---\n",
    "    # This implements discriminative learning rate, allowing \n",
    "        # the new classifier head to learn quickly while only \n",
    "        # making small, careful updates to pre-trained backbone\n",
    "    # Freeze all layers by default\n",
    "    for param in model_s1.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze the classifier head\n",
    "    for param in model_s1.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Unfreeze the last 4 encoder layers for fine-tuning\n",
    "    for param in model_s1.vit.encoder.layer[-4:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Create parameter groups with different learning rates\n",
    "    optimizer_grouped_parameters_s1 = [\n",
    "        {'params': model_s1.classifier.parameters(), 'lr': 3e-5},\n",
    "        {'params': model_s1.vit.encoder.layer[-4:].parameters(), 'lr': 1e-7}\n",
    "    ]\n",
    "    optimizer_s1 = torch.optim.AdamW(optimizer_grouped_parameters_s1, weight_decay=0.01)\n",
    "\n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=list(id2label_s1.values()), stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict={}), # Use base augmentation for all\n",
    "        class_weights=class_weights_s1, # Pass weights to the trainer\n",
    "        callbacks=[early_stop_callback], # early stopping\n",
    "        optimizers=(optimizer_s1, None) # Pass the custom optimizer\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"ðŸš€ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"âŒ› Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\nâœ… Stage 1 Training Complete.\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(sorted(train_dataset_s2['label'])))\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        PRETRAINED_CHECKPOINT_PATH,\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=9),\n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "\n",
    "    minority_classes_s2 = [label2id_s2[name] for name in ['disgust', 'questioning', 'contempt', 'fear', 'sadness', 'speech_action']]\n",
    "    minority_augment_map_s2 = {label_id: minority_aug for label_id in minority_classes_s2}\n",
    "\n",
    "    # Use the custom loss function to turn off label smoothing for historically difficult classes.\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,\n",
    "        target_class_names=['contempt', 'disgust'],\n",
    "        label2id_map=label2id_s2\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True, \n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Discriminative Learning Rate for Stage 2 ---\n",
    "    # Implements a discriminative learning rate, allowing \n",
    "        # new classifier head to learn quickly while only \n",
    "        # making small, careful updates to pre-trained backbone\n",
    "    # Freeze all layers by default\n",
    "    for param in model_s2.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze the classifier head\n",
    "    for param in model_s2.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    # Unfreeze the last 4 encoder layers for fine-tuning\n",
    "    for param in model_s2.vit.encoder.layer[-4:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Create parameter groups with different learning rates\n",
    "    optimizer_grouped_parameters_s2 = [\n",
    "        {'params': model_s2.classifier.parameters(), 'lr': 5e-5},\n",
    "        {'params': model_s2.vit.encoder.layer[-4:].parameters(), 'lr': 2e-7}\n",
    "    ]\n",
    "    optimizer_s2 = torch.optim.AdamW(optimizer_grouped_parameters_s2, weight_decay=0.01)\n",
    "\n",
    "    # Use the CustomLossTrainer again, passing the targeted loss function and new optimizer.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=RELEVANT_CLASSES, stage_name=\"Stage2\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict=minority_augment_map_s2),\n",
    "        loss_fct=loss_fct_s2, # Pass custom loss function\n",
    "        callbacks=[early_stop_callback], # early stopping\n",
    "        optimizers=(optimizer_s2, None) # Pass the custom optimizer\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"ðŸš€ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"âŒ› Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\nâœ… Stage 2 Training Complete.\")\n",
    "    print(\"\\nðŸŽ‰ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "    \n",
    "    # Return the trained models and processor to be used by analysis functions\n",
    "    return trainer_s1.model, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for new images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"ðŸ”¬ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # --- Stage 1 Prediction: Is the image relevant? ---\n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits\n",
    "            preds_s1 = torch.argmax(logits_s1, dim=-1)\n",
    "\n",
    "        # Create a mask of images that were classified as 'relevant'\n",
    "        relevant_mask = (preds_s1 == label2id_s1['relevant'])\n",
    "\n",
    "        # --- Stage 2 Prediction (only on relevant images) ---\n",
    "        if relevant_mask.any():\n",
    "            # Filter the input tensors to only include the relevant images\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "\n",
    "        # --- Aggregate Results ---\n",
    "        # Loop through the original batch and assign the correct prediction\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            if relevant_mask[j]:\n",
    "                # If relevant, get the prediction from the Stage 2 model\n",
    "                pred_label = id2label_s2[preds_s2[s2_idx].item()]\n",
    "                confidence = confs_s2[s2_idx].item()\n",
    "                s2_idx += 1\n",
    "            else:\n",
    "                # If not relevant, label it and stop\n",
    "                pred_label = \"irrelevant\"\n",
    "                confidence = torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item()\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\nâœ… Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = 0.85\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"ðŸ“‚ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs ---\n",
    "    # Find images where the model was confused between specific, problematic classes\n",
    "    confusion_pairs_to_mine = [('contempt', 'questioning'), ('contempt', 'neutral'), ('fear', 'surprise')]\n",
    "    \n",
    "    print(\"\\nâ›ï¸  Mining for hard negative confusion pairs...\")\n",
    "    for pair in confusion_pairs_to_mine:\n",
    "        c1, c2 = pair\n",
    "        # Find images where true is c1 but predicted is c2, OR true is c2 and predicted is c1\n",
    "        mask = ((df['true_label'] == c1) & (df['prediction'] == c2)) | \\\n",
    "               ((df['true_label'] == c2) & (df['prediction'] == c1))\n",
    "        \n",
    "        hard_negatives = df[mask]\n",
    "        \n",
    "        if not hard_negatives.empty:\n",
    "            out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "            hard_negatives.to_csv(out_path, index=False)\n",
    "            print(f\"  - Found {len(hard_negatives)} hard negatives for {pair}. Saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"ðŸ“Š Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ–¥ï¸ Using device: cpu\n",
      "ðŸ—‚ï¸ Preparing hierarchical datasets at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/prepared_datasets\n",
      "\n",
      "--- Creating Stage 1 Dataset ---\n",
      "Processing 'irrelevant' classes...\n",
      "  Recursively copying from 'hard_case'...\n",
      "Processing 'relevant' classes...\n",
      "  Recursively copying from 'anger'...\n",
      "  Recursively copying from 'contempt'...\n",
      "  Recursively copying from 'disgust'...\n",
      "  Recursively copying from 'fear'...\n",
      "  Recursively copying from 'happiness'...\n",
      "  Recursively copying from 'neutral'...\n",
      "  Recursively copying from 'questioning'...\n",
      "  Recursively copying from 'sadness'...\n",
      "  Recursively copying from 'surprise'...\n",
      "  Recursively copying from 'neutral_speech'...\n",
      "  Recursively copying from 'speech_action'...\n",
      "\n",
      "--- Creating Stage 2 Dataset ---\n",
      "  Copying 'anger' to Stage 2 directory...\n",
      "  Copying 'contempt' to Stage 2 directory...\n",
      "  Copying 'disgust' to Stage 2 directory...\n",
      "  Copying 'fear' to Stage 2 directory...\n",
      "  Copying 'happiness' to Stage 2 directory...\n",
      "  Copying 'neutral' to Stage 2 directory...\n",
      "  Copying 'questioning' to Stage 2 directory...\n",
      "  Copying 'sadness' to Stage 2 directory...\n",
      "  Copying 'surprise' to Stage 2 directory...\n",
      "  Copying 'neutral_speech' to Stage 2 directory...\n",
      "  Copying 'speech_action' to Stage 2 directory...\n",
      "\n",
      "âœ… Hierarchical dataset preparation complete.\n",
      "\n",
      "============================================================\n",
      "  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8363b54ed7fa487ca6cee6100e7e71ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa620d3d5e94f63a8b0db6d06a31a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: 21504 training samples, 5377 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions_v5/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([10, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Stage 1 Class Weights: tensor([0.6492, 2.1761])\n",
      "ðŸš€ Starting Stage 1 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6720' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6720/6720 4:22:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.643800</td>\n",
       "      <td>0.610737</td>\n",
       "      <td>0.678817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.631300</td>\n",
       "      <td>0.601480</td>\n",
       "      <td>0.705412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>0.597912</td>\n",
       "      <td>0.716943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.620400</td>\n",
       "      <td>0.605464</td>\n",
       "      <td>0.708760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.600617</td>\n",
       "      <td>0.715827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.87      0.68      0.77      4132\n",
      "    relevant       0.39      0.66      0.49      1245\n",
      "\n",
      "    accuracy                           0.68      5377\n",
      "   macro avg       0.63      0.67      0.63      5377\n",
      "weighted avg       0.76      0.68      0.70      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 1305 instances\n",
      "  - relevant â†’ irrelevant: 422 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.6290\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.6329\n",
      "  - irrelevant: entropy = 0.6279\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.87      0.73      0.79      4132\n",
      "    relevant       0.41      0.64      0.50      1245\n",
      "\n",
      "    accuracy                           0.71      5377\n",
      "   macro avg       0.64      0.68      0.65      5377\n",
      "weighted avg       0.76      0.71      0.72      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 1130 instances\n",
      "  - relevant â†’ irrelevant: 454 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.6180\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - irrelevant: entropy = 0.6184\n",
      "  - relevant: entropy = 0.6167\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.87      0.74      0.80      4132\n",
      "    relevant       0.42      0.63      0.51      1245\n",
      "\n",
      "    accuracy                           0.72      5377\n",
      "   macro avg       0.65      0.69      0.65      5377\n",
      "weighted avg       0.77      0.72      0.73      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 1057 instances\n",
      "  - relevant â†’ irrelevant: 465 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.6083\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.6127\n",
      "  - irrelevant: entropy = 0.6069\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.86      0.74      0.80      4132\n",
      "    relevant       0.41      0.62      0.50      1245\n",
      "\n",
      "    accuracy                           0.71      5377\n",
      "   macro avg       0.64      0.68      0.65      5377\n",
      "weighted avg       0.76      0.71      0.73      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 1091 instances\n",
      "  - relevant â†’ irrelevant: 475 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.6104\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.6126\n",
      "  - irrelevant: entropy = 0.6098\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.87      0.75      0.80      4132\n",
      "    relevant       0.42      0.62      0.50      1245\n",
      "\n",
      "    accuracy                           0.72      5377\n",
      "   macro avg       0.64      0.68      0.65      5377\n",
      "weighted avg       0.76      0.72      0.73      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant â†’ relevant: 1051 instances\n",
      "  - relevant â†’ irrelevant: 477 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.6100\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - relevant: entropy = 0.6121\n",
      "  - irrelevant: entropy = 0.6094\n",
      "âŒ› Stage 1 training took: 04:22:26\n",
      "ðŸ’¾ Saving relevance_filter_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512\n",
      "âœ… relevance_filter_model saved successfully.\n",
      "\n",
      "âœ… Stage 1 Training Complete.\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c53c2913ba48d6b12787efe3ae30df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4327f2d1b99d43eea5153614c86d3bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([10, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3090' max='3090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3090/3090 1:11:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.706000</td>\n",
       "      <td>1.340401</td>\n",
       "      <td>0.629960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.303600</td>\n",
       "      <td>1.264631</td>\n",
       "      <td>0.634008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.217100</td>\n",
       "      <td>1.228003</td>\n",
       "      <td>0.650202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>1.208875</td>\n",
       "      <td>0.663158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.175900</td>\n",
       "      <td>1.194901</td>\n",
       "      <td>0.647773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.64      0.60      0.62        85\n",
      "      contempt       0.46      0.72      0.56        60\n",
      "       disgust       0.46      1.00      0.63        26\n",
      "          fear       0.64      0.79      0.71        71\n",
      "     happiness       0.79      0.75      0.77       167\n",
      "       neutral       0.62      0.49      0.55       135\n",
      "   questioning       0.50      0.49      0.49        92\n",
      "       sadness       0.00      0.00      0.00        40\n",
      "      surprise       0.66      0.78      0.71       147\n",
      "neutral_speech       0.68      0.65      0.67       381\n",
      " speech_action       0.15      0.13      0.14        31\n",
      "\n",
      "      accuracy                           0.63      1235\n",
      "     macro avg       0.51      0.58      0.53      1235\n",
      "  weighted avg       0.62      0.63      0.62      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - questioning â†’ neutral_speech: 32 instances\n",
      "  - neutral_speech â†’ happiness: 28 instances\n",
      "  - sadness â†’ neutral_speech: 26 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.7873\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - disgust: entropy = 2.0735\n",
      "  - speech_action: entropy = 2.0317\n",
      "  - anger: entropy = 2.0175\n",
      "  - questioning: entropy = 1.9842\n",
      "  - contempt: entropy = 1.9605\n",
      "  - sadness: entropy = 1.9313\n",
      "  - fear: entropy = 1.9288\n",
      "  - neutral: entropy = 1.9107\n",
      "  - surprise: entropy = 1.7597\n",
      "  - neutral_speech: entropy = 1.7425\n",
      "  - happiness: entropy = 1.3417\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.53      0.42      0.47        85\n",
      "      contempt       0.49      0.67      0.57        60\n",
      "       disgust       0.45      0.96      0.62        26\n",
      "          fear       0.62      0.72      0.67        71\n",
      "     happiness       0.77      0.72      0.75       167\n",
      "       neutral       0.60      0.58      0.59       135\n",
      "   questioning       0.41      0.65      0.50        92\n",
      "       sadness       0.00      0.00      0.00        40\n",
      "      surprise       0.66      0.72      0.69       147\n",
      "neutral_speech       0.76      0.68      0.72       381\n",
      " speech_action       0.57      0.26      0.36        31\n",
      "\n",
      "      accuracy                           0.63      1235\n",
      "     macro avg       0.53      0.58      0.54      1235\n",
      "  weighted avg       0.63      0.63      0.63      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech â†’ happiness: 30 instances\n",
      "  - sadness â†’ questioning: 27 instances\n",
      "  - neutral_speech â†’ surprise: 26 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.5323\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - speech_action: entropy = 1.9178\n",
      "  - disgust: entropy = 1.8804\n",
      "  - questioning: entropy = 1.8432\n",
      "  - sadness: entropy = 1.8255\n",
      "  - anger: entropy = 1.8011\n",
      "  - contempt: entropy = 1.7388\n",
      "  - neutral: entropy = 1.7118\n",
      "  - fear: entropy = 1.6146\n",
      "  - surprise: entropy = 1.5562\n",
      "  - neutral_speech: entropy = 1.3786\n",
      "  - happiness: entropy = 1.1031\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.54      0.47      0.50        85\n",
      "      contempt       0.51      0.67      0.58        60\n",
      "       disgust       0.45      0.96      0.61        26\n",
      "          fear       0.71      0.80      0.75        71\n",
      "     happiness       0.81      0.74      0.78       167\n",
      "       neutral       0.56      0.61      0.58       135\n",
      "   questioning       0.46      0.68      0.55        92\n",
      "       sadness       0.00      0.00      0.00        40\n",
      "      surprise       0.66      0.73      0.69       147\n",
      "neutral_speech       0.77      0.67      0.72       381\n",
      " speech_action       0.62      0.26      0.36        31\n",
      "\n",
      "      accuracy                           0.65      1235\n",
      "     macro avg       0.55      0.60      0.56      1235\n",
      "  weighted avg       0.65      0.65      0.64      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech â†’ neutral: 31 instances\n",
      "  - sadness â†’ questioning: 26 instances\n",
      "  - neutral_speech â†’ anger: 25 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.4167\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - disgust: entropy = 1.8354\n",
      "  - questioning: entropy = 1.7956\n",
      "  - speech_action: entropy = 1.7924\n",
      "  - sadness: entropy = 1.7265\n",
      "  - contempt: entropy = 1.6633\n",
      "  - anger: entropy = 1.6385\n",
      "  - neutral: entropy = 1.5099\n",
      "  - fear: entropy = 1.4539\n",
      "  - surprise: entropy = 1.4106\n",
      "  - neutral_speech: entropy = 1.2596\n",
      "  - happiness: entropy = 1.0703\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.68      0.53      0.60        85\n",
      "      contempt       0.54      0.62      0.58        60\n",
      "       disgust       0.48      0.96      0.64        26\n",
      "          fear       0.79      0.76      0.78        71\n",
      "     happiness       0.81      0.69      0.75       167\n",
      "       neutral       0.53      0.56      0.54       135\n",
      "   questioning       0.46      0.72      0.56        92\n",
      "       sadness       0.00      0.00      0.00        40\n",
      "      surprise       0.65      0.74      0.69       147\n",
      "neutral_speech       0.76      0.73      0.75       381\n",
      " speech_action       0.63      0.39      0.48        31\n",
      "\n",
      "      accuracy                           0.66      1235\n",
      "     macro avg       0.58      0.61      0.58      1235\n",
      "  weighted avg       0.66      0.66      0.65      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness â†’ questioning: 32 instances\n",
      "  - neutral_speech â†’ neutral: 28 instances\n",
      "  - neutral_speech â†’ happiness: 21 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.3807\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - speech_action: entropy = 1.7920\n",
      "  - questioning: entropy = 1.7777\n",
      "  - sadness: entropy = 1.7724\n",
      "  - disgust: entropy = 1.7702\n",
      "  - contempt: entropy = 1.6462\n",
      "  - anger: entropy = 1.5618\n",
      "  - neutral: entropy = 1.5352\n",
      "  - fear: entropy = 1.4139\n",
      "  - surprise: entropy = 1.3628\n",
      "  - neutral_speech: entropy = 1.1951\n",
      "  - happiness: entropy = 1.0439\n",
      "\n",
      "ðŸ“ˆ Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.62      0.53      0.57        85\n",
      "      contempt       0.54      0.72      0.62        60\n",
      "       disgust       0.42      0.88      0.57        26\n",
      "          fear       0.69      0.75      0.72        71\n",
      "     happiness       0.81      0.69      0.75       167\n",
      "       neutral       0.55      0.65      0.60       135\n",
      "   questioning       0.42      0.66      0.52        92\n",
      "       sadness       0.00      0.00      0.00        40\n",
      "      surprise       0.64      0.65      0.64       147\n",
      "neutral_speech       0.79      0.70      0.74       381\n",
      " speech_action       0.58      0.23      0.33        31\n",
      "\n",
      "      accuracy                           0.65      1235\n",
      "     macro avg       0.55      0.59      0.55      1235\n",
      "  weighted avg       0.65      0.65      0.64      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness â†’ questioning: 30 instances\n",
      "  - neutral_speech â†’ neutral: 23 instances\n",
      "  - neutral_speech â†’ surprise: 23 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 1.3812\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - sadness: entropy = 1.8038\n",
      "  - questioning: entropy = 1.7823\n",
      "  - disgust: entropy = 1.7655\n",
      "  - speech_action: entropy = 1.6988\n",
      "  - contempt: entropy = 1.6257\n",
      "  - anger: entropy = 1.5574\n",
      "  - neutral: entropy = 1.5124\n",
      "  - surprise: entropy = 1.4085\n",
      "  - fear: entropy = 1.3877\n",
      "  - neutral_speech: entropy = 1.1995\n",
      "  - happiness: entropy = 1.0442\n",
      "âŒ› Stage 2 training took: 01:11:34\n",
      "ðŸ’¾ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512\n",
      "âœ… emotion_classifier_model saved successfully.\n",
      "\n",
      "âœ… Stage 2 Training Complete.\n",
      "\n",
      "ðŸŽ‰ Hierarchical Training Pipeline Finished Successfully.\n",
      "\n",
      "============================================================\n",
      "  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\n",
      "============================================================\n",
      "Found 26902 images to process for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Running Hierarchical Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 841/841 [54:46<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Full inference log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512/V31_full_inference_log.csv\n",
      "\n",
      "Found 26555 images below 0.85 confidence for review.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting review images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26555/26555 [00:08<00:00, 3255.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Sorted review images into folders at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512/review_candidates_by_predicted_class\n",
      "\n",
      "â›ï¸  Mining for hard negative confusion pairs...\n",
      "  - Found 1 hard negatives for ('contempt', 'questioning'). Saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512/hard_negatives_contempt_vs_questioning.csv\n",
      "  - Found 28 hard negatives for ('contempt', 'neutral'). Saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512/hard_negatives_contempt_vs_neutral.csv\n",
      "  - Found 3 hard negatives for ('fear', 'surprise'). Saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V31_20251007_153512/hard_negatives_fear_vs_surprise.csv\n",
      "\n",
      "============================================================\n",
      "  DEPLOYMENT READINESS CHECK\n",
      "============================================================\n",
      "Threshold: F1-Score >= 0.8\n",
      "\n",
      "  - âŒ anger           | F1-Score: 0.57 (Below Threshold)\n",
      "  - âŒ contempt        | F1-Score: 0.62 (Below Threshold)\n",
      "  - âŒ disgust         | F1-Score: 0.57 (Below Threshold)\n",
      "  - âŒ fear            | F1-Score: 0.72 (Below Threshold)\n",
      "  - âŒ happiness       | F1-Score: 0.75 (Below Threshold)\n",
      "  - âŒ neutral         | F1-Score: 0.60 (Below Threshold)\n",
      "  - âŒ questioning     | F1-Score: 0.52 (Below Threshold)\n",
      "  - âŒ sadness         | F1-Score: 0.00 (Below Threshold)\n",
      "  - âŒ surprise        | F1-Score: 0.64 (Below Threshold)\n",
      "  - âŒ neutral_speech  | F1-Score: 0.74 (Below Threshold)\n",
      "  - âŒ speech_action   | F1-Score: 0.33 (Below Threshold)\n",
      "\n",
      " M odel is NOT ready for production.\n",
      "\n",
      "============================================================\n",
      "  CALIBRATING STAGE 2 MODEL\n",
      "============================================================\n",
      "âœ… Optimal temperature for Stage 2 model: -7.2810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device once for the entire script run.\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # --- Step 1: Execute Training Pipeline ---\n",
    "    # The main function now returns the trained models and processor\n",
    "    model_s1, model_s2, processor = main(device)\n",
    "    \n",
    "    # --- Step 2: Run Post-Training Analysis & Curation ---\n",
    "    if RUN_INFERENCE:\n",
    "        # This function runs the full inference pass and generates logs for review.\n",
    "        # It uses the in-memory models returned from main().\n",
    "        run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "    \n",
    "    # --- Step 3: Run Final Model Checks ---\n",
    "    # Check if the model is ready for \"deployment\" based on F1 scores\n",
    "    stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "    check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "    \n",
    "    # --- Step 4: Calibrate the Stage 2 Model ---\n",
    "    logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "    labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "    \n",
    "    if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  CALIBRATING STAGE 2 MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        logits_s2 = np.load(logits_s2_path)\n",
    "        labels_s2 = np.load(labels_s2_path)\n",
    "        \n",
    "        optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "        print(f\"âœ… Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "        # plot_reliability_diagram(logits_s2, labels_s2, optimal_temp, SAVE_DIR, VERSION, \"Stage2\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Skipping calibration, logits/labels files for Stage 2 not found.\")\n",
    "\n",
    "    # --- Step 5: (Hypothetical) Run Ensemble Analysis ---\n",
    "    # This is a hypothetical example assuming a V30 model has been trained\n",
    "    v30_path = \"/path/to/your/V30_run_folder\" \n",
    "    \n",
    "    if os.path.exists(v30_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  RUNNING HIERARCHICAL ENSEMBLE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the older V30 models for the ensemble\n",
    "        s1_model_v30 = AutoModelForImageClassification.from_pretrained(os.path.join(v30_path, \"relevance_filter_model\")).to(device).eval()\n",
    "        s2_model_v30 = AutoModelForImageClassification.from_pretrained(os.path.join(v30_path, \"emotion_classifier_model\")).to(device).eval()\n",
    "        \n",
    "        # Use the in-memory V31 models (model_s1, model_s2) from the current run\n",
    "        s1_models_ensemble = [model_s1, s1_model_v30]\n",
    "        s2_models_ensemble = [model_s2, s2_model_v30]\n",
    "        \n",
    "        example_image_path = \"path/to/a/difficult/image.jpg\"\n",
    "        if os.path.exists(example_image_path):\n",
    "            prediction, confidence = hierarchical_ensemble_predict(example_image_path, processor, s1_models_ensemble, s2_models_ensemble, device)\n",
    "            print(f\"Ensemble prediction for {Path(example_image_path).name}: {prediction} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e00a50-b414-4e05-af34-ced5ab009ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
