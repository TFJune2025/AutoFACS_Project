{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#\n",
    "#   V30_Hierarchical_FER_Model\n",
    "#\n",
    "#   Description:\n",
    "#   This script implements the \"Strategy A: Hierarchical Classification\" pipeline\n",
    "#   as specified in the project handover document. It refactors the V29 script\n",
    "#   to support a two-stage training and inference process to handle extreme\n",
    "#   class imbalance and improve real-world performance.\n",
    "#\n",
    "#   - Stage 1: Trains a binary \"Relevance Filter\" to classify images as either\n",
    "#     'relevant' (an emotion/speech action) or 'irrelevant' (hard cases,\n",
    "#     non-faces).\n",
    "#\n",
    "#   - Stage 2: Trains a fine-grained 11-class classifier on *only* the\n",
    "#     'relevant' images to predict the final emotion label.\n",
    "\n",
    "#V30 changes:\n",
    "    # overview: Complete refactor to a two-stage hierarchical classification \n",
    "        # pipeline to solve the core class imbalance problem.\n",
    "    # section #1 - Updated global configurations to define 'RELEVANT_CLASSES' \n",
    "        #(11 emotions/actions) and 'IRRELEVANT_CLASSES' ('hard_case') to \n",
    "        #drive the new two-stage process.\n",
    "    # section #2 - Added a new, robust 'prepare_hierarchical_datasets' \n",
    "        #function. This function automatically reorganizes the source \n",
    "        #dataset into two separate structures for training and is designed \n",
    "        #to recursively search all sub-folders (no matter how deep) while \n",
    "        #skipping non-image files.\n",
    "    # section #3 - Replaced the single training block with a full two-stage \n",
    "        #training pipeline:\n",
    "    #   - Stage 1: Trains a binary 'Relevance Filter' model. Implemented \n",
    "        #class weighting in the loss function to handle the extreme imbalance \n",
    "        #from the 'hard_case' folder.\n",
    "    #   - Stage 2: Trains the final 11-class 'Emotion Classifier' model only \n",
    "        #on the 'relevant' data, isolating it from noisy examples and \n",
    "        #allowing it to focus on subtle differences.\n",
    "    # section #4 - Enhanced the 'CustomLossTrainer' to be more flexible, \n",
    "        #supporting either class weights (for Stage 1) or a targeted loss \n",
    "        #function (for Stage 2).\n",
    "    # section #5 - Overhauled the entire inference process. Created a new \n",
    "        #'hierarchical_predict' function that loads both models and chains \n",
    "        #them: first checking relevance, then classifying the emotion.\n",
    "    # section #6 - Removed the previous data balancing/oversampling logic \n",
    "        #(using MINORITY_CAP), as the hierarchical structure is a superior \n",
    "        #and more direct method for handling the class imbalance.\n",
    "    # overview: This new architecture prevents the final emotion model from \n",
    "        #being biased by ambiguous or irrelevant images, aiming for \n",
    "        #significantly better real-world performance and generalization.\n",
    "#\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V30_20251007_075715\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- üìÇ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- ‚öôÔ∏è Run Configuration ---\n",
    "# Set to True to run the hierarchical inference pipeline on the full dataset after training is complete.\n",
    "RUN_INFERENCE = True\n",
    "# Set to True on the first run to copy and organize files. Set to False on subsequent runs to save time.\n",
    "PREPARE_DATASETS = True\n",
    "\n",
    "# --- ü§ñ Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "# Path to a previous model checkpoint to start from (e.g., your V29 model).\n",
    "# This allows the new models to leverage prior learning.\n",
    "PRETRAINED_CHECKPOINT_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "\n",
    "# --- üè∑Ô∏è Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# --- üñºÔ∏è File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- üî¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"üóÇÔ∏è Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\n‚úÖ Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# üì¶ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self, processor, augment_dict):\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = T.Compose([\n",
    "            T.RandomResizedCrop(size=(224, 224)), # <-- Use this instead of T.Resize\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomRotation(10),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "        ])\n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            # Select the correct augmentation pipeline, default to base_augment\n",
    "            aug_pipeline = self.augment_dict.get(label, self.base_augment)\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "            augmented_image = aug_pipeline(rgb_image)\n",
    "            processed_images.append(augmented_image)\n",
    "\n",
    "        batch = self.processor(\n",
    "            images=processed_images,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# üèãÔ∏è Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# üîÑ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# üìä Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(eval_pred, label_names, stage_name=\"\"):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(f\"\\nüìà Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw logits/labels for later analysis like temperature scaling\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Generate and save a heatmap of the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# üíæ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"üíæ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"‚úÖ {model_name} saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main():\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"‚úÖ Skipping dataset preparation, using existing directories.\")\n",
    "\n",
    "    # --- Set hardware device ---\n",
    "    # WORKAROUND: Forcing CPU to bypass the persistent MPS backend bug.\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"\\nüñ•Ô∏è Using device: {device} (Forced to bypass MPS bug)\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        PRETRAINED_CHECKPOINT_PATH,\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"‚öñÔ∏è Stage 1 Class Weights: {class_weights_s1}\")\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=list(id2label_s1.values()), stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict={}), # Use base augmentation for all\n",
    "        class_weights=class_weights_s1 # Pass weights to the trainer\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"üöÄ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"‚åõ Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\n‚úÖ Stage 1 Training Complete.\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(sorted(train_dataset_s2['label'])))\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        PRETRAINED_CHECKPOINT_PATH,\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=9),\n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "    minority_classes_s2 = [label2id_s2[name] for name in ['disgust', 'questioning', 'contempt', 'fear']]\n",
    "    minority_augment_map_s2 = {label_id: minority_aug for label_id in minority_classes_s2}\n",
    "\n",
    "    # Use the custom loss function to turn off label smoothing for historically difficult classes.\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,\n",
    "        target_class_names=['contempt', 'disgust'],\n",
    "        label2id_map=label2id_s2\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True, \n",
    "        learning_rate=4e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Use the CustomLossTrainer again, this time passing the targeted loss function.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, label_names=RELEVANT_CLASSES, stage_name=\"Stage2\"),\n",
    "        data_collator=DataCollatorWithAugmentation(processor=processor, augment_dict=minority_augment_map_s2),\n",
    "        loss_fct=loss_fct_s2 # Pass custom loss function\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"üöÄ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"‚åõ Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\n‚úÖ Stage 2 Training Complete.\")\n",
    "    print(\"\\nüéâ Hierarchical Training Pipeline Finished Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for new images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"üî¨ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # --- Stage 1 Prediction: Is the image relevant? ---\n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits\n",
    "            preds_s1 = torch.argmax(logits_s1, dim=-1)\n",
    "\n",
    "        # Create a mask of images that were classified as 'relevant'\n",
    "        relevant_mask = (preds_s1 == label2id_s1['relevant'])\n",
    "\n",
    "        # --- Stage 2 Prediction (only on relevant images) ---\n",
    "        if relevant_mask.any():\n",
    "            # Filter the input tensors to only include the relevant images\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "\n",
    "        # --- Aggregate Results ---\n",
    "        # Loop through the original batch and assign the correct prediction\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            if relevant_mask[j]:\n",
    "                # If relevant, get the prediction from the Stage 2 model\n",
    "                pred_label = id2label_s2[preds_s2[s2_idx].item()]\n",
    "                confidence = confs_s2[s2_idx].item()\n",
    "                s2_idx += 1\n",
    "            else:\n",
    "                # If not relevant, label it and stop\n",
    "                pred_label = \"irrelevant\"\n",
    "                confidence = torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item()\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f347207-cc27-420e-92f7-2d30ea5f6037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è Preparing hierarchical datasets at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/prepared_datasets\n",
      "\n",
      "--- Creating Stage 1 Dataset ---\n",
      "Processing 'irrelevant' classes...\n",
      "  Recursively copying from 'hard_case'...\n",
      "Processing 'relevant' classes...\n",
      "  Recursively copying from 'anger'...\n",
      "  Recursively copying from 'contempt'...\n",
      "  Recursively copying from 'disgust'...\n",
      "  Recursively copying from 'fear'...\n",
      "  Recursively copying from 'happiness'...\n",
      "  Recursively copying from 'neutral'...\n",
      "  Recursively copying from 'questioning'...\n",
      "  Recursively copying from 'sadness'...\n",
      "  Recursively copying from 'surprise'...\n",
      "  Recursively copying from 'neutral_speech'...\n",
      "  Recursively copying from 'speech_action'...\n",
      "\n",
      "--- Creating Stage 2 Dataset ---\n",
      "  Copying 'anger' to Stage 2 directory...\n",
      "  Copying 'contempt' to Stage 2 directory...\n",
      "  Copying 'disgust' to Stage 2 directory...\n",
      "  Copying 'fear' to Stage 2 directory...\n",
      "  Copying 'happiness' to Stage 2 directory...\n",
      "  Copying 'neutral' to Stage 2 directory...\n",
      "  Copying 'questioning' to Stage 2 directory...\n",
      "  Copying 'sadness' to Stage 2 directory...\n",
      "  Copying 'surprise' to Stage 2 directory...\n",
      "  Copying 'neutral_speech' to Stage 2 directory...\n",
      "  Copying 'speech_action' to Stage 2 directory...\n",
      "\n",
      "‚úÖ Hierarchical dataset preparation complete.\n",
      "\n",
      "üñ•Ô∏è Using device: cpu (Forced to bypass MPS bug)\n",
      "\n",
      "============================================================\n",
      "  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f519538c0d49f5ad03939353f13fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f5d4f2147940d7b45eb1bd1af6d33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: 21504 training samples, 5377 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions_v5/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([10, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Stage 1 Class Weights: tensor([0.6492, 2.1761])\n",
      "üöÄ Starting Stage 1 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 2:14:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.540800</td>\n",
       "      <td>0.494553</td>\n",
       "      <td>0.743537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.479724</td>\n",
       "      <td>0.789102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.91      0.74      0.82      4132\n",
      "    relevant       0.47      0.77      0.58      1245\n",
      "\n",
      "    accuracy                           0.74      5377\n",
      "   macro avg       0.69      0.75      0.70      5377\n",
      "weighted avg       0.81      0.74      0.76      5377\n",
      "\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.90      0.82      0.86      4132\n",
      "    relevant       0.53      0.70      0.61      1245\n",
      "\n",
      "    accuracy                           0.79      5377\n",
      "   macro avg       0.72      0.76      0.73      5377\n",
      "weighted avg       0.82      0.79      0.80      5377\n",
      "\n",
      "‚åõ Stage 1 training took: 02:14:38\n",
      "üíæ Saving relevance_filter_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V30_20251007_075715\n",
      "‚úÖ relevance_filter_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 1 Training Complete.\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea935f189694fdd8e40089d1b66c326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9392c03bb4e8432abb754211347dead5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([10]) in the checkpoint and torch.Size([11]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([10, 768]) in the checkpoint and torch.Size([11, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3090' max='3090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3090/3090 1:06:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.074200</td>\n",
       "      <td>0.814775</td>\n",
       "      <td>0.808097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.778700</td>\n",
       "      <td>0.846999</td>\n",
       "      <td>0.804858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.820377</td>\n",
       "      <td>0.799190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>0.763455</td>\n",
       "      <td>0.825911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.760402</td>\n",
       "      <td>0.832389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.83      0.75      0.79        85\n",
      "      contempt       0.89      0.85      0.87        60\n",
      "       disgust       0.91      0.77      0.83        26\n",
      "          fear       0.93      0.92      0.92        71\n",
      "     happiness       0.90      0.78      0.84       167\n",
      "       neutral       0.70      0.79      0.74       135\n",
      "   questioning       0.78      0.82      0.80        92\n",
      "       sadness       0.67      0.05      0.09        40\n",
      "      surprise       0.77      0.84      0.81       147\n",
      "neutral_speech       0.79      0.91      0.85       381\n",
      " speech_action       0.81      0.42      0.55        31\n",
      "\n",
      "      accuracy                           0.81      1235\n",
      "     macro avg       0.82      0.72      0.74      1235\n",
      "  weighted avg       0.81      0.81      0.80      1235\n",
      "\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.83      0.75      0.79        85\n",
      "      contempt       0.84      0.78      0.81        60\n",
      "       disgust       0.92      0.85      0.88        26\n",
      "          fear       0.96      0.93      0.94        71\n",
      "     happiness       0.87      0.78      0.82       167\n",
      "       neutral       0.70      0.73      0.72       135\n",
      "   questioning       0.80      0.85      0.82        92\n",
      "       sadness       0.33      0.03      0.05        40\n",
      "      surprise       0.79      0.83      0.81       147\n",
      "neutral_speech       0.78      0.92      0.85       381\n",
      " speech_action       0.89      0.52      0.65        31\n",
      "\n",
      "      accuracy                           0.80      1235\n",
      "     macro avg       0.79      0.72      0.74      1235\n",
      "  weighted avg       0.80      0.80      0.79      1235\n",
      "\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.67      0.80      0.73        85\n",
      "      contempt       0.80      0.87      0.83        60\n",
      "       disgust       0.91      0.81      0.86        26\n",
      "          fear       0.90      0.89      0.89        71\n",
      "     happiness       0.86      0.75      0.80       167\n",
      "       neutral       0.76      0.73      0.74       135\n",
      "   questioning       0.83      0.79      0.81        92\n",
      "       sadness       0.71      0.12      0.21        40\n",
      "      surprise       0.78      0.81      0.80       147\n",
      "neutral_speech       0.81      0.92      0.86       381\n",
      " speech_action       0.65      0.35      0.46        31\n",
      "\n",
      "      accuracy                           0.80      1235\n",
      "     macro avg       0.79      0.71      0.73      1235\n",
      "  weighted avg       0.80      0.80      0.79      1235\n",
      "\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.82      0.82      0.82        85\n",
      "      contempt       0.84      0.78      0.81        60\n",
      "       disgust       0.88      0.88      0.88        26\n",
      "          fear       0.91      0.86      0.88        71\n",
      "     happiness       0.83      0.84      0.84       167\n",
      "       neutral       0.72      0.87      0.79       135\n",
      "   questioning       0.85      0.83      0.84        92\n",
      "       sadness       0.46      0.15      0.23        40\n",
      "      surprise       0.85      0.83      0.84       147\n",
      "neutral_speech       0.85      0.89      0.87       381\n",
      " speech_action       0.80      0.52      0.63        31\n",
      "\n",
      "      accuracy                           0.83      1235\n",
      "     macro avg       0.80      0.75      0.77      1235\n",
      "  weighted avg       0.82      0.83      0.82      1235\n",
      "\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.81      0.80      0.80        85\n",
      "      contempt       0.84      0.82      0.83        60\n",
      "       disgust       0.92      0.85      0.88        26\n",
      "          fear       0.86      0.90      0.88        71\n",
      "     happiness       0.89      0.75      0.82       167\n",
      "       neutral       0.80      0.79      0.80       135\n",
      "   questioning       0.91      0.88      0.90        92\n",
      "       sadness       0.80      0.20      0.32        40\n",
      "      surprise       0.81      0.84      0.82       147\n",
      "neutral_speech       0.81      0.95      0.87       381\n",
      " speech_action       0.85      0.55      0.67        31\n",
      "\n",
      "      accuracy                           0.83      1235\n",
      "     macro avg       0.85      0.76      0.78      1235\n",
      "  weighted avg       0.83      0.83      0.82      1235\n",
      "\n",
      "‚åõ Stage 2 training took: 01:06:57\n",
      "üíæ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V30_20251007_075715\n",
      "‚úÖ emotion_classifier_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 2 Training Complete.\n",
      "\n",
      "üéâ Hierarchical Training Pipeline Finished Successfully.\n",
      "\n",
      "============================================================\n",
      "  EXECUTING HIERARCHICAL INFERENCE ON FULL DATASET\n",
      "============================================================\n",
      "Reloading trained models for inference...\n",
      "‚úÖ Models loaded.\n",
      "Found 26902 images to process for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Running Hierarchical Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 841/841 [30:33<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Inference complete. Results saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V30_20251007_075715/V30_hierarchical_predictions.csv\n",
      "\n",
      "Prediction distribution:\n",
      "prediction\n",
      "irrelevant        16820\n",
      "neutral_speech     2455\n",
      "neutral            1504\n",
      "contempt           1374\n",
      "anger              1240\n",
      "surprise           1167\n",
      "questioning         880\n",
      "fear                756\n",
      "happiness           268\n",
      "disgust             227\n",
      "speech_action       112\n",
      "sadness              99\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# 6. Script Execution Entry Point\n",
    "# ----------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Execute Training ---\n",
    "    main()\n",
    "\n",
    "    # --- Execute Inference (if enabled) ---\n",
    "    if RUN_INFERENCE:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  EXECUTING HIERARCHICAL INFERENCE ON FULL DATASET\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # --- Load Both Trained Models ---\n",
    "        print(\"Reloading trained models for inference...\")\n",
    "        # WORKAROUND: Forcing CPU to bypass the persistent MPS backend bug.\n",
    "        device_inf = torch.device(\"cpu\")\n",
    "\n",
    "        # # commented out due to present mps and pytorch incompatibilities\n",
    "        # print(\"Reloading trained models for inference...\")\n",
    "        # device_inf = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        model_s1_inf = AutoModelForImageClassification.from_pretrained(os.path.join(SAVE_DIR, \"relevance_filter_model\")).to(device_inf).eval()\n",
    "        model_s2_inf = AutoModelForImageClassification.from_pretrained(os.path.join(SAVE_DIR, \"emotion_classifier_model\")).to(device_inf).eval()\n",
    "        processor_inf = AutoImageProcessor.from_pretrained(os.path.join(SAVE_DIR, \"relevance_filter_model\"))\n",
    "        print(\"‚úÖ Models loaded.\")\n",
    "\n",
    "        # --- Run Inference on the entire original dataset to test the pipeline ---\n",
    "        all_image_paths = [str(p) for p in Path(BASE_DATASET_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    "        print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "\n",
    "        predictions = hierarchical_predict(all_image_paths, model_s1_inf, model_s2_inf, processor_inf, device_inf)\n",
    "\n",
    "        # --- Save results to CSV for analysis ---\n",
    "        df_preds = pd.DataFrame(predictions)\n",
    "        output_csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_hierarchical_predictions.csv\")\n",
    "        df_preds.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\n‚úÖ Inference complete. Results saved to: {output_csv_path}\")\n",
    "        print(\"\\nPrediction distribution:\")\n",
    "        print(df_preds['prediction'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
