{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c33c0ec-d3ae-40d4-8899-ada5d12fbab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:12:57.479121: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-14 20:12:58.430052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-12.6/lib64\n",
      "2025-02-14 20:12:58.430147: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-12.6/lib64\n",
      "2025-02-14 20:12:58.430157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc48d01-0f0f-4bd8-b819-e71763933a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:13:00.555598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.596169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.598368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.601349: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-14 20:13:00.601919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.604022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.606113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.924758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.927138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.929218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-02-14 20:13:00.931194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12940 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture loaded from JSON.\n",
      "Model weights loaded from H5.\n",
      "Model re-loaded successfully from JSON and H5 files.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 1: Load the Pre-trained Model (Keras Format)\n",
    "# -----------------------------\n",
    "\n",
    "# Define paths for the architecture and weights files\n",
    "arch_path = \"/home/natalyagrokh/img_expressions/final_efficientnet_trained_model_2_architecture.json\"\n",
    "weights_path = \"/home/natalyagrokh/img_expressions/final_efficientnet_trained_model_2_weights.h5\"\n",
    "\n",
    "# Load the model architecture from the JSON file.\n",
    "with open(arch_path, \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "print(\"Model architecture loaded from JSON.\")\n",
    "\n",
    "# Load the model weights from the H5 file.\n",
    "loaded_model.load_weights(weights_path)\n",
    "print(\"Model weights loaded from H5.\")\n",
    "print(\"Model re-loaded successfully from JSON and H5 files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9100d232-b1bb-46ef-a325-d9711268826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model constructed from pre-trained model (without final classification layer).\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 2: Use the Pre-trained Model as a Base\n",
    "# -----------------------------\n",
    "# Use the loaded model (from JSON/H5) as the pre-trained model.\n",
    "base_output = loaded_model.layers[-2].output  # Here, '-2' refers to the output before the final classification layer\n",
    "\n",
    "# Create a new base model using the original input and the selected output.\n",
    "base_model = Model(inputs=loaded_model.input, outputs=base_output)\n",
    "print(\"Base model constructed from pre-trained model (without final classification layer).\")\n",
    "\n",
    "# Freeze all layers in the base model so that training on fer2013 initially doesn't modify these weights.\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd29c3d5-f417-4d15-b863-300f9074553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 3: Build a New Classification Head for fer2013\n",
    "# -----------------------------\n",
    "# fer2013 has 7 emotion classes (the same number as AffectNet in your case),\n",
    "# but you might still benefit from reinitializing the head for better adaptation.\n",
    "new_head = Sequential([\n",
    "    # (Optional) Add a Dense layer for further processing.\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    # Final classification layer for 7 classes.\n",
    "    Dense(7, activation=\"softmax\")\n",
    "], name=\"new_head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a769ba0-85de-42f3-9a91-b02c81bcd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model for fer2013 created.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 4: Combine the Base and New Head into a New Model\n",
    "# -----------------------------\n",
    "# Create an input tensor matching your fer2013 data shape.\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "# Add a resizing layer to convert the input to the base modelâ€™s expected shape.\n",
    "resized_tensor = tf.keras.layers.Resizing(48, 48)(input_tensor)\n",
    "# Pass the resized input through the base model.\n",
    "x = base_model(resized_tensor)\n",
    "# Then pass through the new head.\n",
    "output_tensor = new_head(x)\n",
    "# Build the final model.\n",
    "model_fer = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "print(\"New model for fer2013 created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b95ee2-06b7-409d-969e-020f9e48a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 48, 48, 3)         0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 128)               4213539   \n",
      "                                                                 \n",
      " new_head (Sequential)       (None, 7)                 69639     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,283,178\n",
      "Trainable params: 69,639\n",
      "Non-trainable params: 4,213,539\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 5: Compile the Model for Fine-Tuning on fer2013\n",
    "# -----------------------------\n",
    "# Use a relatively low learning rate since we are fine-tuning.\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "model_fer.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model_fer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5183e4bf-e2b8-43d4-9da6-3f579e3a4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Batch size set to: 2\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 6: Set Up Callbacks and Data Generators (as in your base code)\n",
    "# -----------------------------\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# (Reuse your existing data augmentation code for fer2013)\n",
    "train_aug_gen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "batch_size = 2\n",
    "image_shape = (224, 224, 3)\n",
    "train_dir = '/home/natalyagrokh/img_datasets/fer_2013_dataset/train'\n",
    "test_dir = '/home/natalyagrokh/img_datasets/fer_2013_dataset/test'\n",
    "\n",
    "train_image_gen = train_aug_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "test_image_gen = val_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=image_shape[:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "# Define callbacks (adjust patience and learning rate factors as needed)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.01,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.7,\n",
    "    min_delta=0.005,\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "steps_per_epoch = max(1, train_image_gen.samples // batch_size)\n",
    "validation_steps = max(1, test_image_gen.samples // batch_size)\n",
    "print(\"Batch size set to:\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84904751-8bc2-43c1-bf32-5d2cf5b67768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set TF_ENABLE_FUSED_BN to 0 (disabled).\n"
     ]
    }
   ],
   "source": [
    "# Disable fused batch normalization to force the use of non-fused (slower, but more robust) BN kernels.\n",
    "os.environ[\"TF_ENABLE_FUSED_BN\"] = \"0\"\n",
    "print(\"Set TF_ENABLE_FUSED_BN to 0 (disabled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c32008-2a97-40dd-b304-aa2fb343fae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 20:13:15.911699: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_1/model/efficientnetb0/block2b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-02-14 20:13:16.425035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2025-02-14 20:13:16.665017: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x56459e64ef00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-02-14 20:13:16.665053: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-02-14 20:13:16.671158: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-02-14 20:13:16.809075: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14354/14354 [==============================] - 390s 27ms/step - loss: 1.8366 - accuracy: 0.2656 - val_loss: 1.6597 - val_accuracy: 0.3862 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "14354/14354 [==============================] - 381s 27ms/step - loss: 1.7798 - accuracy: 0.2888 - val_loss: 1.6855 - val_accuracy: 0.3816 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "14354/14354 [==============================] - 381s 27ms/step - loss: 1.7635 - accuracy: 0.3024 - val_loss: 1.6883 - val_accuracy: 0.3858 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "14354/14354 [==============================] - 376s 26ms/step - loss: 1.7599 - accuracy: 0.2952 - val_loss: 1.6853 - val_accuracy: 0.3906 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "14354/14354 [==============================] - 376s 26ms/step - loss: 1.7537 - accuracy: 0.3032 - val_loss: 1.6681 - val_accuracy: 0.3859 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "14352/14354 [============================>.] - ETA: 0s - loss: 1.7603 - accuracy: 0.3021       \n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 6.999999823165126e-05.\n",
      "14354/14354 [==============================] - 397s 28ms/step - loss: 1.7604 - accuracy: 0.3021 - val_loss: 1.6808 - val_accuracy: 0.3770 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "14354/14354 [==============================] - 375s 26ms/step - loss: 1.7530 - accuracy: 0.3033 - val_loss: 1.6811 - val_accuracy: 0.3716 - lr: 7.0000e-05\n",
      "Epoch 8/100\n",
      "14354/14354 [==============================] - 375s 26ms/step - loss: 1.7530 - accuracy: 0.3074 - val_loss: 1.6724 - val_accuracy: 0.3884 - lr: 7.0000e-05\n",
      "Epoch 9/100\n",
      "14354/14354 [==============================] - 377s 26ms/step - loss: 1.7517 - accuracy: 0.3076 - val_loss: 1.6791 - val_accuracy: 0.3665 - lr: 7.0000e-05\n",
      "Epoch 10/100\n",
      "14354/14354 [==============================] - 373s 26ms/step - loss: 1.7466 - accuracy: 0.3095 - val_loss: 1.6796 - val_accuracy: 0.3810 - lr: 7.0000e-05\n",
      "Epoch 11/100\n",
      "Restoring model weights from the end of the best epoch: 1.oss: 1.7512 - accuracy: 0.3059   \n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.899999621557071e-05.\n",
      "14354/14354 [==============================] - 373s 26ms/step - loss: 1.7512 - accuracy: 0.3059 - val_loss: 1.6811 - val_accuracy: 0.3617 - lr: 7.0000e-05\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Step 8: Train the Model on fer2013\n",
    "# -----------------------------\n",
    "# (Optional) Ensure soft device placement is enabled.\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Train the Model on fer2013\n",
    "# -----------------------------\n",
    "history = model_fer.fit(\n",
    "    train_image_gen,\n",
    "    epochs=100,  # Adjust epochs as needed\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=test_image_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ac382-4573-451a-bdb0-ae459d754646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Print GPU information to verify detection and memory\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available:\", gpus)\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        details = tf.config.experimental.get_device_details(gpu)\n",
    "        print(f\"Details for {gpu.name}:\", details)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving details for {gpu}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae48924-df83-4153-b376-6846afc54bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', '/dt9/usr/bin/gcc'), ('cuda_compute_capabilities', ['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']), ('cuda_version', '11.2'), ('cudnn_version', '8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', True)])\n"
     ]
    }
   ],
   "source": [
    "print(tf.sysconfig.get_build_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81637e00-9221-41dd-863f-97de0bc04ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions) (Local)",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
