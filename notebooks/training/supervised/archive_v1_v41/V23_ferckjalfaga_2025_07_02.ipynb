{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V23 changes:\n",
    "    # overview: set up to use V20 model, got rid of crop_face\n",
    "    # section #2 - deleted crop_face, \n",
    "        # added def extract_version_from_path, ensemble_predict\n",
    "    # section #3 - simplified to only load V20, for now\n",
    "    # section #8 - reverted back to earlier versions, got rid of crop_face\n",
    "    # section #15 - unfroze 4 layers of backbone\n",
    "    # section #24 - addressed error, changed to only compare V20 and V23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, precision_recall_fscore_support\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    GaussianBlur,\n",
    "    RandAugment,\n",
    "    RandomAffine,\n",
    "    RandomApply,\n",
    "    RandomPerspective,\n",
    "    RandomAdjustSharpness,\n",
    "    ToPILImage,\n",
    "    ToTensor\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    'anger', 'disgust', 'fear', 'happiness', 'neutral',\n",
    "    'questioning', 'sadness', 'surprise', 'contempt', 'unknown'\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "HARD_CLASS_NAMES = ['contempt', 'disgust', 'fear', 'questioning']\n",
    "hard_class_ids = [label2id[n] for n in HARD_CLASS_NAMES]\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\", VERSION_TAG)\n",
    "LOGITS_PATH = os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\")\n",
    "LABELS_PATH = os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Utility Functions (Metrics & Calibration)\n",
    "# --------------------------\n",
    "\n",
    "# performs attention over patch tokens to create single context vector -\n",
    "    # more robust way to implement attention mechanism   \n",
    "class AttentionPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We only use the patch tokens for attention, ignoring the [CLS] token\n",
    "        patch_tokens = hidden_states[:, 1:, :]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # Shape: (batch_size, sequence_length - 1, 1)\n",
    "        attention_scores = self.attention_net(patch_tokens)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        # Shape: (batch_size, sequence_length - 1, 1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Compute the context vector (weighted sum of patch tokens)\n",
    "        # Shape: (batch_size, hidden_size)\n",
    "        context_vector = torch.sum(attention_weights * patch_tokens, dim=1)\n",
    "        \n",
    "        # The ViT pooler is expected to return a pooled output of this shape\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "# üîç Compute perceptual hash for image similarity clustering (used in REVIEW and Disgust curation)\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Inject image_path BEFORE any map/filter\n",
    "def add_image_path(example):\n",
    "    # Handle DatasetsImage and PIL.Image types robustly\n",
    "    img_obj = example[\"image\"]\n",
    "    path = getattr(img_obj, \"filename\", None)\n",
    "    if path is None:\n",
    "        # Fallback: Try reconstructing path from folder and file (rarely needed)\n",
    "        if \"file\" in example:\n",
    "            path = os.path.join(BASE_PATH, example[\"file\"])\n",
    "        else:\n",
    "            path = \"\"\n",
    "    example[\"image_path\"] = path\n",
    "    return example\n",
    "\n",
    "\n",
    "# Reconcile labels as before, but preserve all fields\n",
    "def reconcile_labels(example):\n",
    "    label = example.get(\"label\", None)\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = example[\"image_path\"]\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example    \n",
    "\n",
    "        \n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Use smoothed CE + confidence penalty\n",
    "        smooth_ce_loss = SmoothedCrossEntropyLoss(smoothing=0.05)\n",
    "        loss = smooth_ce_loss(logits, labels) + confidence_penalty(logits, beta=0.05)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)  # instead of eval_loss\n",
    "    return result\n",
    "    \n",
    "\n",
    "# üîÑ Smoothed Cross Entropy Loss (Œµ = 0.05)\n",
    "class SmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# ‚ö†Ô∏è Confidence Penalty to Reduce Overconfidence\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "\n",
    "# üìä Compute Metrics with Confusion Matrix Logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(labels, preds, target_names=LABEL_NAMES, output_dict=True)\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw logits/labels for calibration or further analysis\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in LABEL_NAMES]\n",
    "    recalls = [report[name][\"recall\"] for name in LABEL_NAMES]\n",
    "    precisions = [report[name][\"precision\"] for name in LABEL_NAMES]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    # Sort for display only; CSV row stays in canonical label order\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "    epoch = getattr(trainer.state, \"epoch\", None) if \"trainer\" in globals() else None\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(LABEL_NAMES, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(LABEL_NAMES, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(LABEL_NAMES, precisions)},\n",
    "        **{f\"entropy_{n}\": [e] for n, e in entropy_per_class}\n",
    "    })\n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # Generate and print confusion matrix heatmap\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    print(\"\\nüîç Class entropies (sorted):\")\n",
    "    for class_name, entropy in sorted_entropy:\n",
    "        print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# üå°Ô∏è Apply Temperature Scaling for Calibration\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"‚ùå Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Loading logits from: {logits_path}\")\n",
    "    print(f\"üìÇ Loading labels from: {labels_path}\")\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()\n",
    "\n",
    "\n",
    "# üìà Plot Reliability Diagram (Calibration Curve)\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {output_path}\")\n",
    "\n",
    "# saving model and processor\n",
    "def save_model_and_processor(model, processor, save_dir, trainer=None):\n",
    "    print(f\"Saving model and processor to: {save_dir}\")\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Save processor\n",
    "    processor.save_pretrained(save_dir)\n",
    "    print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "    \n",
    "    # Save full model\n",
    "    model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "    print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save state dict\n",
    "    final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "    # Save trainer state\n",
    "    if trainer is not None:\n",
    "        try:\n",
    "            trainer.save_model(os.path.join(save_dir, \"backup_trainer_model\"))\n",
    "            print(\"‚úÖ Trainer backup saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass  # Not all systems have CUDA\n",
    "    print(\"‚úÖ Memory cleanup complete after save.\")\n",
    "\n",
    "\n",
    "# üö¶ Prints label distribution for a dataset\n",
    "    #only calling for ad hoc debugging, experiments, sanity checks \n",
    "def check_label_integrity(dataset, LABEL_NAMES, label2id):\n",
    "    # Count all mapped labels\n",
    "    label_counts = Counter(dataset['label'])\n",
    "    print(\"\\nüö® Label distribution after mapping (before split):\")\n",
    "    for label_id in range(len(LABEL_NAMES)):\n",
    "        label_name = LABEL_NAMES[label_id]\n",
    "        print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "\n",
    "    # Specifically highlight 'surprise'\n",
    "    surprise_id = label2id['surprise']\n",
    "    if label_counts.get(surprise_id, 0) == 0:\n",
    "        print(\"‚ùóWARNING: No 'surprise' images found after mapping!\")\n",
    "    elif label_counts[surprise_id] < 50:  # arbitrary threshold\n",
    "        print(f\"‚ö†Ô∏è Only {label_counts[surprise_id]} 'surprise' images found! Check curation or mapping.\")\n",
    "\n",
    "\n",
    "# üö¶ Prints label distribution for multiple datasets\n",
    "def check_all_label_integrity(datasets_dict, LABEL_NAMES, label2id):\n",
    "    for name, dataset in datasets_dict.items():\n",
    "        print(f\"\\nüö® Label distribution for: {name}\")\n",
    "        label_counts = Counter(dataset['label'])\n",
    "        for label_id in range(len(LABEL_NAMES)):\n",
    "            label_name = LABEL_NAMES[label_id]\n",
    "            print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "        surprise_id = label2id['surprise']\n",
    "        if label_counts.get(surprise_id, 0) == 0:\n",
    "            print(\"‚ùóWARNING: No 'surprise' images found in this split!\")\n",
    "        elif label_counts[surprise_id] < 50:\n",
    "            print(f\"‚ö†Ô∏è Only {label_counts[surprise_id]} 'surprise' images in {name}! Check curation or mapping.\")\n",
    "\n",
    "\n",
    "# --- Stronger Augmentation Utility ---\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "        aug_count[label] += 1\n",
    "\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        \n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function\n",
    "    \n",
    "\n",
    "# Extracts the version number from a directory path\n",
    "def extract_version_from_path(path):\n",
    "    match = re.search(r\"V(\\d+)\", os.path.basename(path))\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "\n",
    "# Takes list of models and predicts label for single image by averaging\n",
    "    # their softmax probability scores\n",
    "def ensemble_predict(models, processor, image_path, device=\"cpu\"):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    softmaxes = []\n",
    "    individual_preds = []\n",
    "    \n",
    "    for m in models:\n",
    "        with torch.no_grad():\n",
    "            logits = m(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            individual_preds.append(id2label[torch.argmax(probs, dim=-1).item()])\n",
    "            softmaxes.append(probs.cpu().numpy())\n",
    "            \n",
    "    avg_probs = np.mean(softmaxes, axis=0)\n",
    "    ensemble_pred_idx = np.argmax(avg_probs)\n",
    "    ensemble_conf = avg_probs[0, ensemble_pred_idx]\n",
    "            \n",
    "    return id2label[ensemble_pred_idx], ensemble_conf, individual_preds \n",
    "\n",
    "\n",
    "# Returns a boolean tensor: True if the prediction is low-confidence\n",
    "def is_uncertain(probs, threshold=0.85, entropy_thresh=1.3):\n",
    "    \"\"\"Returns a boolean tensor: True if the prediction is low-confidence\"\"\"\n",
    "    conf, _ = torch.max(probs, dim=-1)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1)\n",
    "    return (conf < threshold) | (entropy > entropy_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Explicitly loading V20 golden checkpoint from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\n",
      "‚úÖ Classifier head reset for new training.\n",
      "\n",
      "üñ•Ô∏è Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Auto-Load V20 Golden Checkpoint\n",
    "# --------------------------\n",
    "# Manually set the path to your best-performing model\n",
    "model_path = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\"\n",
    "print(f\"‚úÖ Explicitly loading V20 golden checkpoint from: {model_path}\")\n",
    "\n",
    "# Load model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Reset the classifier head for new training\n",
    "model.classifier = nn.Linear(model.config.hidden_size, len(LABEL_NAMES))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "print(\"‚úÖ Classifier head reset for new training.\")\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"\\nüñ•Ô∏è Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Counting valid image files on disk for verification...\n",
      "‚úÖ Found 17501 image files in /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\n",
      "‚úÖ Datasets caching disabled for this run to ensure fresh data load.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8318f6adad4fc8a828fbc9720a3376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae03ba94a39b466aa92ea64a1fb95a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Add file path to each record:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156054d18efa4ac3bb8690edb2edab85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Re-labeling dataset (preserving image_path):   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b26d2630f04a2aaf79b1424c7f20d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total examples after filtering: 17500\n",
      "Sample with path: /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset/anger/Abel_Pacheco_0002.jpg\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Load and Prepare Dataset (with filename preservation)\n",
    "# ==============================\n",
    "\n",
    "# --- Dynamic File Count ---\n",
    "print(\"üîç Counting valid image files on disk for verification...\")\n",
    "# This will recursively find all valid image files in your dataset directory\n",
    "expected_file_count = len(\n",
    "    [p for p in Path(BASE_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    ")\n",
    "print(f\"‚úÖ Found {expected_file_count} image files in {BASE_PATH}\")\n",
    "\n",
    "# Disable caching BEFORE loading\n",
    "datasets.disable_caching()\n",
    "print(\"‚úÖ Datasets caching disabled for this run to ensure fresh data load.\")\n",
    "\n",
    "# Step 1: Load dataset and capture filepaths\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=BASE_PATH,\n",
    "    split=\"train\" # No need to specify cache_dir when caching is off\n",
    ")\n",
    "\n",
    "# Only run ONCE and only here, so \"image_path\" is never dropped later!\n",
    "dataset = dataset.map(add_image_path, desc=\"Add file path to each record\")\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset (preserving image_path)\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "# ** Robust Verification **\n",
    "final_count = len(dataset)\n",
    "print(f\"‚úÖ Total examples after filtering: {final_count}\")\n",
    "print(\"Sample with path:\", dataset[0][\"image_path\"])\n",
    "\n",
    "# Assertion checks whether loaded count is very close to the disk count\n",
    "# Small tolerance accounts for any files that fail to load or be filtered\n",
    "assert abs(final_count - expected_file_count) < 10, \\\n",
    "    f\"Dataset size mismatch! Found {expected_file_count} files but loaded {final_count}.\"\n",
    "\n",
    "assert dataset[0].get(\"image_path\", None), \"image_path missing from first record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise', 'unknown'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 2302 examples\n",
      "  disgust: 309 examples\n",
      "  fear: 1432 examples\n",
      "  happiness: 2892 examples\n",
      "  neutral: 3334 examples\n",
      "  questioning: 1939 examples\n",
      "  sadness: 1706 examples\n",
      "  surprise: 2779 examples\n",
      "  contempt: 421 examples\n",
      "  unknown: 386 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes: ['contempt', 'disgust', 'unknown']\n",
      "\n",
      "üìÇ Image count per label folder:\n",
      "  anger: 2302 images\n",
      "  contempt: 421 images\n",
      "  disgust: 309 images\n",
      "  fear: 1432 images\n",
      "  happiness: 2892 images\n",
      "  neutral: 3334 images\n",
      "  questioning: 1939 images\n",
      "  sadness: 1706 images\n",
      "  surprise: 2779 images\n",
      "  unknown: 386 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Disgust hash clusters with more than 1 image:\n",
      "üîç Sadness hash clusters with more than 1 image:\n",
      "  - Cluster 958c52e1: 2 images copied for review\n",
      "  - Cluster ee9a8d33: 2 images copied for review\n",
      "  - Cluster d0890396: 2 images copied for review\n",
      "  - Cluster bb0d06f2: 2 images copied for review\n",
      "  - Cluster d7f00fa2: 2 images copied for review\n",
      "üîç Fear hash clusters with more than 1 image:\n",
      "  - Cluster 9ae56592: 2 images copied for review\n",
      "  - Cluster 91c8ee81: 2 images copied for review\n",
      "  - Cluster dae5a596: 2 images copied for review\n",
      "üîç Questioning hash clusters with more than 1 image:\n",
      "  - Cluster da014886: 2 images copied for review\n",
      "  - Cluster 9db42783: 2 images copied for review\n",
      "üîç Contempt hash clusters with more than 1 image:\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Perceptual Clustering for Ambiguous/Confused Classes\n",
    "# --------------------------\n",
    "\n",
    "CLUSTER_TARGETS = [\"disgust\", \"sadness\", \"fear\", \"questioning\", \"contempt\"]\n",
    "\n",
    "for class_name in CLUSTER_TARGETS:\n",
    "    class_dir = os.path.join(BASE_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"‚ö†Ô∏è Class dir not found: {class_dir} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    class_images = [\n",
    "        os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
    "        if is_valid_image(f)\n",
    "    ]\n",
    "    hash_map = {}\n",
    "    for path in class_images:\n",
    "        h = compute_hash(path)\n",
    "        if h:\n",
    "            hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "    cluster_dir = os.path.join(SAVE_DIR, f\"{class_name}_clusters\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"üîç {class_name.capitalize()} hash clusters with more than 1 image:\")\n",
    "    for h, paths in hash_map.items():\n",
    "        if len(paths) > 1:\n",
    "            cluster_path = os.path.join(cluster_dir, h)\n",
    "            os.makedirs(cluster_path, exist_ok=True)\n",
    "            for p in paths:\n",
    "                shutil.copy(p, cluster_path)\n",
    "            print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Targeted minority augmentation will apply to: ['contempt', 'questioning', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "label_freqs = Counter(dataset[\"label\"])\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "minority_by_name = [n for n in minority_by_name if n != \"unknown\"]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning', 'contempt']\n",
    "\n",
    "# Merge and deduplicate\n",
    "minority_class_names = list(set(minority_by_name + manual_focus_classes))\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"üéØ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dcca9ac28b44b2a36b1c56951493ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying augmentations:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmentation counts: {'anger': 2302, 'contempt': 421, 'disgust': 309, 'fear': 1432, 'happiness': 2892, 'neutral': 3334, 'questioning': 1939, 'sadness': 1706, 'surprise': 2779, 'unknown': 386}\n",
      "‚úÖ Saved augmentation snapshot to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/V23_augmentation_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# RandAugment for specific minority classes only\n",
    "minority_classes_names = minority_class_names\n",
    "minority_classes = [label2id[label] for label in minority_classes_names]\n",
    "\n",
    "minority_aug = T.Compose([\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "])\n",
    "\n",
    "# Augmentation counter tracking\n",
    "aug_count = Counter()\n",
    "\n",
    "# Apply the transformations\n",
    "transform_fn = make_transform_function(processor, minority_classes)\n",
    "dataset = dataset.map(transform_fn, desc=\"Applying augmentations\")\n",
    "\n",
    "# Print diagnostics\n",
    "formatted_counts = {LABEL_NAMES[k]: v for k, v in aug_count.items()}\n",
    "print(f\"‚úÖ Augmentation counts: {formatted_counts}\")\n",
    "\n",
    "# Explicitly log dataset snapshots (class distribution) to a \n",
    "# CSV or JSON after each run for easy future diffing and tracking\n",
    "snapshot_path = os.path.join(SAVE_DIR, f\"{VERSION}_augmentation_snapshot.csv\")\n",
    "aug_snapshot = pd.DataFrame.from_dict(dict(aug_count), orient='index', columns=['count'])\n",
    "aug_snapshot.to_csv(snapshot_path)\n",
    "\n",
    "print(f\"‚úÖ Saved augmentation snapshot to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö® Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set\n",
      "  anger       : 1843\n",
      "  disgust     : 248\n",
      "  fear        : 1151\n",
      "  happiness   : 2330\n",
      "  neutral     : 2670\n",
      "  questioning : 1550\n",
      "  sadness     : 1369\n",
      "  surprise    : 2214\n",
      "  contempt    : 317\n",
      "  unknown     : 308\n",
      "\n",
      "üö® Label distribution for: val set\n",
      "  anger       : 459\n",
      "  disgust     : 61\n",
      "  fear        : 281\n",
      "  happiness   : 562\n",
      "  neutral     : 664\n",
      "  questioning : 389\n",
      "  sadness     : 337\n",
      "  surprise    : 565\n",
      "  contempt    : 104\n",
      "  unknown     : 78\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e37d1e-a6fb-4e9f-aec5-ee945f11c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Saved label distribution snapshot: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/label_snapshots/V23_label_distribution.csv\n",
      "\n",
      "üö® Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set\n",
      "  anger       : 1843\n",
      "  disgust     : 248\n",
      "  fear        : 1151\n",
      "  happiness   : 2330\n",
      "  neutral     : 2670\n",
      "  questioning : 1550\n",
      "  sadness     : 1369\n",
      "  surprise    : 2214\n",
      "  contempt    : 317\n",
      "  unknown     : 308\n",
      "\n",
      "üö® Label distribution for: val set\n",
      "  anger       : 459\n",
      "  disgust     : 61\n",
      "  fear        : 281\n",
      "  happiness   : 562\n",
      "  neutral     : 664\n",
      "  questioning : 389\n",
      "  sadness     : 337\n",
      "  surprise    : 565\n",
      "  contempt    : 104\n",
      "  unknown     : 78\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Label Distribution Snapshot and Drift Monitor\n",
    "# --------------------------\n",
    "snapshot_dir = os.path.join(SAVE_DIR, \"label_snapshots\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Count current training labels\n",
    "train_label_names = [LABEL_NAMES[i] for i in train_dataset['label']]\n",
    "label_counts = pd.Series(train_label_names).value_counts().sort_index()\n",
    "label_counts.name = VERSION\n",
    "\n",
    "# Save snapshot CSV\n",
    "snapshot_path = os.path.join(snapshot_dir, f\"{VERSION}_label_distribution.csv\")\n",
    "label_counts.to_csv(snapshot_path)\n",
    "print(f\"üìä Saved label distribution snapshot: {snapshot_path}\")\n",
    "\n",
    "# Optionally compare to previous version\n",
    "previous_versions = sorted([\n",
    "    f for f in os.listdir(snapshot_dir) if f.endswith(\".csv\") and not f.startswith(VERSION)\n",
    "])\n",
    "if previous_versions:\n",
    "    latest_prev = previous_versions[-1]\n",
    "    prev_df = pd.read_csv(os.path.join(snapshot_dir, latest_prev), index_col=0)\n",
    "    diff = label_counts.subtract(prev_df.iloc[:, 0], fill_value=0)\n",
    "    print(\"üîç Label count change since last snapshot:\")\n",
    "    print(diff)\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({4: 3334, 3: 2892, 7: 2779, 0: 2302, 5: 1939, 6: 1706, 2: 1432, 8: 421, 9: 386, 1: 309})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceee1498442e44609a541e0005269777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7452f4d8a4984383816ab4d6309dba4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac782a3b7d7c4a8da4f868fe3159eb76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2e49488808411da459a3e98eec5f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6810ccac29064c27ae172e58efcc7430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8bf916de22435294c6d50a6b22a1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ece11f54544df8b5bc1f33e9b64e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4517538fef1402e90e8846c1bb43f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579551b3cb5d4de0b9fb4a709ab18ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c95d08d034a4cdd94e5f856d8099ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({4: 3334, 3: 2892, 7: 2779, 0: 2302, 2: 2250, 5: 2250, 1: 2250, 6: 2250, 8: 2250, 9: 386})\n",
      "\n",
      "üö® Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: train set\n",
      "  anger       : 2302\n",
      "  disgust     : 2250\n",
      "  fear        : 2250\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 2250\n",
      "  sadness     : 2250\n",
      "  surprise    : 2779\n",
      "  contempt    : 2250\n",
      "  unknown     : 386\n",
      "\n",
      "üö® Label distribution for: val set\n",
      "  anger       : 459\n",
      "  disgust     : 61\n",
      "  fear        : 281\n",
      "  happiness   : 562\n",
      "  neutral     : 664\n",
      "  questioning : 389\n",
      "  sadness     : 337\n",
      "  surprise    : 565\n",
      "  contempt    : 104\n",
      "  unknown     : 78\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11. Balance Dataset (with NO oversampling for 'unknown')\n",
    "# --------------------------\n",
    "MINORITY_CAP = 2250\n",
    "balanced_subsets = []\n",
    "label_counts = Counter(dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    class_name = LABEL_NAMES[label]\n",
    "    if class_name == \"unknown\":\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count < MINORITY_CAP:\n",
    "        multiplier = MINORITY_CAP // len(subset)\n",
    "        remainder = MINORITY_CAP % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        balanced_subsets.append(subset)\n",
    "    else:\n",
    "        # Append full set (no downsampling for majority classes)\n",
    "        balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))\n",
    "\n",
    "hard_classes = ['contempt', 'disgust', 'questioning', 'surprise', 'fear']\n",
    "hard_class_ids = [label2id[c] for c in hard_classes]\n",
    "\n",
    "# Calculate weights: Give hard classes 2x, others 1x\n",
    "weights = [2.0 if l in hard_class_ids else 1.0 for l in train_dataset[\"label\"]]\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# üö¶ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9367aa62-f6a3-410e-9ce5-3e297a4cb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum split: 16664 easy, 0 hard\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 12. Curriculum Learning: Staged Hard/Easy Sample Scheduling (with preserved image_path)\n",
    "# ==========================\n",
    "\n",
    "# (A) Use audit_csv_path, or rebuild audit if needed\n",
    "audit_csv_path = os.path.join(SAVE_DIR, \"review_assignment_audit.csv\")\n",
    "if not os.path.exists(audit_csv_path):\n",
    "    # Rebuild (all records have image_path now!)\n",
    "    pd.DataFrame([{\"image_path\": ex[\"image_path\"], \"label\": ex[\"label\"]}\n",
    "                  for ex in dataset]).to_csv(audit_csv_path, index=False)\n",
    "\n",
    "audit_df = pd.read_csv(audit_csv_path)\n",
    "\n",
    "# (B) Map image_path (full path or filename) to index\n",
    "dataset_path_to_idx = {\n",
    "    os.path.basename(ex[\"image_path\"]): i for i, ex in enumerate(dataset)\n",
    "}\n",
    "\n",
    "easy_idxs, hard_idxs = [], []\n",
    "for _, row in audit_df.iterrows():\n",
    "    path_val = row[\"image_path\"]\n",
    "    if not isinstance(path_val, str) or not path_val:\n",
    "        continue\n",
    "    basename = os.path.basename(path_val)\n",
    "    idx = dataset_path_to_idx.get(basename)\n",
    "    if idx is not None:\n",
    "        # For now, assign all as easy unless you have other columns\n",
    "        easy_idxs.append(idx)\n",
    "    else:\n",
    "        print(f\"Image in audit CSV not found in current dataset: {row['image_path']}\")\n",
    "\n",
    "easy_dataset = dataset.select(easy_idxs)\n",
    "hard_dataset = dataset.select(hard_idxs)\n",
    "print(f\"Curriculum split: {len(easy_dataset)} easy, {len(hard_dataset)} hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 13. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # ‚úÖ (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # ‚úÖ Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # ‚úÖ Log once per epoch\n",
    "    save_safetensors=True                  # ‚úÖ Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 14. Define Compute Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up optimizer with DEEPER discriminative learning rates (V23 strategy) ---\n",
      "Unfreezing for training: classifier\n",
      "Unfreezing for fine-tuning: 0.attention.attention.query.weight\n",
      "Unfreezing for fine-tuning: 0.attention.attention.query.bias\n",
      "Unfreezing for fine-tuning: 0.attention.attention.key.weight\n",
      "Unfreezing for fine-tuning: 0.attention.attention.key.bias\n",
      "Unfreezing for fine-tuning: 0.attention.attention.value.weight\n",
      "Unfreezing for fine-tuning: 0.attention.attention.value.bias\n",
      "Unfreezing for fine-tuning: 0.attention.output.dense.weight\n",
      "Unfreezing for fine-tuning: 0.attention.output.dense.bias\n",
      "Unfreezing for fine-tuning: 0.intermediate.dense.weight\n",
      "Unfreezing for fine-tuning: 0.intermediate.dense.bias\n",
      "Unfreezing for fine-tuning: 0.output.dense.weight\n",
      "Unfreezing for fine-tuning: 0.output.dense.bias\n",
      "Unfreezing for fine-tuning: 0.layernorm_before.weight\n",
      "Unfreezing for fine-tuning: 0.layernorm_before.bias\n",
      "Unfreezing for fine-tuning: 0.layernorm_after.weight\n",
      "Unfreezing for fine-tuning: 0.layernorm_after.bias\n",
      "Unfreezing for fine-tuning: 1.attention.attention.query.weight\n",
      "Unfreezing for fine-tuning: 1.attention.attention.query.bias\n",
      "Unfreezing for fine-tuning: 1.attention.attention.key.weight\n",
      "Unfreezing for fine-tuning: 1.attention.attention.key.bias\n",
      "Unfreezing for fine-tuning: 1.attention.attention.value.weight\n",
      "Unfreezing for fine-tuning: 1.attention.attention.value.bias\n",
      "Unfreezing for fine-tuning: 1.attention.output.dense.weight\n",
      "Unfreezing for fine-tuning: 1.attention.output.dense.bias\n",
      "Unfreezing for fine-tuning: 1.intermediate.dense.weight\n",
      "Unfreezing for fine-tuning: 1.intermediate.dense.bias\n",
      "Unfreezing for fine-tuning: 1.output.dense.weight\n",
      "Unfreezing for fine-tuning: 1.output.dense.bias\n",
      "Unfreezing for fine-tuning: 1.layernorm_before.weight\n",
      "Unfreezing for fine-tuning: 1.layernorm_before.bias\n",
      "Unfreezing for fine-tuning: 1.layernorm_after.weight\n",
      "Unfreezing for fine-tuning: 1.layernorm_after.bias\n",
      "Unfreezing for fine-tuning: 2.attention.attention.query.weight\n",
      "Unfreezing for fine-tuning: 2.attention.attention.query.bias\n",
      "Unfreezing for fine-tuning: 2.attention.attention.key.weight\n",
      "Unfreezing for fine-tuning: 2.attention.attention.key.bias\n",
      "Unfreezing for fine-tuning: 2.attention.attention.value.weight\n",
      "Unfreezing for fine-tuning: 2.attention.attention.value.bias\n",
      "Unfreezing for fine-tuning: 2.attention.output.dense.weight\n",
      "Unfreezing for fine-tuning: 2.attention.output.dense.bias\n",
      "Unfreezing for fine-tuning: 2.intermediate.dense.weight\n",
      "Unfreezing for fine-tuning: 2.intermediate.dense.bias\n",
      "Unfreezing for fine-tuning: 2.output.dense.weight\n",
      "Unfreezing for fine-tuning: 2.output.dense.bias\n",
      "Unfreezing for fine-tuning: 2.layernorm_before.weight\n",
      "Unfreezing for fine-tuning: 2.layernorm_before.bias\n",
      "Unfreezing for fine-tuning: 2.layernorm_after.weight\n",
      "Unfreezing for fine-tuning: 2.layernorm_after.bias\n",
      "Unfreezing for fine-tuning: 3.attention.attention.query.weight\n",
      "Unfreezing for fine-tuning: 3.attention.attention.query.bias\n",
      "Unfreezing for fine-tuning: 3.attention.attention.key.weight\n",
      "Unfreezing for fine-tuning: 3.attention.attention.key.bias\n",
      "Unfreezing for fine-tuning: 3.attention.attention.value.weight\n",
      "Unfreezing for fine-tuning: 3.attention.attention.value.bias\n",
      "Unfreezing for fine-tuning: 3.attention.output.dense.weight\n",
      "Unfreezing for fine-tuning: 3.attention.output.dense.bias\n",
      "Unfreezing for fine-tuning: 3.intermediate.dense.weight\n",
      "Unfreezing for fine-tuning: 3.intermediate.dense.bias\n",
      "Unfreezing for fine-tuning: 3.output.dense.weight\n",
      "Unfreezing for fine-tuning: 3.output.dense.bias\n",
      "Unfreezing for fine-tuning: 3.layernorm_before.weight\n",
      "Unfreezing for fine-tuning: 3.layernorm_before.bias\n",
      "Unfreezing for fine-tuning: 3.layernorm_after.weight\n",
      "Unfreezing for fine-tuning: 3.layernorm_after.bias\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11472' max='14340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11472/14340 2:03:55 < 30:59, 1.54 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>0.406838</td>\n",
       "      <td>0.973429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.405006</td>\n",
       "      <td>0.974571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.404229</td>\n",
       "      <td>0.974571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.403287</td>\n",
       "      <td>0.974571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.98      0.98      0.98       459\n",
      "     disgust       0.97      0.93      0.95        61\n",
      "        fear       0.94      0.96      0.95       281\n",
      "   happiness       0.99      1.00      1.00       562\n",
      "     neutral       0.98      0.98      0.98       664\n",
      " questioning       0.98      0.94      0.96       389\n",
      "     sadness       0.97      0.97      0.97       337\n",
      "    surprise       0.98      0.98      0.98       565\n",
      "    contempt       0.82      0.88      0.85       104\n",
      "     unknown       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           0.97      3500\n",
      "   macro avg       0.96      0.96      0.96      3500\n",
      "weighted avg       0.97      0.97      0.97      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - questioning ‚Üí contempt: 14 instances\n",
      "  - surprise ‚Üí fear: 9 instances\n",
      "  - contempt ‚Üí questioning: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2984\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4146\n",
      "  - contempt: entropy = 0.4046\n",
      "  - fear: entropy = 0.3146\n",
      "  - disgust: entropy = 0.3001\n",
      "  - surprise: entropy = 0.2938\n",
      "  - anger: entropy = 0.2823\n",
      "  - unknown: entropy = 0.2755\n",
      "  - sadness: entropy = 0.2751\n",
      "  - neutral: entropy = 0.2668\n",
      "  - happiness: entropy = 0.2620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.98      0.98      0.98       459\n",
      "     disgust       0.97      0.93      0.95        61\n",
      "        fear       0.94      0.96      0.95       281\n",
      "   happiness       0.99      1.00      1.00       562\n",
      "     neutral       0.98      0.98      0.98       664\n",
      " questioning       0.99      0.94      0.96       389\n",
      "     sadness       0.97      0.97      0.97       337\n",
      "    surprise       0.98      0.98      0.98       565\n",
      "    contempt       0.81      0.90      0.85       104\n",
      "     unknown       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           0.97      3500\n",
      "   macro avg       0.96      0.97      0.96      3500\n",
      "weighted avg       0.98      0.97      0.97      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - questioning ‚Üí contempt: 15 instances\n",
      "  - surprise ‚Üí fear: 9 instances\n",
      "  - fear ‚Üí surprise: 5 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3079\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4276\n",
      "  - contempt: entropy = 0.3801\n",
      "  - surprise: entropy = 0.3435\n",
      "  - fear: entropy = 0.2943\n",
      "  - disgust: entropy = 0.2904\n",
      "  - neutral: entropy = 0.2846\n",
      "  - sadness: entropy = 0.2808\n",
      "  - anger: entropy = 0.2784\n",
      "  - unknown: entropy = 0.2648\n",
      "  - happiness: entropy = 0.2584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.99       459\n",
      "     disgust       0.97      0.93      0.95        61\n",
      "        fear       0.94      0.96      0.95       281\n",
      "   happiness       0.99      1.00      1.00       562\n",
      "     neutral       0.98      0.98      0.98       664\n",
      " questioning       0.99      0.93      0.96       389\n",
      "     sadness       0.97      0.97      0.97       337\n",
      "    surprise       0.98      0.98      0.98       565\n",
      "    contempt       0.80      0.91      0.85       104\n",
      "     unknown       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           0.97      3500\n",
      "   macro avg       0.96      0.97      0.96      3500\n",
      "weighted avg       0.98      0.97      0.97      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - questioning ‚Üí contempt: 17 instances\n",
      "  - surprise ‚Üí fear: 9 instances\n",
      "  - fear ‚Üí surprise: 5 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2961\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4324\n",
      "  - contempt: entropy = 0.3713\n",
      "  - disgust: entropy = 0.3070\n",
      "  - fear: entropy = 0.2958\n",
      "  - sadness: entropy = 0.2904\n",
      "  - anger: entropy = 0.2796\n",
      "  - surprise: entropy = 0.2763\n",
      "  - neutral: entropy = 0.2664\n",
      "  - happiness: entropy = 0.2646\n",
      "  - unknown: entropy = 0.2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.99      0.98      0.99       459\n",
      "     disgust       0.97      0.93      0.95        61\n",
      "        fear       0.94      0.96      0.95       281\n",
      "   happiness       0.99      1.00      1.00       562\n",
      "     neutral       0.98      0.98      0.98       664\n",
      " questioning       0.99      0.93      0.96       389\n",
      "     sadness       0.97      0.97      0.97       337\n",
      "    surprise       0.98      0.98      0.98       565\n",
      "    contempt       0.79      0.92      0.85       104\n",
      "     unknown       1.00      1.00      1.00        78\n",
      "\n",
      "    accuracy                           0.97      3500\n",
      "   macro avg       0.96      0.97      0.96      3500\n",
      "weighted avg       0.98      0.97      0.97      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - questioning ‚Üí contempt: 18 instances\n",
      "  - surprise ‚Üí fear: 9 instances\n",
      "  - fear ‚Üí surprise: 5 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2993\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.4429\n",
      "  - contempt: entropy = 0.3821\n",
      "  - disgust: entropy = 0.3085\n",
      "  - fear: entropy = 0.2911\n",
      "  - surprise: entropy = 0.2867\n",
      "  - sadness: entropy = 0.2866\n",
      "  - anger: entropy = 0.2812\n",
      "  - neutral: entropy = 0.2711\n",
      "  - happiness: entropy = 0.2623\n",
      "  - unknown: entropy = 0.2538\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Final Diagnostics ---\n",
      "Classifier head weights (after train): Parameter containing:\n",
      "tensor([[ 0.0118,  0.0113, -0.0480,  ..., -0.0078, -0.0160, -0.0284],\n",
      "        [-0.0216,  0.0144,  0.0297,  ...,  0.0492, -0.0038,  0.0520],\n",
      "        [-0.0387, -0.0199, -0.0274,  ...,  0.0157,  0.0204, -0.0044],\n",
      "        ...,\n",
      "        [ 0.0111,  0.0022, -0.0370,  ...,  0.0322, -0.0259,  0.0338],\n",
      "        [-0.0032,  0.0189,  0.0170,  ...,  0.0399, -0.0031, -0.0181],\n",
      "        [ 0.0260,  0.0609,  0.0087,  ..., -0.0369,  0.0399, -0.0126]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Classifier head bias (after train): Parameter containing:\n",
      "tensor([-0.0115, -0.0111,  0.0032, -0.0034, -0.0130,  0.0169, -0.0093, -0.0203,\n",
      "         0.0249, -0.0404], device='mps:0', requires_grad=True)\n",
      "Classifier weight in optimizer param group: True\n",
      "Classifier bias in optimizer param group: True\n",
      "Saving model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236\n",
      "‚úÖ Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236\n",
      "‚úÖ Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236\n",
      "‚úÖ State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/final_model.pth\n",
      "‚úÖ Memory cleanup complete after save.\n",
      "--- Model saved to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236 ---\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 15. Optimizer, Scheduler, and Training (Attention Pooling)\n",
    "# --------------------------\n",
    "\n",
    "# --- Part A: Discriminative Learning Rate Optimizer Setup ---\n",
    "print(\"--- Setting up optimizer with DEEPER discriminative learning rates (V23 strategy) ---\")\n",
    "\n",
    "# Define different learning rates for the head and the backbone\n",
    "head_lr = 5e-5      # High learning rate for the classifier head\n",
    "backbone_lr = 2e-7  # Very low learning rate for the fine-tuned backbone layers\n",
    "\n",
    "# First, ensure all layers are frozen by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classifier head to be trained\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"Unfreezing for training: classifier\")\n",
    "\n",
    "\n",
    "# Unfreeze the last 4 layers of the ViT encoder\n",
    "# This gives the model more capacity to adapt its feature extraction.\n",
    "for name, param in model.vit.encoder.layer[-4:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "    print(f\"Unfreezing for fine-tuning: {name}\")\n",
    "\n",
    "# Create parameter groups for the optimizer\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.classifier.parameters(), 'lr': head_lr},\n",
    "    {'params': model.vit.encoder.layer[-4:].parameters(), 'lr': backbone_lr}\n",
    "]\n",
    "\n",
    "# Create the AdamW optimizer with the specified parameter groups.\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "\n",
    "# --- Part B: Trainer Initialization ---\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.metric_for_best_model = \"eval_loss\"\n",
    "training_args.evaluation_strategy = \"epoch\"\n",
    "training_args.save_strategy = \"epoch\"\n",
    "\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    "    optimizers=(optimizer, None),\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "# --- Part C: Custom Scheduler Setup ---\n",
    "scheduler = CosineAnnealingWarmRestarts(trainer.optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "original_train = trainer.train\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)\n",
    "    return result\n",
    "trainer.train = modified_train\n",
    "\n",
    "# --- Part D: Train the Model and Finalize ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "trainer.train()\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "# === Diagnostics: Model Head and Optimizer State ===\n",
    "print(\"\\n--- Final Diagnostics ---\")\n",
    "print(\"Classifier head weights (after train):\", model.classifier.weight)\n",
    "print(\"Classifier head bias (after train):\", model.classifier.bias)\n",
    "\n",
    "opt_params = set([p for group in trainer.optimizer.param_groups for p in group['params']])\n",
    "print(\"Classifier weight in optimizer param group:\", model.classifier.weight in opt_params)\n",
    "print(\"Classifier bias in optimizer param group:\", model.classifier.bias in opt_params)\n",
    "\n",
    "assert model.classifier.weight in opt_params, \"CRITICAL ERROR: Classifier weight NOT in optimizer!\"\n",
    "assert model.classifier.bias in opt_params, \"CRITICAL ERROR: Classifier bias NOT in optimizer!\"\n",
    "\n",
    "# Save the final model and processor\n",
    "save_model_and_processor(model, processor, SAVE_DIR)\n",
    "print(f\"--- Model saved to {SAVE_DIR} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # 16. Rescue & Save from Last Checkpoint (after training)\n",
    "# # --------------------------\n",
    "# #in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"‚úÖ Processor manually re-saved.\")\n",
    "\n",
    "# # Use parent directory of SAVE_DIR to locate latest V* folder\n",
    "# parent_dir = os.path.dirname(SAVE_DIR)\n",
    "# v_folders = [\n",
    "#     d for d in os.listdir(parent_dir)\n",
    "#     if os.path.isdir(os.path.join(parent_dir, d)) and d.startswith(\"V\")\n",
    "# ]\n",
    "\n",
    "# def extract_timestamp(name):\n",
    "#     try:\n",
    "#         _, date_str, time_str = name.split(\"_\")\n",
    "#         return datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "#     except Exception:\n",
    "#         return datetime.min\n",
    "\n",
    "# latest_version_folder = max(v_folders, key=extract_timestamp)\n",
    "# latest_version_path = os.path.join(parent_dir, latest_version_folder)\n",
    "# print(f\"üóÇÔ∏è Using latest version folder: {latest_version_path}\")\n",
    "\n",
    "# # Locate latest checkpoint within that version folder\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(latest_version_path, d)\n",
    "#     for d in os.listdir(latest_version_path)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(latest_version_path, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"‚ùå No checkpoint found in latest version folder.\")\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # Load model and processor from latest checkpoint and save them\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(latest_version_path)\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"‚úÖ Model reloaded for inference.\")\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 274/274 [06:05<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "üìù Saved REVIEW file paths to V23_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/V23_distribution_plot_20250702_105236.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"üìù Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236\n",
      "üìÇ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/logits_eval_V23.npy\n",
      "üìÇ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/labels_eval_V23.npy\n",
      "‚úÖ Optimal temperature: 1.1372\n",
      "‚úÖ Calibrated Log Loss: 0.1589\n",
      "üìä Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/V23_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 19. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"üìÅ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaeec19-d4c7-43a6-80cb-86aafd573989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed tagging + copying REVIEW predictions to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/review_predictions_by_class\n",
      "üìÑ CSV log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/V23_review_predictions_with_preds.csv\n",
      "‚úÖ Review assignments (with audit) complete.\n",
      "Assignment summary: Counter({'neutral': 3188, 'happiness': 2955, 'surprise': 2881, 'anger': 2353, 'sadness': 1610, 'fear': 1389, 'unknown': 1135, 'questioning': 762, 'REVIEW': 546, 'contempt': 286, 'disgust': 255})\n",
      "‚úÖ Saved 267 clusters to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/review_predictions_clustered\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 20. Review & Relabel 'REVIEW' Predictions (with Audit Logging & Clustering)\n",
    "# --------------------------\n",
    "MINORITY_LABELS = [\"disgust\", \"contempt\", \"fear\", \"questioning\"]\n",
    "MINORITY_ENTROPY_THRESH = 0.6\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "REVIEW_CLUSTER_DIR = os.path.join(SAVE_DIR, \"review_predictions_clustered\")\n",
    "\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "os.makedirs(REVIEW_CLUSTER_DIR, exist_ok=True)\n",
    "\n",
    "# ---- If you HAVE NOT already generated review CSV (inference stage) ----\n",
    "if not os.path.exists(REVIEW_CSV_LOG):\n",
    "    review_log = []\n",
    "    image_paths = [\n",
    "        p for p in Path(IMAGE_DIR).rglob(\"*\")\n",
    "        if p.is_file() and p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "    ]\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                conf, pred_idx = torch.max(probs, dim=-1)\n",
    "            conf_val = conf.item()\n",
    "            pred_label = id2label[pred_idx.item()]\n",
    "            \n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1).item()\n",
    "\n",
    "            if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "                tag = \"unknown\"\n",
    "            elif conf_val < REVIEW_THRESHOLD:\n",
    "                tag = \"REVIEW\"\n",
    "            else:\n",
    "                tag = pred_label\n",
    "            \n",
    "            review_log.append({\n",
    "                \"image_path\": str(img_path),\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"confidence\": round(conf_val, 4),\n",
    "                \"entropy\": round(entropy, 4),\n",
    "                \"tag\": tag\n",
    "            })\n",
    "            \n",
    "            # For backward compatibility, still copy to REVIEW_BY_CLASS_DIR if tag is not \"unknown\"\n",
    "            if tag not in [\"unknown\"]:\n",
    "                target_dir = os.path.join(REVIEW_BY_CLASS_DIR, tag)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                shutil.copy(str(img_path), target_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with image: {img_path} | {e}\")\n",
    "            \n",
    "    pd.DataFrame(review_log).to_csv(REVIEW_CSV_LOG, index=False)\n",
    "    print(f\"‚úÖ Completed tagging + copying REVIEW predictions to: {REVIEW_BY_CLASS_DIR}\")\n",
    "    print(f\"üìÑ CSV log saved to: {REVIEW_CSV_LOG}\")\n",
    "\n",
    "# ---- If you already HAVE a review CSV (assignment/audit stage) ----\n",
    "df = pd.read_csv(REVIEW_CSV_LOG)\n",
    "review_assignment_log = []\n",
    "for _, row in df.iterrows():\n",
    "    path = row[\"image_path\"]\n",
    "    pred_label = row[\"predicted_label\"]\n",
    "    conf = float(row[\"confidence\"])\n",
    "    true_label = os.path.basename(os.path.dirname(path))\n",
    "    \n",
    "    entropy = float(row.get(\"entropy\", 0))  # default to 0 if not present\n",
    "    if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "        assigned = \"unknown\"\n",
    "    elif conf < REVIEW_THRESHOLD:\n",
    "        assigned = \"REVIEW\"\n",
    "    else:\n",
    "        assigned = pred_label\n",
    "    \n",
    "    dest_dir = os.path.join(REVIEW_BY_CLASS_DIR, assigned)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    shutil.copy(path, dest_dir)\n",
    "    review_assignment_log.append([path, true_label, pred_label, conf, assigned, entropy])\n",
    "\n",
    "log_df = pd.DataFrame(\n",
    "    review_assignment_log,\n",
    "    columns=[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\", \"entropy\"]\n",
    ")\n",
    "\n",
    "log_df.to_csv(os.path.join(SAVE_DIR, \"review_assignment_audit.csv\"), index=False)\n",
    "print(\"‚úÖ Review assignments (with audit) complete.\")\n",
    "\n",
    "print(\"Assignment summary:\", Counter(log_df[\"assigned_folder\"]))\n",
    "\n",
    "# ---- Perceptual hash clustering of review pool ----\n",
    "def phash_distance(hash1, hash2):\n",
    "    return hash1 - hash2\n",
    "\n",
    "PHASH_CLUSTER_THRESHOLD = 6\n",
    "image_paths = [row[0] for row in review_assignment_log if row[4] != \"unknown\"]  # assigned to a class\n",
    "\n",
    "hashes = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"L\").resize((64, 64))\n",
    "        hashes.append(hex_to_hash(str(phash(img))))\n",
    "    except Exception as e:\n",
    "        print(f\"phash error: {img_path} | {e}\")\n",
    "\n",
    "clusters = []\n",
    "used = set()\n",
    "for i, h1 in enumerate(hashes):\n",
    "    if i in used:\n",
    "        continue\n",
    "    cluster = [image_paths[i]]\n",
    "    used.add(i)\n",
    "    for j, h2 in enumerate(hashes):\n",
    "        if j <= i or j in used:\n",
    "            continue\n",
    "        if phash_distance(h1, h2) <= PHASH_CLUSTER_THRESHOLD:\n",
    "            cluster.append(image_paths[j])\n",
    "            used.add(j)\n",
    "    if len(cluster) > 1:\n",
    "        clusters.append(cluster)\n",
    "\n",
    "if not clusters:\n",
    "    print(f\"‚ö†Ô∏è No clusters found for review. {REVIEW_CLUSTER_DIR} will remain empty.\")\n",
    "else:\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        out_dir = os.path.join(REVIEW_CLUSTER_DIR, f\"cluster_{idx}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for p in cluster:\n",
    "            shutil.copy(p, out_dir)\n",
    "    print(f\"‚úÖ Saved {len(clusters)} clusters to {REVIEW_CLUSTER_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc464dc0-3204-453c-9a4c-4e63700ad6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flagged 8 hard negatives for ('contempt', 'questioning'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/review_hardneg_contempt_questioning.txt\n",
      "\n",
      "Flagged 10 hard negatives for ('fear', 'surprise'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/review_hardneg_fear_surprise.txt\n",
      "üîç Found 17360 total predictions (CSV) and 1231 REVIEW-tagged paths.\n",
      "üìÇ Grouped 1220 REVIEW images into folders by predicted label in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V23_20250702_105236/review_predictions_by_class\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 21. REVIEW Pool Diagnostics & Hard Confusion Mining\n",
    "# --------------------------\n",
    "\n",
    "# A. Flag hard confusion pairs for manual review\n",
    "REVIEW_CONFUSION_PAIRS = [(\"contempt\", \"questioning\"), (\"fear\", \"surprise\")]\n",
    "\n",
    "def parse_review_confusions(csv_path, confusion_pairs):\n",
    "    import csv\n",
    "    flagged_imgs = {pair: [] for pair in confusion_pairs}\n",
    "    with open(csv_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            pred = row[\"predicted_label\"]\n",
    "            true = os.path.basename(os.path.dirname(row[\"image_path\"]))\n",
    "            conf = float(row[\"confidence\"])\n",
    "            for a, b in confusion_pairs:\n",
    "                if ((pred == a and true == b) or (pred == b and true == a)) and conf < 0.8:\n",
    "                    flagged_imgs[(a, b)].append(row[\"image_path\"])\n",
    "    return flagged_imgs\n",
    "\n",
    "confusion_candidates = parse_review_confusions(REVIEW_CSV_LOG, REVIEW_CONFUSION_PAIRS)\n",
    "for pair, imgs in confusion_candidates.items():\n",
    "    print(f\"\\nFlagged {len(imgs)} hard negatives for {pair}:\")\n",
    "    out_path = os.path.join(SAVE_DIR, f\"review_hardneg_{pair[0]}_{pair[1]}.txt\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(imgs))\n",
    "    print(f\"  Saved list: {out_path}\")\n",
    "\n",
    "# B. Organize REVIEW-tagged images by predicted class (for curation)\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "review_txt_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "\n",
    "if os.path.exists(review_txt_path) and os.path.exists(csv_path):\n",
    "    with open(review_txt_path, \"r\") as f:\n",
    "        review_paths = {line.strip() for line in f.readlines()}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    count = 0\n",
    "\n",
    "    print(f\"üîç Found {len(df)} total predictions (CSV) and {len(review_paths)} REVIEW-tagged paths.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = row[\"image_path\"]\n",
    "        label = row[\"predicted_label\"]\n",
    "        conf = row[\"confidence\"]\n",
    "\n",
    "        if path in review_paths and label != \"REVIEW\":\n",
    "            dest_dir = os.path.join(REVIEW_SORT_DIR, label)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy(path, dest_dir)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"üìÇ Grouped {count} REVIEW images into folders by predicted label in: {REVIEW_SORT_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing review candidates file or prediction CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label name/id mapping:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: happiness\n",
      "4: neutral\n",
      "5: questioning\n",
      "6: sadness\n",
      "7: surprise\n",
      "8: contempt\n",
      "9: unknown\n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.8990        contempt  \n",
      "1      0.9457        contempt  \n",
      "2      0.9493         neutral  \n",
      "3      0.9461        contempt  \n",
      "4      0.9200        contempt  \n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.8990        contempt  \n",
      "1      0.9457        contempt  \n",
      "2      0.9493         neutral  \n",
      "3      0.9461        contempt  \n",
      "4      0.9200        contempt  \n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 22. Visualization & Error Tracking\n",
    "# --------------------------\n",
    "\n",
    "print(\"Label name/id mapping:\")\n",
    "for idx, name in enumerate(LABEL_NAMES):\n",
    "    print(f\"{idx}: {name}\")\n",
    "\n",
    "# Defensive: Check that metrics file exists before plotting\n",
    "per_class_csv = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "if not os.path.exists(per_class_csv):\n",
    "    print(f\"‚ö†Ô∏è Metrics file {per_class_csv} not found.\")\n",
    "else:\n",
    "    metrics_df = pd.read_csv(per_class_csv)\n",
    "    last_row = metrics_df.iloc[-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    f1s = [last_row[f\"f1_{n}\"] for n in LABEL_NAMES]\n",
    "    ax.bar(LABEL_NAMES, f1s)\n",
    "    ax.set_title(\"Per-Class F1 (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_f1.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Bar plot of per-class entropy\n",
    "    entropies = [last_row[f\"entropy_{n}\"] for n in LABEL_NAMES]\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.bar(LABEL_NAMES, entropies)\n",
    "    ax.set_title(\"Per-Class Mean Entropy (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_entropy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram for REVIEW pool\n",
    "    review_counts = Counter()\n",
    "    if os.path.exists(REVIEW_SORT_DIR):\n",
    "        for label_dir in os.listdir(REVIEW_SORT_DIR):\n",
    "            count = len(os.listdir(os.path.join(REVIEW_SORT_DIR, label_dir)))\n",
    "            review_counts[label_dir] = count\n",
    "        plt.bar(review_counts.keys(), review_counts.values())\n",
    "        plt.title(\"REVIEW Pool Distribution by Predicted Class\")\n",
    "        plt.savefig(os.path.join(SAVE_DIR, \"review_pool_distribution.png\"))\n",
    "        plt.close()\n",
    "        # Flag if >70% in one class\n",
    "        total = sum(review_counts.values())\n",
    "        for label, count in review_counts.items():\n",
    "            if total > 0 and count / total > 0.7:\n",
    "                print(f\"‚ö†Ô∏è REVIEW pool highly imbalanced: {count/total:.1%} in '{label}'\")\n",
    "\n",
    "    # Audit print block (as before)\n",
    "    print(\"Sample review predictions (audit):\")\n",
    "    if 'log_df' in locals():\n",
    "        print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "    elif 'df' in locals():\n",
    "        print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "    else:\n",
    "        print(\"No review/audit DataFrame found for printing.\")\n",
    "\n",
    "# ‚úÖ AUDIT BLOCK\n",
    "print(\"Sample review predictions (audit):\")\n",
    "if 'log_df' in locals():\n",
    "    print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "elif 'df' in locals():\n",
    "    print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "else:\n",
    "    print(\"No review/audit DataFrame found for printing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55576ae4-91ae-4d68-b538-2b57a0b46089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Entropy > 0.4 for class 'questioning': 0.44\n",
      "‚ö†Ô∏è Some classes not deployment-ready! Address above issues before production.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 23. Deployment Readiness Assertions and Flags\n",
    "# --------------------------\n",
    "\n",
    "# Load metrics\n",
    "metrics_df = pd.read_csv(os.path.join(SAVE_DIR, \"per_class_metrics.csv\"))\n",
    "last = metrics_df.iloc[-1]\n",
    "warn = False\n",
    "\n",
    "for cname in LABEL_NAMES:\n",
    "    f1 = last[f\"f1_{cname}\"]\n",
    "    entropy = last[f\"entropy_{cname}\"]\n",
    "    if f1 < 0.8:\n",
    "        print(f\"üö® F1 < 0.8 for class '{cname}': {f1:.2f}\")\n",
    "        warn = True\n",
    "    if entropy > 0.4:\n",
    "        print(f\"üö® Entropy > 0.4 for class '{cname}': {entropy:.2f}\")\n",
    "        warn = True\n",
    "\n",
    "if not warn:\n",
    "    print(\"‚úÖ All classes ready for deployment: F1 >= 0.8 and entropy <= 0.4\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some classes not deployment-ready! Address above issues before production.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9e53267-5a68-47f7-b0c9-babcecdcfe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up Model Ensembling Analysis (V23 vs V20) ---\n",
      "‚úÖ Selected models for ensembling:\n",
      "   - Model 1 (Base): V20_20250629_134956\n",
      "   - Model 2 (New):  V23_20250702_105236\n",
      "\n",
      "--- Running Ensemble Analysis on V23 Hard Cases ---\n",
      "\n",
      "Image: img_2617.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (V20) Pred: neutral\n",
      "  - Model 2 (V23) Pred: neutral\n",
      "  - ENSEMBLE Pred:   neutral (Confidence: 0.95)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'neutral'.\n",
      "\n",
      "Image: Iain_Richmond_0001.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (V20) Pred: neutral\n",
      "  - Model 2 (V23) Pred: neutral\n",
      "  - ENSEMBLE Pred:   neutral (Confidence: 0.96)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'neutral'.\n",
      "\n",
      "Image: Richard_Gephardt_0001.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (V20) Pred: happiness\n",
      "  - Model 2 (V23) Pred: happiness\n",
      "  - ENSEMBLE Pred:   happiness (Confidence: 0.90)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'happiness'.\n",
      "\n",
      "Image: img_1685.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (V20) Pred: anger\n",
      "  - Model 2 (V23) Pred: anger\n",
      "  - ENSEMBLE Pred:   anger (Confidence: 0.91)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'anger'.\n",
      "\n",
      "Image: Bill_Frist_0008.jpg\n",
      "  - True Label:      contempt\n",
      "  - Model 1 (V20) Pred: anger\n",
      "  - Model 2 (V23) Pred: anger\n",
      "  - ENSEMBLE Pred:   anger (Confidence: 0.95)\n",
      "  - ‚ùå FAILURE: Ensemble also misclassified as 'anger'.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 24. Model Ensembling Analysis (V23 vs V20)\n",
    "# --------------------------\n",
    "\n",
    "print(\"--- Setting up Model Ensembling Analysis (V23 vs V20) ---\")\n",
    "\n",
    "# --- Part A: Explicit Model Loading ---\n",
    "# Define paths for the models we want to compare\n",
    "V20_MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V20_20250629_134956\"\n",
    "V23_MODEL_PATH = SAVE_DIR # The SAVE_DIR variable holds the path to the current V23 run\n",
    "\n",
    "# Check if both model directories exist before proceeding\n",
    "if not os.path.exists(V20_MODEL_PATH) or not os.path.exists(V23_MODEL_PATH):\n",
    "    print(\"‚ö†Ô∏è Could not find both V20 and V23 model directories. Skipping ensembling.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Selected models for ensembling:\")\n",
    "    print(f\"   - Model 1 (Base): {os.path.basename(V20_MODEL_PATH)}\")\n",
    "    print(f\"   - Model 2 (New):  {os.path.basename(V23_MODEL_PATH)}\")\n",
    "\n",
    "    # Load the models\n",
    "    model_1 = AutoModelForImageClassification.from_pretrained(V20_MODEL_PATH).to(device).eval()\n",
    "    model_2 = AutoModelForImageClassification.from_pretrained(V23_MODEL_PATH).to(device).eval()\n",
    "    ensemble_models = [model_1, model_2]\n",
    "    \n",
    "    # --- Part B: Run Analysis on Misclassified Images ---\n",
    "    # Load the audit file from the CURRENT V23 run to find its challenging images\n",
    "    audit_csv_path = os.path.join(SAVE_DIR, \"review_assignment_audit.csv\")\n",
    "    if os.path.exists(audit_csv_path):\n",
    "        print(\"\\n--- Running Ensemble Analysis on V23 Hard Cases ---\")\n",
    "        audit_df = pd.read_csv(audit_csv_path)\n",
    "        \n",
    "        # Filter for images that were misclassified by the new V23 model\n",
    "        misclassified_df = audit_df[audit_df['true_label'] != audit_df['pred_label']]\n",
    "        \n",
    "        if misclassified_df.empty:\n",
    "            print(\"‚úÖ No misclassified images found in the V23 audit file. Nothing to analyze.\")\n",
    "        else:\n",
    "            # Analyze the top 5 hardest cases from the V23 run\n",
    "            for _, row in misclassified_df.head(5).iterrows():\n",
    "                image_path = row['image_path']\n",
    "                true_label = row['true_label']\n",
    "                \n",
    "                # This now calls the utility function defined in Cell In[4]\n",
    "                ensemble_pred, ensemble_conf, individual_preds = ensemble_predict(\n",
    "                    ensemble_models, processor, image_path, device=device\n",
    "                )\n",
    "                \n",
    "                print(f\"\\nImage: {os.path.basename(image_path)}\")\n",
    "                print(f\"  - True Label:      {true_label}\")\n",
    "                print(f\"  - Model 1 (V20) Pred: {individual_preds[0]}\")\n",
    "                print(f\"  - Model 2 (V23) Pred: {individual_preds[1]}\")\n",
    "                print(f\"  - ENSEMBLE Pred:   {ensemble_pred} (Confidence: {ensemble_conf:.2f})\")\n",
    "                \n",
    "                if ensemble_pred == true_label:\n",
    "                    print(\"  - ‚úÖ SUCCESS: Ensemble corrected the V23 misclassification!\")\n",
    "                else:\n",
    "                    print(f\"  - ‚ùå FAILURE: Ensemble also misclassified as '{ensemble_pred}'.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Could not find V23 audit CSV at {audit_csv_path}. Skipping ensemble analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7b01b-6900-40df-a538-604f62893caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
