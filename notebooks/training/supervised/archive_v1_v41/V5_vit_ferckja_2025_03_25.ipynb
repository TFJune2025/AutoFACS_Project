{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63737c85-49aa-40de-ae86-47b185adc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #In lambdalabs jupyter lab instance, run these:\n",
    "# pip install transformers\n",
    "# pip install seaborn\n",
    "# pip install tf-keras\n",
    "# pip install --upgrade \"numpy<2\"\n",
    "# pip install datasets\n",
    "# pip install --upgrade datasets pillow\n",
    "# pip install --upgrade \"accelerate>=0.26.0\"\n",
    "# #then check dependency warnings\n",
    "# pip check\n",
    "# #if any issues run these SEPARATELY!\n",
    "# pip install debugpy\n",
    "# pip install --upgrade argcomplete\n",
    "# # then install these\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install python3-cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-03-25 21:13:46.539043: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 21:13:46.559725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742937226.579072   28305 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742937226.585333   28305 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742937226.603387   28305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742937226.603402   28305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742937226.603404   28305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742937226.603405   28305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-25 21:13:46.609197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, Image as DatasetsImage\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor, \n",
    "    AutoModelForImageClassification, \n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de48cd2-aba6-46e2-8340-aeffa93211ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process restricted to GPUs: 0,1\n",
      "Memory growth enabled on GPUs.\n",
      "GPUs available to this process (as seen by TensorFlow): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Current GPU usage:\n",
      " Tue Mar 25 21:13:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.06             Driver Version: 570.124.06     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             42W /  300W |       4MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:08:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             45W /  300W |       4MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             43W /  300W |       1MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             42W /  300W |       1MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-16GB           On  |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             42W /  300W |       1MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-16GB           On  |   00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             43W /  300W |       1MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-16GB           On  |   00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             44W /  300W |       1MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-16GB           On  |   00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             42W /  300W |       1MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# GPU Environment Setup for Multi-GPU Optimization (GPUs 0-n)\n",
    "# --------------------------\n",
    "# Limit process to specific GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" #0, 1, 2, 3,...n\n",
    "print(\"Process restricted to GPUs:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# Ensure pip executables are available\n",
    "os.environ[\"PATH\"] = f\"{os.path.expanduser('~/.local/bin')}:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Enable memory growth for TensorFlow\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled on GPUs.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Error configuring GPUs:\", e)\n",
    "print(\"GPUs available to this process (as seen by TensorFlow):\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Optional: Monitor current GPU usage\n",
    "gpu_usage = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\")\n",
    "print(\"Current GPU usage:\\n\", gpu_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Global Configuration\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/home/ubuntu/MLexpressionsStorage/img_datasets/combo_ferckja_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Load Pretrained Model and Processor\n",
    "# --------------------------\n",
    "model_path = \"/home/ubuntu/MLexpressionsStorage/vit_final_independent_V4\"\n",
    "\n",
    "# Load the full model with its config and architecture\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)  \n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples after filtering: 37081\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. Load and Prepare Dataset\n",
    "# --------------------------\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=\"/home/ubuntu/MLexpressionsStorage/img_datasets/combo_ferckja_dataset\", split=\"train\")\n",
    "\n",
    "# Update mapping using lowercase keys\n",
    "label_mapping = {\n",
    "    'anger': 'Angry', 'contempt': 'Disgust', 'disgust': 'Disgust',\n",
    "    'fear': 'Fear', 'happiness': 'Happy', 'sadness': 'Sad',\n",
    "    'surprise': 'Surprise', 'neutral': 'Neutral'\n",
    "}\n",
    "\n",
    "# Numerical mapping for the pre-trained model's labels.\n",
    "num_mapping = {\n",
    "    'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3,\n",
    "    'Sad': 4, 'Surprise': 5, 'Neutral': 6\n",
    "}\n",
    "\n",
    "def reconcile_labels(example):\n",
    "    # If the label is already an integer, convert it to a string using the dataset features.\n",
    "    if isinstance(example[\"label\"], int):\n",
    "        # Use dataset.features[\"label\"].int2str to get the string label.\n",
    "        original_label = dataset.features[\"label\"].int2str(example[\"label\"]).strip().lower()\n",
    "    else:\n",
    "        original_label = example[\"label\"].strip().lower()\n",
    "    \n",
    "    # Map the lowercased label to the pre-trained model's expected label.\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    \n",
    "    if pretrain_label is None:\n",
    "        # If not recognized, mark it for filtering.\n",
    "        example[\"label\"] = -1\n",
    "    else:\n",
    "        # Convert the mapped label to its corresponding integer.\n",
    "        example[\"label\"] = num_mapping[pretrain_label]\n",
    "    return example\n",
    "\n",
    "# Apply reconciliation function to dataset.\n",
    "dataset = dataset.map(reconcile_labels)\n",
    "# Filter out any examples that were marked as unrecognized.\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "print(\"Total examples after filtering:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 3. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Use torchvision transforms for lightweight CPU-based augmentation.\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),                # Random horizontal flip\n",
    "    T.RandomRotation(10),                      # Random rotation within ±10 degrees\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)  # Slight brightness and contrast changes\n",
    "])\n",
    "\n",
    "def transform_function(example, processor):\n",
    "    label = example[\"label\"]\n",
    "\n",
    "    # Heavier augmentation for rare classes (e.g. Disgust)\n",
    "    if label == 1:  # Disgust (minority class)\n",
    "        aug_pipeline = T.Compose([\n",
    "            T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            T.RandomHorizontalFlip(p=0.7),\n",
    "            T.RandomRotation(20),\n",
    "            T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "            T.RandomGrayscale(p=0.2)\n",
    "        ])\n",
    "    else:\n",
    "        aug_pipeline = data_augment\n",
    "\n",
    "    if example[\"image\"].mode != \"RGB\":\n",
    "        example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "    augmented_image = aug_pipeline(example[\"image\"])\n",
    "    inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    inputs[\"labels\"] = example[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "# Map the transformation to every example in the dataset.\n",
    "dataset = dataset.map(partial(transform_function, processor=processor))\n",
    "\n",
    "# def transform_function(example, processor):\n",
    "#     # Ensure the image is loaded as a PIL image.\n",
    "#     if not isinstance(example[\"image\"], Image.Image):\n",
    "#         example[\"image\"] = Image.open(example[\"image\"])\n",
    "    \n",
    "#     # Convert image to RGB mode if it isn't already.\n",
    "#     if example[\"image\"].mode != \"RGB\":\n",
    "#         example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "    \n",
    "#     # Apply data augmentation.\n",
    "#     augmented_image = data_augment(example[\"image\"])\n",
    "    \n",
    "#     # Process the augmented image using the pre-trained processor.\n",
    "#     inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "#     inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "    \n",
    "#     # Add the label (ensure the label is in the proper format, e.g. integer).\n",
    "#     inputs[\"labels\"] = example[\"label\"]\n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a5ab27-d5e2-4340-8450-23693f34b4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:34<00:00, 138.22 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:33<00:00, 139.23 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:34<00:00, 138.49 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:34<00:00, 138.06 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:35<00:00, 137.73 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:34<00:00, 138.38 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 29664/29664 [03:28<00:00, 142.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After oversampling: Counter({6: 7426, 2: 7426, 0: 7426, 1: 7426, 4: 7426, 5: 7426, 3: 7426})\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4 Oversample Underrepresented Classes\n",
    "# --------------------------\n",
    "def oversample_dataset(dataset):\n",
    "    label_counts = Counter(dataset['label'])\n",
    "    max_count = max(label_counts.values())\n",
    "    label_datasets = []\n",
    "\n",
    "    for label in sorted(label_counts):\n",
    "        subset = dataset.filter(lambda x: x['label'] == label, num_proc=8)\n",
    "        multiplier = max_count // len(subset)\n",
    "        remainder = max_count % len(subset)\n",
    "        oversampled = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        label_datasets.append(oversampled)\n",
    "\n",
    "    return concatenate_datasets(label_datasets).shuffle(seed=42)\n",
    "\n",
    "train_dataset = oversample_dataset(train_dataset)\n",
    "print(\"After oversampling:\", Counter(train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 5. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_vit_model\",    # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",           # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=\"./logs\",                  # Directory for TensorBoard logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 6. Define a Compute Metrics Function for Evaluation\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# --------------------------\n",
    "# Confusion Matrix and Per-Class Accuracy Tracking\n",
    "# --------------------------\n",
    "\n",
    "# Define a compute_metrics function with confusion matrix logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Log classification report (per-class precision/recall/f1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=[id2label[i] for i in sorted(id2label.keys())]))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=[id2label[i] for i in sorted(id2label)], yticklabels=[id2label[i] for i in sorted(id2label)])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix_epoch.png\")  # Saves confusion matrix image\n",
    "    plt.close()\n",
    "\n",
    "    # Optionally: Return overall accuracy\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# 3. Ensure id2label is defined before training\n",
    "id2label = {\n",
    "    0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\",\n",
    "    4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16245' max='16245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16245/16245 6:05:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.201900</td>\n",
       "      <td>0.378631</td>\n",
       "      <td>0.881354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.506438</td>\n",
       "      <td>0.886073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.573338</td>\n",
       "      <td>0.888095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.622112</td>\n",
       "      <td>0.889174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.606022</td>\n",
       "      <td>0.892814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.85      0.89      0.87      1012\n",
      "     Disgust       0.94      0.99      0.96       148\n",
      "        Fear       0.77      0.85      0.81      1039\n",
      "       Happy       0.96      0.95      0.95      1801\n",
      "         Sad       0.89      0.75      0.82      1280\n",
      "    Surprise       0.93      0.94      0.93       895\n",
      "     Neutral       0.85      0.88      0.87      1242\n",
      "\n",
      "    accuracy                           0.88      7417\n",
      "   macro avg       0.88      0.89      0.89      7417\n",
      "weighted avg       0.88      0.88      0.88      7417\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.83      0.89      0.86      1012\n",
      "     Disgust       0.96      1.00      0.98       148\n",
      "        Fear       0.85      0.79      0.82      1039\n",
      "       Happy       0.96      0.95      0.96      1801\n",
      "         Sad       0.85      0.81      0.83      1280\n",
      "    Surprise       0.92      0.93      0.93       895\n",
      "     Neutral       0.85      0.89      0.87      1242\n",
      "\n",
      "    accuracy                           0.89      7417\n",
      "   macro avg       0.89      0.90      0.89      7417\n",
      "weighted avg       0.89      0.89      0.89      7417\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.90      0.86      0.88      1012\n",
      "     Disgust       0.97      1.00      0.99       148\n",
      "        Fear       0.84      0.80      0.82      1039\n",
      "       Happy       0.95      0.96      0.95      1801\n",
      "         Sad       0.86      0.81      0.84      1280\n",
      "    Surprise       0.93      0.93      0.93       895\n",
      "     Neutral       0.82      0.92      0.87      1242\n",
      "\n",
      "    accuracy                           0.89      7417\n",
      "   macro avg       0.90      0.90      0.90      7417\n",
      "weighted avg       0.89      0.89      0.89      7417\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.89      0.82      0.86      1012\n",
      "     Disgust       0.95      1.00      0.98       148\n",
      "        Fear       0.85      0.79      0.82      1039\n",
      "       Happy       0.94      0.98      0.96      1801\n",
      "         Sad       0.83      0.85      0.84      1280\n",
      "    Surprise       0.94      0.94      0.94       895\n",
      "     Neutral       0.86      0.89      0.88      1242\n",
      "\n",
      "    accuracy                           0.89      7417\n",
      "   macro avg       0.90      0.90      0.89      7417\n",
      "weighted avg       0.89      0.89      0.89      7417\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.88      0.85      0.87      1012\n",
      "     Disgust       0.96      1.00      0.98       148\n",
      "        Fear       0.86      0.79      0.82      1039\n",
      "       Happy       0.94      0.98      0.96      1801\n",
      "         Sad       0.85      0.85      0.85      1280\n",
      "    Surprise       0.93      0.94      0.94       895\n",
      "     Neutral       0.87      0.89      0.88      1242\n",
      "\n",
      "    accuracy                           0.89      7417\n",
      "   macro avg       0.90      0.90      0.90      7417\n",
      "weighted avg       0.89      0.89      0.89      7417\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16245, training_loss=0.07345656096733472, metrics={'train_runtime': 21936.3656, 'train_samples_per_second': 11.848, 'train_steps_per_second': 0.741, 'total_flos': 2.014184560523692e+19, 'train_loss': 0.07345656096733472, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Trainer with Class-Weighted Loss\n",
    "# --------------------------\n",
    "\n",
    "# Compute class weights from training set\n",
    "label_freqs = Counter(train_dataset['label'])\n",
    "total = sum(label_freqs.values())\n",
    "class_weights = torch.tensor([total / label_freqs[i] for i in range(len(label_freqs))], dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# trainer initialization\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/MLexpressionsStorage/vit_final_independent_V5/preprocessor_config.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Save Final Independent Model\n",
    "# --------------------------\n",
    "torch.save(model.state_dict(), '/home/ubuntu/MLexpressionsStorage/final_model_V5.pth')\n",
    "model.save_pretrained(\"/home/ubuntu/MLexpressionsStorage/vit_final_independent_V5\")\n",
    "processor.save_pretrained(\"/home/ubuntu/MLexpressionsStorage/vit_final_independent_V5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 9. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Load model + processor once\n",
    "model_path = \"/home/ubuntu/MLexpressionsStorage/vit_final_independent_V5\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path).to(device).eval()\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "id2label = model.config.id2label\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction (for large folders)\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    image_paths = [p for p in Path(IMAGE_DIR).rglob(\"*\") if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            if conf >= threshold:\n",
    "                all_preds.append(id2label[pred])\n",
    "            else:\n",
    "                all_preds.append(\"REVIEW\")  # Flag uncertain cases\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|██████████| 580/580 [03:34<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution plot saved to: /home/ubuntu/MLexpressionsStorage/distribution_plot_20250326_035318.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "    from datetime import datetime\n",
    "\n",
    "    OUTPUT_PATH = (\n",
    "        \"/home/ubuntu/MLexpressionsStorage/distribution_plot_\"\n",
    "        + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        + \".png\"\n",
    "    )\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment to test):\n",
    "# label = predict_label(\"/path/to/image.jpg\")\n",
    "# print(\"Predicted Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8c18b0-85d3-431f-9f06-cd72ccf4ab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training, evaluation, and saving complete. Preparing to shut down the instance...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #THIS DID NOT WORK - NEED TO MANUALLY SHUT DOWN!\n",
    "# # OPTIONAL: Final message/log\n",
    "# print(\"✅ Training, evaluation, and saving complete. Preparing to shut down the instance...\")\n",
    "\n",
    "# # Trigger full VM shutdown\n",
    "# os.system(\"sudo shutdown -h now\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b53e3b-a0bd-4018-984f-37bde6c3c1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
