{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V22 changes:\n",
    "    # overview: reverting to best-performing training strategy from V20\n",
    "    # section #2 - added crop_face\n",
    "    # section #8 - updated Augmentation counter tracking, make_transform\n",
    "    # section #15 - updated-> deleted attention_lr\n",
    "        # changed model.vit.params -> model.params\n",
    "        # changed model.vit.pooler.params -> model.classifier.params\n",
    "        # in optimizer_grouped_params, got rid of vit.pooler.param\n",
    "        # in assertions model.vit.pooler.att->model.classifier.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss, precision_recall_fscore_support\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    GaussianBlur,\n",
    "    RandAugment,\n",
    "    RandomAffine,\n",
    "    RandomApply,\n",
    "    RandomPerspective,\n",
    "    RandomAdjustSharpness,\n",
    "    ToPILImage,\n",
    "    ToTensor\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    'anger', 'disgust', 'fear', 'happiness', 'neutral',\n",
    "    'questioning', 'sadness', 'surprise', 'contempt', 'unknown'\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "HARD_CLASS_NAMES = ['contempt', 'disgust', 'fear', 'questioning']\n",
    "hard_class_ids = [label2id[n] for n in HARD_CLASS_NAMES]\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# ðŸ”¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\", VERSION_TAG)\n",
    "LOGITS_PATH = os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\")\n",
    "LABELS_PATH = os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Utility Functions (Metrics & Calibration)\n",
    "# --------------------------\n",
    "\n",
    "# performs attention over patch tokens to create single context vector -\n",
    "    # more robust way to implement attention mechanism   \n",
    "class AttentionPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We only use the patch tokens for attention, ignoring the [CLS] token\n",
    "        patch_tokens = hidden_states[:, 1:, :]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # Shape: (batch_size, sequence_length - 1, 1)\n",
    "        attention_scores = self.attention_net(patch_tokens)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        # Shape: (batch_size, sequence_length - 1, 1)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Compute the context vector (weighted sum of patch tokens)\n",
    "        # Shape: (batch_size, hidden_size)\n",
    "        context_vector = torch.sum(attention_weights * patch_tokens, dim=1)\n",
    "        \n",
    "        # The ViT pooler is expected to return a pooled output of this shape\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "# ðŸ” Compute perceptual hash for image similarity clustering (used in REVIEW and Disgust curation)\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Inject image_path BEFORE any map/filter\n",
    "def add_image_path(example):\n",
    "    # Handle DatasetsImage and PIL.Image types robustly\n",
    "    img_obj = example[\"image\"]\n",
    "    path = getattr(img_obj, \"filename\", None)\n",
    "    if path is None:\n",
    "        # Fallback: Try reconstructing path from folder and file (rarely needed)\n",
    "        if \"file\" in example:\n",
    "            path = os.path.join(BASE_PATH, example[\"file\"])\n",
    "        else:\n",
    "            path = \"\"\n",
    "    example[\"image_path\"] = path\n",
    "    return example\n",
    "\n",
    "\n",
    "# Reconcile labels as before, but preserve all fields\n",
    "def reconcile_labels(example):\n",
    "    label = example.get(\"label\", None)\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = example[\"image_path\"]\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example    \n",
    "\n",
    "\n",
    "#Detects face in PIL image, crops it, and adds padding\n",
    "    # If no face is found, it returns original imag\n",
    "def crop_face(image, padding=0.2):\n",
    "    \n",
    "    # Convert PIL image to numpy array for face_recognition library\n",
    "    np_image = np.array(image)\n",
    "    \n",
    "    # Find all face locations in the image\n",
    "    face_locations = face_recognition.face_locations(np_image)\n",
    "    \n",
    "    if face_locations:\n",
    "        # If faces are found, use the first one\n",
    "        top, right, bottom, left = face_locations[0]\n",
    "        \n",
    "        # Calculate padding\n",
    "        height = bottom - top\n",
    "        width = right - left\n",
    "        pad_y = int(height * padding)\n",
    "        pad_x = int(width * padding)\n",
    "        \n",
    "        # Apply padding, ensuring we don't go out of image bounds\n",
    "        top = max(0, top - pad_y)\n",
    "        left = max(0, left - pad_x)\n",
    "        bottom = min(np_image.shape[0], bottom + pad_y)\n",
    "        right = min(np_image.shape[1], right + pad_x)\n",
    "        \n",
    "        # Crop the image using the new coordinates\n",
    "        face_image = image.crop((left, top, right, bottom))\n",
    "        return face_image\n",
    "    else:\n",
    "        # If no face is found, return the original image to avoid errors\n",
    "        return image\n",
    "\n",
    "        \n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Use smoothed CE + confidence penalty\n",
    "        smooth_ce_loss = SmoothedCrossEntropyLoss(smoothing=0.05)\n",
    "        loss = smooth_ce_loss(logits, labels) + confidence_penalty(logits, beta=0.05)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)  # instead of eval_loss\n",
    "    return result\n",
    "    \n",
    "\n",
    "# ðŸ”„ Smoothed Cross Entropy Loss (Îµ = 0.05)\n",
    "class SmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "# âš ï¸ Confidence Penalty to Reduce Overconfidence\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "\n",
    "# ðŸ“Š Compute Metrics with Confusion Matrix Logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(labels, preds, target_names=LABEL_NAMES, output_dict=True)\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw logits/labels for calibration or further analysis\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in LABEL_NAMES]\n",
    "    recalls = [report[name][\"recall\"] for name in LABEL_NAMES]\n",
    "    precisions = [report[name][\"precision\"] for name in LABEL_NAMES]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    # Sort for display only; CSV row stays in canonical label order\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "    epoch = getattr(trainer.state, \"epoch\", None) if \"trainer\" in globals() else None\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(LABEL_NAMES, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(LABEL_NAMES, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(LABEL_NAMES, precisions)},\n",
    "        **{f\"entropy_{n}\": [e] for n, e in entropy_per_class}\n",
    "    })\n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # Generate and print confusion matrix heatmap\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} â†’ {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nðŸ§  Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    print(\"\\nðŸ” Class entropies (sorted):\")\n",
    "    for class_name, entropy in sorted_entropy:\n",
    "        print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# ðŸŒ¡ï¸ Apply Temperature Scaling for Calibration\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"âŒ Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"ðŸ“‚ Loading logits from: {logits_path}\")\n",
    "    print(f\"ðŸ“‚ Loading labels from: {labels_path}\")\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    print(f\"âœ… Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"âœ… Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()\n",
    "\n",
    "\n",
    "# ðŸ“ˆ Plot Reliability Diagram (Calibration Curve)\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ“Š Saved reliability diagram to {output_path}\")\n",
    "\n",
    "# saving model and processor\n",
    "def save_model_and_processor(model, processor, save_dir, trainer=None):\n",
    "    print(f\"Saving model and processor to: {save_dir}\")\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Save processor\n",
    "    processor.save_pretrained(save_dir)\n",
    "    print(f\"âœ… Processor saved to: {SAVE_DIR}\")\n",
    "    \n",
    "    # Save full model\n",
    "    model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "    print(f\"âœ… Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save state dict\n",
    "    final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"âœ… State dict saved to: {final_model_path}\")\n",
    "\n",
    "    # Save trainer state\n",
    "    if trainer is not None:\n",
    "        try:\n",
    "            trainer.save_model(os.path.join(save_dir, \"backup_trainer_model\"))\n",
    "            print(\"âœ… Trainer backup saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to save trainer backup: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass  # Not all systems have CUDA\n",
    "    print(\"âœ… Memory cleanup complete after save.\")\n",
    "\n",
    "\n",
    "# ðŸš¦ Prints label distribution for a dataset\n",
    "    #only calling for ad hoc debugging, experiments, sanity checks \n",
    "def check_label_integrity(dataset, LABEL_NAMES, label2id):\n",
    "    # Count all mapped labels\n",
    "    label_counts = Counter(dataset['label'])\n",
    "    print(\"\\nðŸš¨ Label distribution after mapping (before split):\")\n",
    "    for label_id in range(len(LABEL_NAMES)):\n",
    "        label_name = LABEL_NAMES[label_id]\n",
    "        print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "\n",
    "    # Specifically highlight 'surprise'\n",
    "    surprise_id = label2id['surprise']\n",
    "    if label_counts.get(surprise_id, 0) == 0:\n",
    "        print(\"â—WARNING: No 'surprise' images found after mapping!\")\n",
    "    elif label_counts[surprise_id] < 50:  # arbitrary threshold\n",
    "        print(f\"âš ï¸ Only {label_counts[surprise_id]} 'surprise' images found! Check curation or mapping.\")\n",
    "\n",
    "\n",
    "# ðŸš¦ Prints label distribution for multiple datasets\n",
    "def check_all_label_integrity(datasets_dict, LABEL_NAMES, label2id):\n",
    "    for name, dataset in datasets_dict.items():\n",
    "        print(f\"\\nðŸš¨ Label distribution for: {name}\")\n",
    "        label_counts = Counter(dataset['label'])\n",
    "        for label_id in range(len(LABEL_NAMES)):\n",
    "            label_name = LABEL_NAMES[label_id]\n",
    "            print(f\"  {label_name:12}: {label_counts.get(label_id, 0)}\")\n",
    "        surprise_id = label2id['surprise']\n",
    "        if label_counts.get(surprise_id, 0) == 0:\n",
    "            print(\"â—WARNING: No 'surprise' images found in this split!\")\n",
    "        elif label_counts[surprise_id] < 50:\n",
    "            print(f\"âš ï¸ Only {label_counts[surprise_id]} 'surprise' images in {name}! Check curation or mapping.\")\n",
    "\n",
    "\n",
    "# --- Stronger Augmentation Utility ---\n",
    "def make_transform_function(processor, hard_class_ids):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = strong_aug if label in hard_class_ids else data_augment\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Auto-loading base ViT from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V21_20250630_142520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V21_20250630_142520 were not used when initializing ViTForImageClassification: ['vit.pooler.attention_net.0.bias', 'vit.pooler.attention_net.0.weight', 'vit.pooler.attention_net.2.bias', 'vit.pooler.attention_net.2.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Replacing default ViT pooler with custom AttentionPooling layer.\n",
      "âœ… Pooler replaced successfully.\n",
      "âœ… Classifier head reset for new training.\n",
      "\n",
      "ðŸ–¥ï¸ Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): AttentionPooling(\n",
       "      (attention_net): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=384, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Auto-Load Latest Pretrained Model and Processor\n",
    "# --------------------------\n",
    "\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "def extract_version(dirname):\n",
    "    match = re.match(r\"V(\\d+)\", os.path.basename(dirname))\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "model_dirs = [\n",
    "    os.path.join(MODEL_ROOT, d)\n",
    "    for d in os.listdir(MODEL_ROOT)\n",
    "    if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))\n",
    "]\n",
    "\n",
    "model_dirs = [d for d in model_dirs if os.path.abspath(d) != os.path.abspath(SAVE_DIR)]\n",
    "model_dirs = sorted(model_dirs, key=extract_version, reverse=True)\n",
    "\n",
    "if not model_dirs:\n",
    "    raise FileNotFoundError(\"âŒ No earlier model folders found.\")\n",
    "\n",
    "# We still load from the latest V20 fine-tuned model\n",
    "model_path = model_dirs[0]\n",
    "print(f\"âœ… Auto-loading base ViT from: {model_path}\")\n",
    "\n",
    "# Load the processor (this doesn't change)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Load the standard ViT model for image classification\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "\n",
    "# --- SURGICAL REPLACEMENT ---\n",
    "# Replace the model's default pooler with our new AttentionPooling layer.\n",
    "# This is a much more stable way to introduce the attention mechanism.\n",
    "print(\"âš ï¸ Replacing default ViT pooler with custom AttentionPooling layer.\")\n",
    "model.vit.pooler = AttentionPooling(model.config.hidden_size)\n",
    "print(\"âœ… Pooler replaced successfully.\")\n",
    "\n",
    "# Replace classification head to match current label schema\n",
    "model.classifier = nn.Linear(model.config.hidden_size, len(LABEL_NAMES))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "print(\"âœ… Classifier head reset for new training.\")\n",
    "\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"\\nðŸ–¥ï¸ Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Counting valid image files on disk for verification...\n",
      "âœ… Found 17501 image files in /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset\n",
      "âœ… Datasets caching disabled for this run to ensure fresh data load.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b5ce3d91e84c6a8db6460251e410ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31d5b0ba00345a09e9a95928dba8a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Add file path to each record:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1897896210406993aae922f3fe0593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Re-labeling dataset (preserving image_path):   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04de343848c046a0b58b47f8df70a236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total examples after filtering: 17500\n",
      "Sample with path: /Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset/anger/Abel_Pacheco_0002.jpg\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4. Load and Prepare Dataset (with filename preservation)\n",
    "# ==============================\n",
    "\n",
    "# --- Dynamic File Count ---\n",
    "print(\"ðŸ” Counting valid image files on disk for verification...\")\n",
    "# This will recursively find all valid image files in your dataset directory\n",
    "expected_file_count = len(\n",
    "    [p for p in Path(BASE_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    ")\n",
    "print(f\"âœ… Found {expected_file_count} image files in {BASE_PATH}\")\n",
    "\n",
    "# Disable caching BEFORE loading\n",
    "datasets.disable_caching()\n",
    "print(\"âœ… Datasets caching disabled for this run to ensure fresh data load.\")\n",
    "\n",
    "# Step 1: Load dataset and capture filepaths\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=BASE_PATH,\n",
    "    split=\"train\" # No need to specify cache_dir when caching is off\n",
    ")\n",
    "\n",
    "# Only run ONCE and only here, so \"image_path\" is never dropped later!\n",
    "dataset = dataset.map(add_image_path, desc=\"Add file path to each record\")\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset (preserving image_path)\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "# ** Robust Verification **\n",
    "final_count = len(dataset)\n",
    "print(f\"âœ… Total examples after filtering: {final_count}\")\n",
    "print(\"Sample with path:\", dataset[0][\"image_path\"])\n",
    "\n",
    "# Assertion checks whether loaded count is very close to the disk count\n",
    "# Small tolerance accounts for any files that fail to load or be filtered\n",
    "assert abs(final_count - expected_file_count) < 10, \\\n",
    "    f\"Dataset size mismatch! Found {expected_file_count} files but loaded {final_count}.\"\n",
    "\n",
    "assert dataset[0].get(\"image_path\", None), \"image_path missing from first record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise', 'unknown'], id=None)\n",
      "\n",
      "ðŸ“Š Full dataset label distribution (from Dataset object):\n",
      "  anger: 2302 examples\n",
      "  disgust: 309 examples\n",
      "  fear: 1432 examples\n",
      "  happiness: 2892 examples\n",
      "  neutral: 3334 examples\n",
      "  questioning: 1939 examples\n",
      "  sadness: 1706 examples\n",
      "  surprise: 2779 examples\n",
      "  contempt: 421 examples\n",
      "  unknown: 386 examples\n",
      "\n",
      "âš ï¸  Dynamically identified minority classes: ['contempt', 'disgust', 'unknown']\n",
      "\n",
      "ðŸ“‚ Image count per label folder:\n",
      "  anger: 2302 images\n",
      "  contempt: 421 images\n",
      "  disgust: 309 images\n",
      "  fear: 1432 images\n",
      "  happiness: 2892 images\n",
      "  neutral: 3334 images\n",
      "  questioning: 1939 images\n",
      "  sadness: 1706 images\n",
      "  surprise: 2779 images\n",
      "  unknown: 386 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nðŸ“Š Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\nâš ï¸  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nðŸ“‚ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Disgust hash clusters with more than 1 image:\n",
      "ðŸ” Sadness hash clusters with more than 1 image:\n",
      "  - Cluster 958c52e1: 2 images copied for review\n",
      "  - Cluster ee9a8d33: 2 images copied for review\n",
      "  - Cluster d0890396: 2 images copied for review\n",
      "  - Cluster bb0d06f2: 2 images copied for review\n",
      "  - Cluster d7f00fa2: 2 images copied for review\n",
      "ðŸ” Fear hash clusters with more than 1 image:\n",
      "  - Cluster 9ae56592: 2 images copied for review\n",
      "  - Cluster 91c8ee81: 2 images copied for review\n",
      "  - Cluster dae5a596: 2 images copied for review\n",
      "ðŸ” Questioning hash clusters with more than 1 image:\n",
      "  - Cluster da014886: 2 images copied for review\n",
      "  - Cluster 9db42783: 2 images copied for review\n",
      "ðŸ” Contempt hash clusters with more than 1 image:\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Perceptual Clustering for Ambiguous/Confused Classes\n",
    "# --------------------------\n",
    "\n",
    "CLUSTER_TARGETS = [\"disgust\", \"sadness\", \"fear\", \"questioning\", \"contempt\"]\n",
    "\n",
    "for class_name in CLUSTER_TARGETS:\n",
    "    class_dir = os.path.join(BASE_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"âš ï¸ Class dir not found: {class_dir} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    class_images = [\n",
    "        os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
    "        if is_valid_image(f)\n",
    "    ]\n",
    "    hash_map = {}\n",
    "    for path in class_images:\n",
    "        h = compute_hash(path)\n",
    "        if h:\n",
    "            hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "    cluster_dir = os.path.join(SAVE_DIR, f\"{class_name}_clusters\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"ðŸ” {class_name.capitalize()} hash clusters with more than 1 image:\")\n",
    "    for h, paths in hash_map.items():\n",
    "        if len(paths) > 1:\n",
    "            cluster_path = os.path.join(cluster_dir, h)\n",
    "            os.makedirs(cluster_path, exist_ok=True)\n",
    "            for p in paths:\n",
    "                shutil.copy(p, cluster_path)\n",
    "            print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Targeted minority augmentation will apply to: ['contempt', 'disgust', 'questioning']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "label_freqs = Counter(dataset[\"label\"])\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "minority_by_name = [n for n in minority_by_name if n != \"unknown\"]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning', 'contempt']\n",
    "\n",
    "# Merge and deduplicate\n",
    "minority_class_names = list(set(minority_by_name + manual_focus_classes))\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"ðŸŽ¯ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba6434d419f4a3e88bad9bc3305c036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cropping faces and applying augmentations:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Face Cropping: 7272 successful, 10228 failed (used original image).\n",
      "âœ… Augmentation counts: {'anger': 2302, 'contempt': 421, 'disgust': 309, 'fear': 1432, 'happiness': 2892, 'neutral': 3334, 'questioning': 1939, 'sadness': 1706, 'surprise': 2779, 'unknown': 386}\n",
      "âœ… Saved augmentation snapshot to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/V22_augmentation_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# RandAugment for specific minority classes only\n",
    "minority_classes_names = minority_class_names # This is defined in a previous cell\n",
    "minority_classes = [label2id[label] for label in minority_classes_names]\n",
    "\n",
    "minority_aug = T.Compose([\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "])\n",
    "\n",
    "# --- CORRECTED PART ---\n",
    "# Initialize counters in the global scope of the cell\n",
    "aug_count = Counter()\n",
    "face_crop_success_count = 0\n",
    "face_crop_fail_count = 0\n",
    "\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        # Use 'global' to modify variables in the cell's main scope\n",
    "        global face_crop_success_count, face_crop_fail_count\n",
    "        \n",
    "        label = example[\"label\"]\n",
    "        \n",
    "        # --- NEW PREPROCESSING STEP ---\n",
    "        # First, crop the face from the original image.\n",
    "        original_image = example[\"image\"]\n",
    "        cropped_face = crop_face(original_image)\n",
    "        \n",
    "        # Track if cropping was successful\n",
    "        if cropped_face.size != original_image.size:\n",
    "            face_crop_success_count += 1\n",
    "        else:\n",
    "            face_crop_fail_count += 1\n",
    "\n",
    "        # Apply augmentations to the *cropped face*\n",
    "        aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "        aug_count[label] += 1\n",
    "\n",
    "        if cropped_face.mode != \"RGB\":\n",
    "            cropped_face = cropped_face.convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(cropped_face)\n",
    "        \n",
    "        # Process the final augmented image for the model\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function\n",
    "\n",
    "# Apply the transformations\n",
    "transform_fn = make_transform_function(processor, minority_classes)\n",
    "dataset = dataset.map(transform_fn, desc=\"Cropping faces and applying augmentations\")\n",
    "\n",
    "# Print diagnostics\n",
    "print(f\"âœ… Face Cropping: {face_crop_success_count} successful, {face_crop_fail_count} failed (used original image).\")\n",
    "formatted_counts = {LABEL_NAMES[k]: v for k, v in aug_count.items()}\n",
    "print(f\"âœ… Augmentation counts: {formatted_counts}\")\n",
    "\n",
    "# Save snapshot\n",
    "snapshot_path = os.path.join(SAVE_DIR, f\"{VERSION}_augmentation_snapshot.csv\")\n",
    "aug_snapshot = pd.DataFrame.from_dict(dict(aug_count), orient='index', columns=['count'])\n",
    "aug_snapshot.to_csv(snapshot_path)\n",
    "print(f\"âœ… Saved augmentation snapshot to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš¨ Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "ðŸš¨ Label distribution for: train set\n",
      "  anger       : 1859\n",
      "  disgust     : 245\n",
      "  fear        : 1156\n",
      "  happiness   : 2296\n",
      "  neutral     : 2646\n",
      "  questioning : 1535\n",
      "  sadness     : 1388\n",
      "  surprise    : 2230\n",
      "  contempt    : 344\n",
      "  unknown     : 301\n",
      "\n",
      "ðŸš¨ Label distribution for: val set\n",
      "  anger       : 443\n",
      "  disgust     : 64\n",
      "  fear        : 276\n",
      "  happiness   : 596\n",
      "  neutral     : 688\n",
      "  questioning : 404\n",
      "  sadness     : 318\n",
      "  surprise    : 549\n",
      "  contempt    : 77\n",
      "  unknown     : 85\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# ðŸš¦ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e37d1e-a6fb-4e9f-aec5-ee945f11c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Saved label distribution snapshot: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/label_snapshots/V22_label_distribution.csv\n",
      "\n",
      "ðŸš¨ Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "ðŸš¨ Label distribution for: train set\n",
      "  anger       : 1859\n",
      "  disgust     : 245\n",
      "  fear        : 1156\n",
      "  happiness   : 2296\n",
      "  neutral     : 2646\n",
      "  questioning : 1535\n",
      "  sadness     : 1388\n",
      "  surprise    : 2230\n",
      "  contempt    : 344\n",
      "  unknown     : 301\n",
      "\n",
      "ðŸš¨ Label distribution for: val set\n",
      "  anger       : 443\n",
      "  disgust     : 64\n",
      "  fear        : 276\n",
      "  happiness   : 596\n",
      "  neutral     : 688\n",
      "  questioning : 404\n",
      "  sadness     : 318\n",
      "  surprise    : 549\n",
      "  contempt    : 77\n",
      "  unknown     : 85\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Label Distribution Snapshot and Drift Monitor\n",
    "# --------------------------\n",
    "snapshot_dir = os.path.join(SAVE_DIR, \"label_snapshots\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Count current training labels\n",
    "train_label_names = [LABEL_NAMES[i] for i in train_dataset['label']]\n",
    "label_counts = pd.Series(train_label_names).value_counts().sort_index()\n",
    "label_counts.name = VERSION\n",
    "\n",
    "# Save snapshot CSV\n",
    "snapshot_path = os.path.join(snapshot_dir, f\"{VERSION}_label_distribution.csv\")\n",
    "label_counts.to_csv(snapshot_path)\n",
    "print(f\"ðŸ“Š Saved label distribution snapshot: {snapshot_path}\")\n",
    "\n",
    "# Optionally compare to previous version\n",
    "previous_versions = sorted([\n",
    "    f for f in os.listdir(snapshot_dir) if f.endswith(\".csv\") and not f.startswith(VERSION)\n",
    "])\n",
    "if previous_versions:\n",
    "    latest_prev = previous_versions[-1]\n",
    "    prev_df = pd.read_csv(os.path.join(snapshot_dir, latest_prev), index_col=0)\n",
    "    diff = label_counts.subtract(prev_df.iloc[:, 0], fill_value=0)\n",
    "    print(\"ðŸ” Label count change since last snapshot:\")\n",
    "    print(diff)\n",
    "\n",
    "# ðŸš¦ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({4: 3334, 3: 2892, 7: 2779, 0: 2302, 5: 1939, 6: 1706, 2: 1432, 8: 421, 9: 386, 1: 309})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904293161543473c82bf2123115fc9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bfa9fedd9941d7921fc859eb04d2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362781fea16b457f934b65bf7d63ba17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b58ebf588384dd49b6c495b45f48cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3547a85a7ee41cabf4458f683e22fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0c44389cfb4eabbd4e719f380499c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aca8a7f8529494eafff6932e3ddd2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1f1d0b9a384badbebf7eda1f3d460f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eca4314169344b0a5455cc86db34f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d99f9516faa4b35a973759e7eb7ea21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({4: 3334, 3: 2892, 7: 2779, 0: 2302, 2: 2250, 5: 2250, 1: 2250, 6: 2250, 8: 2250, 9: 386})\n",
      "\n",
      "ðŸš¨ Label distribution for: full dataset (post-aug)\n",
      "  anger       : 2302\n",
      "  disgust     : 309\n",
      "  fear        : 1432\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 1939\n",
      "  sadness     : 1706\n",
      "  surprise    : 2779\n",
      "  contempt    : 421\n",
      "  unknown     : 386\n",
      "\n",
      "ðŸš¨ Label distribution for: train set\n",
      "  anger       : 2302\n",
      "  disgust     : 2250\n",
      "  fear        : 2250\n",
      "  happiness   : 2892\n",
      "  neutral     : 3334\n",
      "  questioning : 2250\n",
      "  sadness     : 2250\n",
      "  surprise    : 2779\n",
      "  contempt    : 2250\n",
      "  unknown     : 386\n",
      "\n",
      "ðŸš¨ Label distribution for: val set\n",
      "  anger       : 443\n",
      "  disgust     : 64\n",
      "  fear        : 276\n",
      "  happiness   : 596\n",
      "  neutral     : 688\n",
      "  questioning : 404\n",
      "  sadness     : 318\n",
      "  surprise    : 549\n",
      "  contempt    : 77\n",
      "  unknown     : 85\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11. Balance Dataset (with NO oversampling for 'unknown')\n",
    "# --------------------------\n",
    "MINORITY_CAP = 2250\n",
    "balanced_subsets = []\n",
    "label_counts = Counter(dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    class_name = LABEL_NAMES[label]\n",
    "    if class_name == \"unknown\":\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count < MINORITY_CAP:\n",
    "        multiplier = MINORITY_CAP // len(subset)\n",
    "        remainder = MINORITY_CAP % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        balanced_subsets.append(subset)\n",
    "    else:\n",
    "        # Append full set (no downsampling for majority classes)\n",
    "        balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))\n",
    "\n",
    "hard_classes = ['contempt', 'disgust', 'questioning', 'surprise', 'fear']\n",
    "hard_class_ids = [label2id[c] for c in hard_classes]\n",
    "\n",
    "# Calculate weights: Give hard classes 2x, others 1x\n",
    "weights = [2.0 if l in hard_class_ids else 1.0 for l in train_dataset[\"label\"]]\n",
    "weights = torch.DoubleTensor(weights)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=weights,\n",
    "    num_samples=len(weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# ðŸš¦ Check and print label distributions across all important splits\n",
    "check_all_label_integrity(\n",
    "    {\n",
    "        \"full dataset (post-aug)\": dataset,\n",
    "        \"train set\": train_dataset,\n",
    "        \"val set\": eval_dataset,\n",
    "        # \"post-balance train\": train_dataset_balanced,\n",
    "    },\n",
    "    LABEL_NAMES, label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9367aa62-f6a3-410e-9ce5-3e297a4cb795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum split: 16664 easy, 0 hard\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 12. Curriculum Learning: Staged Hard/Easy Sample Scheduling (with preserved image_path)\n",
    "# ==========================\n",
    "\n",
    "# (A) Use audit_csv_path, or rebuild audit if needed\n",
    "audit_csv_path = os.path.join(SAVE_DIR, \"review_assignment_audit.csv\")\n",
    "if not os.path.exists(audit_csv_path):\n",
    "    # Rebuild (all records have image_path now!)\n",
    "    pd.DataFrame([{\"image_path\": ex[\"image_path\"], \"label\": ex[\"label\"]}\n",
    "                  for ex in dataset]).to_csv(audit_csv_path, index=False)\n",
    "\n",
    "audit_df = pd.read_csv(audit_csv_path)\n",
    "\n",
    "# (B) Map image_path (full path or filename) to index\n",
    "dataset_path_to_idx = {\n",
    "    os.path.basename(ex[\"image_path\"]): i for i, ex in enumerate(dataset)\n",
    "}\n",
    "\n",
    "easy_idxs, hard_idxs = [], []\n",
    "for _, row in audit_df.iterrows():\n",
    "    path_val = row[\"image_path\"]\n",
    "    if not isinstance(path_val, str) or not path_val:\n",
    "        continue\n",
    "    basename = os.path.basename(path_val)\n",
    "    idx = dataset_path_to_idx.get(basename)\n",
    "    if idx is not None:\n",
    "        # For now, assign all as easy unless you have other columns\n",
    "        easy_idxs.append(idx)\n",
    "    else:\n",
    "        print(f\"Image in audit CSV not found in current dataset: {row['image_path']}\")\n",
    "\n",
    "easy_dataset = dataset.select(easy_idxs)\n",
    "hard_dataset = dataset.select(hard_idxs)\n",
    "print(f\"Curriculum split: {len(easy_dataset)} easy, {len(hard_dataset)} hard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 13. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # âœ… (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # âœ… Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # âœ… Log once per epoch\n",
    "    save_safetensors=True                  # âœ… Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 14. Define Compute Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up optimizer with discriminative learning rates (V20 strategy) ---\n",
      "Unfreezing for fine-tuning: 0.attention.attention.query.weight\n",
      "Unfreezing for fine-tuning: 0.attention.attention.query.bias\n",
      "Unfreezing for fine-tuning: 0.attention.attention.key.weight\n",
      "Unfreezing for fine-tuning: 0.attention.attention.key.bias\n",
      "Unfreezing for fine-tuning: 0.attention.attention.value.weight\n",
      "Unfreezing for fine-tuning: 0.attention.attention.value.bias\n",
      "Unfreezing for fine-tuning: 0.attention.output.dense.weight\n",
      "Unfreezing for fine-tuning: 0.attention.output.dense.bias\n",
      "Unfreezing for fine-tuning: 0.intermediate.dense.weight\n",
      "Unfreezing for fine-tuning: 0.intermediate.dense.bias\n",
      "Unfreezing for fine-tuning: 0.output.dense.weight\n",
      "Unfreezing for fine-tuning: 0.output.dense.bias\n",
      "Unfreezing for fine-tuning: 0.layernorm_before.weight\n",
      "Unfreezing for fine-tuning: 0.layernorm_before.bias\n",
      "Unfreezing for fine-tuning: 0.layernorm_after.weight\n",
      "Unfreezing for fine-tuning: 0.layernorm_after.bias\n",
      "Unfreezing for fine-tuning: 1.attention.attention.query.weight\n",
      "Unfreezing for fine-tuning: 1.attention.attention.query.bias\n",
      "Unfreezing for fine-tuning: 1.attention.attention.key.weight\n",
      "Unfreezing for fine-tuning: 1.attention.attention.key.bias\n",
      "Unfreezing for fine-tuning: 1.attention.attention.value.weight\n",
      "Unfreezing for fine-tuning: 1.attention.attention.value.bias\n",
      "Unfreezing for fine-tuning: 1.attention.output.dense.weight\n",
      "Unfreezing for fine-tuning: 1.attention.output.dense.bias\n",
      "Unfreezing for fine-tuning: 1.intermediate.dense.weight\n",
      "Unfreezing for fine-tuning: 1.intermediate.dense.bias\n",
      "Unfreezing for fine-tuning: 1.output.dense.weight\n",
      "Unfreezing for fine-tuning: 1.output.dense.bias\n",
      "Unfreezing for fine-tuning: 1.layernorm_before.weight\n",
      "Unfreezing for fine-tuning: 1.layernorm_before.bias\n",
      "Unfreezing for fine-tuning: 1.layernorm_after.weight\n",
      "Unfreezing for fine-tuning: 1.layernorm_after.bias\n",
      "\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11472' max='14340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11472/14340 2:12:25 < 33:06, 1.44 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>0.475748</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.469324</td>\n",
       "      <td>0.952286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.502400</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.953714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>0.466331</td>\n",
       "      <td>0.953714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.98      0.91      0.94       443\n",
      "     disgust       0.94      0.95      0.95        64\n",
      "        fear       0.89      0.92      0.91       276\n",
      "   happiness       0.99      0.99      0.99       596\n",
      "     neutral       0.97      0.95      0.96       688\n",
      " questioning       0.93      0.95      0.94       404\n",
      "     sadness       0.96      0.94      0.95       318\n",
      "    surprise       0.94      0.98      0.96       549\n",
      "    contempt       0.70      0.83      0.76        77\n",
      "     unknown       1.00      1.00      1.00        85\n",
      "\n",
      "    accuracy                           0.95      3500\n",
      "   macro avg       0.93      0.94      0.94      3500\n",
      "weighted avg       0.95      0.95      0.95      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear â†’ surprise: 18 instances\n",
      "  - anger â†’ neutral: 13 instances\n",
      "  - contempt â†’ questioning: 12 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3570\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - questioning: entropy = 0.5969\n",
      "  - contempt: entropy = 0.5086\n",
      "  - anger: entropy = 0.3608\n",
      "  - fear: entropy = 0.3520\n",
      "  - disgust: entropy = 0.3368\n",
      "  - surprise: entropy = 0.3311\n",
      "  - neutral: entropy = 0.3193\n",
      "  - sadness: entropy = 0.3183\n",
      "  - happiness: entropy = 0.2779\n",
      "  - unknown: entropy = 0.2617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.97      0.92      0.94       443\n",
      "     disgust       0.94      0.95      0.95        64\n",
      "        fear       0.89      0.92      0.91       276\n",
      "   happiness       0.99      0.99      0.99       596\n",
      "     neutral       0.97      0.96      0.97       688\n",
      " questioning       0.94      0.94      0.94       404\n",
      "     sadness       0.96      0.94      0.95       318\n",
      "    surprise       0.94      0.97      0.96       549\n",
      "    contempt       0.69      0.84      0.76        77\n",
      "     unknown       1.00      1.00      1.00        85\n",
      "\n",
      "    accuracy                           0.95      3500\n",
      "   macro avg       0.93      0.94      0.94      3500\n",
      "weighted avg       0.95      0.95      0.95      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear â†’ surprise: 18 instances\n",
      "  - anger â†’ neutral: 13 instances\n",
      "  - questioning â†’ contempt: 13 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3564\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - questioning: entropy = 0.5787\n",
      "  - contempt: entropy = 0.4982\n",
      "  - fear: entropy = 0.3742\n",
      "  - anger: entropy = 0.3588\n",
      "  - surprise: entropy = 0.3423\n",
      "  - disgust: entropy = 0.3210\n",
      "  - sadness: entropy = 0.3127\n",
      "  - neutral: entropy = 0.3110\n",
      "  - happiness: entropy = 0.2843\n",
      "  - unknown: entropy = 0.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.97      0.92      0.94       443\n",
      "     disgust       0.92      0.95      0.94        64\n",
      "        fear       0.89      0.93      0.91       276\n",
      "   happiness       0.99      0.99      0.99       596\n",
      "     neutral       0.97      0.96      0.97       688\n",
      " questioning       0.94      0.94      0.94       404\n",
      "     sadness       0.96      0.94      0.95       318\n",
      "    surprise       0.95      0.97      0.96       549\n",
      "    contempt       0.71      0.84      0.77        77\n",
      "     unknown       1.00      1.00      1.00        85\n",
      "\n",
      "    accuracy                           0.95      3500\n",
      "   macro avg       0.93      0.95      0.94      3500\n",
      "weighted avg       0.96      0.95      0.95      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear â†’ surprise: 14 instances\n",
      "  - anger â†’ neutral: 13 instances\n",
      "  - questioning â†’ contempt: 12 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3480\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - questioning: entropy = 0.5365\n",
      "  - contempt: entropy = 0.5076\n",
      "  - fear: entropy = 0.3492\n",
      "  - anger: entropy = 0.3482\n",
      "  - disgust: entropy = 0.3390\n",
      "  - surprise: entropy = 0.3342\n",
      "  - neutral: entropy = 0.3266\n",
      "  - sadness: entropy = 0.3145\n",
      "  - happiness: entropy = 0.2690\n",
      "  - unknown: entropy = 0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.97      0.92      0.94       443\n",
      "     disgust       0.92      0.95      0.94        64\n",
      "        fear       0.89      0.93      0.91       276\n",
      "   happiness       0.99      0.99      0.99       596\n",
      "     neutral       0.97      0.96      0.97       688\n",
      " questioning       0.95      0.94      0.94       404\n",
      "     sadness       0.96      0.95      0.96       318\n",
      "    surprise       0.95      0.97      0.96       549\n",
      "    contempt       0.69      0.86      0.77        77\n",
      "     unknown       1.00      1.00      1.00        85\n",
      "\n",
      "    accuracy                           0.95      3500\n",
      "   macro avg       0.93      0.95      0.94      3500\n",
      "weighted avg       0.96      0.95      0.95      3500\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear â†’ surprise: 14 instances\n",
      "  - questioning â†’ contempt: 14 instances\n",
      "  - anger â†’ neutral: 13 instances\n",
      "\n",
      "ðŸ§  Avg prediction entropy: 0.3517\n",
      "\n",
      "ðŸ” Class entropies (sorted):\n",
      "  - questioning: entropy = 0.5698\n",
      "  - contempt: entropy = 0.4899\n",
      "  - anger: entropy = 0.3609\n",
      "  - fear: entropy = 0.3534\n",
      "  - disgust: entropy = 0.3275\n",
      "  - surprise: entropy = 0.3258\n",
      "  - neutral: entropy = 0.3126\n",
      "  - sadness: entropy = 0.3084\n",
      "  - happiness: entropy = 0.2865\n",
      "  - unknown: entropy = 0.2562\n",
      "--- Training Finished ---\n",
      "\n",
      "--- Final Diagnostics ---\n",
      "Classifier head weights (after train): Parameter containing:\n",
      "tensor([[-0.0041, -0.0302, -0.0013,  ..., -0.0062,  0.0069, -0.0187],\n",
      "        [ 0.0025,  0.0049,  0.0366,  ...,  0.0505,  0.0169,  0.0348],\n",
      "        [-0.0140, -0.0431, -0.0182,  ...,  0.0445, -0.0157,  0.0241],\n",
      "        ...,\n",
      "        [-0.0366,  0.0031,  0.0099,  ..., -0.0049,  0.0039, -0.0331],\n",
      "        [ 0.0349,  0.0229,  0.0426,  ...,  0.0541,  0.0217,  0.0009],\n",
      "        [ 0.0040,  0.0166, -0.0146,  ..., -0.0002,  0.0298, -0.0380]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "Classifier head bias (after train): Parameter containing:\n",
      "tensor([ 0.0134, -0.0043, -0.0224, -0.0157,  0.0048, -0.0424, -0.0321,  0.0303,\n",
      "        -0.0017, -0.0416], device='mps:0', requires_grad=True)\n",
      "Classifier weight in optimizer param group: True\n",
      "Classifier bias in optimizer param group: True\n",
      "Saving model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701\n",
      "âœ… Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701\n",
      "âœ… Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701\n",
      "âœ… State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/final_model.pth\n",
      "âœ… Memory cleanup complete after save.\n",
      "--- Model saved to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701 ---\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 15. Optimizer, Scheduler, and Training (Attention Pooling)\n",
    "# --------------------------\n",
    "\n",
    "# --- Part A: Discriminative Learning Rate Optimizer Setup ---\n",
    "print(\"--- Setting up optimizer with discriminative learning rates (V20 strategy) ---\")\n",
    "\n",
    "# Define different learning rates for the head and the backbone\n",
    "head_lr = 5e-5      # Higher learning rate for the classifier head\n",
    "backbone_lr = 2e-7  # Much lower learning rate for the last two backbone layers\n",
    "\n",
    "# First, ensure all layers are frozen by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the classifier head to be trained\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the last 2 layers of the ViT encoder for fine-tuning.\n",
    "for name, param in model.vit.encoder.layer[-2:].named_parameters():\n",
    "    param.requires_grad = True\n",
    "    print(f\"Unfreezing for fine-tuning: {name}\")\n",
    "\n",
    "# Create parameter groups for the optimizer\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.classifier.parameters(), 'lr': head_lr},\n",
    "    {'params': model.vit.encoder.layer[-2:].parameters(), 'lr': backbone_lr}\n",
    "]\n",
    "\n",
    "# Create the AdamW optimizer with the specified parameter groups.\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
    "\n",
    "# --- Part B: Trainer Initialization ---\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.metric_for_best_model = \"eval_loss\"\n",
    "training_args.evaluation_strategy = \"epoch\"\n",
    "training_args.save_strategy = \"epoch\"\n",
    "\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    "    optimizers=(optimizer, None),\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "# --- Part C: Custom Scheduler Setup ---\n",
    "scheduler = CosineAnnealingWarmRestarts(trainer.optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "original_train = trainer.train\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)\n",
    "    return result\n",
    "trainer.train = modified_train\n",
    "\n",
    "# --- Part D: Train the Model and Finalize ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "trainer.train()\n",
    "print(\"--- Training Finished ---\")\n",
    "\n",
    "# === Diagnostics: Model Head and Optimizer State ===\n",
    "print(\"\\n--- Final Diagnostics ---\")\n",
    "print(\"Classifier head weights (after train):\", model.classifier.weight)\n",
    "print(\"Classifier head bias (after train):\", model.classifier.bias)\n",
    "\n",
    "opt_params = set([p for group in trainer.optimizer.param_groups for p in group['params']])\n",
    "print(\"Classifier weight in optimizer param group:\", model.classifier.weight in opt_params)\n",
    "print(\"Classifier bias in optimizer param group:\", model.classifier.bias in opt_params)\n",
    "\n",
    "assert model.classifier.weight in opt_params, \"CRITICAL ERROR: Classifier weight NOT in optimizer!\"\n",
    "assert model.classifier.bias in opt_params, \"CRITICAL ERROR: Classifier bias NOT in optimizer!\"\n",
    "\n",
    "# Save the final model and processor\n",
    "save_model_and_processor(model, processor, SAVE_DIR)\n",
    "print(f\"--- Model saved to {SAVE_DIR} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # 16. Rescue & Save from Last Checkpoint (after training)\n",
    "# # --------------------------\n",
    "# #in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"âœ… Processor manually re-saved.\")\n",
    "\n",
    "# # Use parent directory of SAVE_DIR to locate latest V* folder\n",
    "# parent_dir = os.path.dirname(SAVE_DIR)\n",
    "# v_folders = [\n",
    "#     d for d in os.listdir(parent_dir)\n",
    "#     if os.path.isdir(os.path.join(parent_dir, d)) and d.startswith(\"V\")\n",
    "# ]\n",
    "\n",
    "# def extract_timestamp(name):\n",
    "#     try:\n",
    "#         _, date_str, time_str = name.split(\"_\")\n",
    "#         return datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "#     except Exception:\n",
    "#         return datetime.min\n",
    "\n",
    "# latest_version_folder = max(v_folders, key=extract_timestamp)\n",
    "# latest_version_path = os.path.join(parent_dir, latest_version_folder)\n",
    "# print(f\"ðŸ—‚ï¸ Using latest version folder: {latest_version_path}\")\n",
    "\n",
    "# # Locate latest checkpoint within that version folder\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(latest_version_path, d)\n",
    "#     for d in os.listdir(latest_version_path)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(latest_version_path, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"âŒ No checkpoint found in latest version folder.\")\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"âœ… Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # Load model and processor from latest checkpoint and save them\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(latest_version_path)\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701 were not used when initializing ViTForImageClassification: ['vit.pooler.attention_net.0.bias', 'vit.pooler.attention_net.0.weight', 'vit.pooler.attention_net.2.bias', 'vit.pooler.attention_net.2.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"âœ… Model reloaded for inference.\")\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"âœ… Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 274/274 [05:40<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference complete. Skipped 0 invalid image(s).\n",
      "ðŸ“ Saved REVIEW file paths to V22_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/V22_distribution_plot_20250701_094701.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"ðŸ“ Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701\n",
      "ðŸ“‚ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/logits_eval_V22.npy\n",
      "ðŸ“‚ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/labels_eval_V22.npy\n",
      "âœ… Optimal temperature: 1.1431\n",
      "âœ… Calibrated Log Loss: 0.2283\n",
      "ðŸ“Š Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/V22_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 19. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"ðŸ“ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"âš ï¸ Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaeec19-d4c7-43a6-80cb-86aafd573989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed tagging + copying REVIEW predictions to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/review_predictions_by_class\n",
      "ðŸ“„ CSV log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/V22_review_predictions_with_preds.csv\n",
      "âœ… Review assignments (with audit) complete.\n",
      "Assignment summary: Counter({'neutral': 3185, 'happiness': 2955, 'surprise': 2877, 'anger': 2366, 'sadness': 1605, 'unknown': 1483, 'fear': 1381, 'REVIEW': 623, 'questioning': 430, 'disgust': 246, 'contempt': 209})\n",
      "âœ… Saved 262 clusters to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/review_predictions_clustered\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 20. Review & Relabel 'REVIEW' Predictions (with Audit Logging & Clustering)\n",
    "# --------------------------\n",
    "MINORITY_LABELS = [\"disgust\", \"contempt\", \"fear\", \"questioning\"]\n",
    "MINORITY_ENTROPY_THRESH = 0.6\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "REVIEW_CLUSTER_DIR = os.path.join(SAVE_DIR, \"review_predictions_clustered\")\n",
    "\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "os.makedirs(REVIEW_CLUSTER_DIR, exist_ok=True)\n",
    "\n",
    "# ---- If you HAVE NOT already generated review CSV (inference stage) ----\n",
    "if not os.path.exists(REVIEW_CSV_LOG):\n",
    "    review_log = []\n",
    "    image_paths = [\n",
    "        p for p in Path(IMAGE_DIR).rglob(\"*\")\n",
    "        if p.is_file() and p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "    ]\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inputs).logits\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                conf, pred_idx = torch.max(probs, dim=-1)\n",
    "            conf_val = conf.item()\n",
    "            pred_label = id2label[pred_idx.item()]\n",
    "            \n",
    "            entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1).item()\n",
    "\n",
    "            if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "                tag = \"unknown\"\n",
    "            elif conf_val < REVIEW_THRESHOLD:\n",
    "                tag = \"REVIEW\"\n",
    "            else:\n",
    "                tag = pred_label\n",
    "            \n",
    "            review_log.append({\n",
    "                \"image_path\": str(img_path),\n",
    "                \"predicted_label\": pred_label,\n",
    "                \"confidence\": round(conf_val, 4),\n",
    "                \"entropy\": round(entropy, 4),\n",
    "                \"tag\": tag\n",
    "            })\n",
    "            \n",
    "            # For backward compatibility, still copy to REVIEW_BY_CLASS_DIR if tag is not \"unknown\"\n",
    "            if tag not in [\"unknown\"]:\n",
    "                target_dir = os.path.join(REVIEW_BY_CLASS_DIR, tag)\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                shutil.copy(str(img_path), target_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error with image: {img_path} | {e}\")\n",
    "            \n",
    "    pd.DataFrame(review_log).to_csv(REVIEW_CSV_LOG, index=False)\n",
    "    print(f\"âœ… Completed tagging + copying REVIEW predictions to: {REVIEW_BY_CLASS_DIR}\")\n",
    "    print(f\"ðŸ“„ CSV log saved to: {REVIEW_CSV_LOG}\")\n",
    "\n",
    "# ---- If you already HAVE a review CSV (assignment/audit stage) ----\n",
    "df = pd.read_csv(REVIEW_CSV_LOG)\n",
    "review_assignment_log = []\n",
    "for _, row in df.iterrows():\n",
    "    path = row[\"image_path\"]\n",
    "    pred_label = row[\"predicted_label\"]\n",
    "    conf = float(row[\"confidence\"])\n",
    "    true_label = os.path.basename(os.path.dirname(path))\n",
    "    \n",
    "    entropy = float(row.get(\"entropy\", 0))  # default to 0 if not present\n",
    "    if pred_label in MINORITY_LABELS and entropy > MINORITY_ENTROPY_THRESH:\n",
    "        assigned = \"unknown\"\n",
    "    elif conf < REVIEW_THRESHOLD:\n",
    "        assigned = \"REVIEW\"\n",
    "    else:\n",
    "        assigned = pred_label\n",
    "    \n",
    "    dest_dir = os.path.join(REVIEW_BY_CLASS_DIR, assigned)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    shutil.copy(path, dest_dir)\n",
    "    review_assignment_log.append([path, true_label, pred_label, conf, assigned, entropy])\n",
    "\n",
    "log_df = pd.DataFrame(\n",
    "    review_assignment_log,\n",
    "    columns=[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\", \"entropy\"]\n",
    ")\n",
    "\n",
    "log_df.to_csv(os.path.join(SAVE_DIR, \"review_assignment_audit.csv\"), index=False)\n",
    "print(\"âœ… Review assignments (with audit) complete.\")\n",
    "\n",
    "print(\"Assignment summary:\", Counter(log_df[\"assigned_folder\"]))\n",
    "\n",
    "# ---- Perceptual hash clustering of review pool ----\n",
    "def phash_distance(hash1, hash2):\n",
    "    return hash1 - hash2\n",
    "\n",
    "PHASH_CLUSTER_THRESHOLD = 6\n",
    "image_paths = [row[0] for row in review_assignment_log if row[4] != \"unknown\"]  # assigned to a class\n",
    "\n",
    "hashes = []\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"L\").resize((64, 64))\n",
    "        hashes.append(hex_to_hash(str(phash(img))))\n",
    "    except Exception as e:\n",
    "        print(f\"phash error: {img_path} | {e}\")\n",
    "\n",
    "clusters = []\n",
    "used = set()\n",
    "for i, h1 in enumerate(hashes):\n",
    "    if i in used:\n",
    "        continue\n",
    "    cluster = [image_paths[i]]\n",
    "    used.add(i)\n",
    "    for j, h2 in enumerate(hashes):\n",
    "        if j <= i or j in used:\n",
    "            continue\n",
    "        if phash_distance(h1, h2) <= PHASH_CLUSTER_THRESHOLD:\n",
    "            cluster.append(image_paths[j])\n",
    "            used.add(j)\n",
    "    if len(cluster) > 1:\n",
    "        clusters.append(cluster)\n",
    "\n",
    "if not clusters:\n",
    "    print(f\"âš ï¸ No clusters found for review. {REVIEW_CLUSTER_DIR} will remain empty.\")\n",
    "else:\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        out_dir = os.path.join(REVIEW_CLUSTER_DIR, f\"cluster_{idx}\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        for p in cluster:\n",
    "            shutil.copy(p, out_dir)\n",
    "    print(f\"âœ… Saved {len(clusters)} clusters to {REVIEW_CLUSTER_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc464dc0-3204-453c-9a4c-4e63700ad6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flagged 11 hard negatives for ('contempt', 'questioning'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/review_hardneg_contempt_questioning.txt\n",
      "\n",
      "Flagged 14 hard negatives for ('fear', 'surprise'):\n",
      "  Saved list: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/review_hardneg_fear_surprise.txt\n",
      "ðŸ” Found 17360 total predictions (CSV) and 1575 REVIEW-tagged paths.\n",
      "ðŸ“‚ Grouped 1563 REVIEW images into folders by predicted label in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V22_20250701_094701/review_predictions_by_class\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 21. REVIEW Pool Diagnostics & Hard Confusion Mining\n",
    "# --------------------------\n",
    "\n",
    "# A. Flag hard confusion pairs for manual review\n",
    "REVIEW_CONFUSION_PAIRS = [(\"contempt\", \"questioning\"), (\"fear\", \"surprise\")]\n",
    "\n",
    "def parse_review_confusions(csv_path, confusion_pairs):\n",
    "    import csv\n",
    "    flagged_imgs = {pair: [] for pair in confusion_pairs}\n",
    "    with open(csv_path) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            pred = row[\"predicted_label\"]\n",
    "            true = os.path.basename(os.path.dirname(row[\"image_path\"]))\n",
    "            conf = float(row[\"confidence\"])\n",
    "            for a, b in confusion_pairs:\n",
    "                if ((pred == a and true == b) or (pred == b and true == a)) and conf < 0.8:\n",
    "                    flagged_imgs[(a, b)].append(row[\"image_path\"])\n",
    "    return flagged_imgs\n",
    "\n",
    "confusion_candidates = parse_review_confusions(REVIEW_CSV_LOG, REVIEW_CONFUSION_PAIRS)\n",
    "for pair, imgs in confusion_candidates.items():\n",
    "    print(f\"\\nFlagged {len(imgs)} hard negatives for {pair}:\")\n",
    "    out_path = os.path.join(SAVE_DIR, f\"review_hardneg_{pair[0]}_{pair[1]}.txt\")\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(imgs))\n",
    "    print(f\"  Saved list: {out_path}\")\n",
    "\n",
    "# B. Organize REVIEW-tagged images by predicted class (for curation)\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "review_txt_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "\n",
    "if os.path.exists(review_txt_path) and os.path.exists(csv_path):\n",
    "    with open(review_txt_path, \"r\") as f:\n",
    "        review_paths = {line.strip() for line in f.readlines()}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    count = 0\n",
    "\n",
    "    print(f\"ðŸ” Found {len(df)} total predictions (CSV) and {len(review_paths)} REVIEW-tagged paths.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = row[\"image_path\"]\n",
    "        label = row[\"predicted_label\"]\n",
    "        conf = row[\"confidence\"]\n",
    "\n",
    "        if path in review_paths and label != \"REVIEW\":\n",
    "            dest_dir = os.path.join(REVIEW_SORT_DIR, label)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy(path, dest_dir)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"ðŸ“‚ Grouped {count} REVIEW images into folders by predicted label in: {REVIEW_SORT_DIR}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Missing review candidates file or prediction CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label name/id mapping:\n",
      "0: anger\n",
      "1: disgust\n",
      "2: fear\n",
      "3: happiness\n",
      "4: neutral\n",
      "5: questioning\n",
      "6: sadness\n",
      "7: surprise\n",
      "8: contempt\n",
      "9: unknown\n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.8880        contempt  \n",
      "1      0.9229        contempt  \n",
      "2      0.9509         neutral  \n",
      "3      0.9280        contempt  \n",
      "4      0.9030        contempt  \n",
      "Sample review predictions (audit):\n",
      "                                          image_path true_label pred_label  \\\n",
      "0  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "1  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "2  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt    neutral   \n",
      "3  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "4  /Users/natalyagrokh/AI/ml_expressions/img_data...   contempt   contempt   \n",
      "\n",
      "   confidence assigned_folder  \n",
      "0      0.8880        contempt  \n",
      "1      0.9229        contempt  \n",
      "2      0.9509         neutral  \n",
      "3      0.9280        contempt  \n",
      "4      0.9030        contempt  \n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 22. Visualization & Error Tracking\n",
    "# --------------------------\n",
    "\n",
    "print(\"Label name/id mapping:\")\n",
    "for idx, name in enumerate(LABEL_NAMES):\n",
    "    print(f\"{idx}: {name}\")\n",
    "\n",
    "# Defensive: Check that metrics file exists before plotting\n",
    "per_class_csv = os.path.join(SAVE_DIR, \"per_class_metrics.csv\")\n",
    "if not os.path.exists(per_class_csv):\n",
    "    print(f\"âš ï¸ Metrics file {per_class_csv} not found.\")\n",
    "else:\n",
    "    metrics_df = pd.read_csv(per_class_csv)\n",
    "    last_row = metrics_df.iloc[-1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    f1s = [last_row[f\"f1_{n}\"] for n in LABEL_NAMES]\n",
    "    ax.bar(LABEL_NAMES, f1s)\n",
    "    ax.set_title(\"Per-Class F1 (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_f1.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Bar plot of per-class entropy\n",
    "    entropies = [last_row[f\"entropy_{n}\"] for n in LABEL_NAMES]\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    ax.bar(LABEL_NAMES, entropies)\n",
    "    ax.set_title(\"Per-Class Mean Entropy (Last Epoch)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"per_class_entropy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram for REVIEW pool\n",
    "    review_counts = Counter()\n",
    "    if os.path.exists(REVIEW_SORT_DIR):\n",
    "        for label_dir in os.listdir(REVIEW_SORT_DIR):\n",
    "            count = len(os.listdir(os.path.join(REVIEW_SORT_DIR, label_dir)))\n",
    "            review_counts[label_dir] = count\n",
    "        plt.bar(review_counts.keys(), review_counts.values())\n",
    "        plt.title(\"REVIEW Pool Distribution by Predicted Class\")\n",
    "        plt.savefig(os.path.join(SAVE_DIR, \"review_pool_distribution.png\"))\n",
    "        plt.close()\n",
    "        # Flag if >70% in one class\n",
    "        total = sum(review_counts.values())\n",
    "        for label, count in review_counts.items():\n",
    "            if total > 0 and count / total > 0.7:\n",
    "                print(f\"âš ï¸ REVIEW pool highly imbalanced: {count/total:.1%} in '{label}'\")\n",
    "\n",
    "    # Audit print block (as before)\n",
    "    print(\"Sample review predictions (audit):\")\n",
    "    if 'log_df' in locals():\n",
    "        print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "    elif 'df' in locals():\n",
    "        print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "    else:\n",
    "        print(\"No review/audit DataFrame found for printing.\")\n",
    "\n",
    "# âœ… AUDIT BLOCK\n",
    "print(\"Sample review predictions (audit):\")\n",
    "if 'log_df' in locals():\n",
    "    print(log_df[[\"image_path\", \"true_label\", \"pred_label\", \"confidence\", \"assigned_folder\"]].head())\n",
    "elif 'df' in locals():\n",
    "    print(df[[\"image_path\", \"true_label\", \"predicted_label\", \"confidence\"]].head())\n",
    "else:\n",
    "    print(\"No review/audit DataFrame found for printing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55576ae4-91ae-4d68-b538-2b57a0b46089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ Entropy > 0.4 for class 'questioning': 0.57\n",
      "ðŸš¨ F1 < 0.8 for class 'contempt': 0.77\n",
      "ðŸš¨ Entropy > 0.4 for class 'contempt': 0.49\n",
      "âš ï¸ Some classes not deployment-ready! Address above issues before production.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 23. Deployment Readiness Assertions and Flags\n",
    "# --------------------------\n",
    "\n",
    "# Load metrics\n",
    "metrics_df = pd.read_csv(os.path.join(SAVE_DIR, \"per_class_metrics.csv\"))\n",
    "last = metrics_df.iloc[-1]\n",
    "warn = False\n",
    "\n",
    "for cname in LABEL_NAMES:\n",
    "    f1 = last[f\"f1_{cname}\"]\n",
    "    entropy = last[f\"entropy_{cname}\"]\n",
    "    if f1 < 0.8:\n",
    "        print(f\"ðŸš¨ F1 < 0.8 for class '{cname}': {f1:.2f}\")\n",
    "        warn = True\n",
    "    if entropy > 0.4:\n",
    "        print(f\"ðŸš¨ Entropy > 0.4 for class '{cname}': {entropy:.2f}\")\n",
    "        warn = True\n",
    "\n",
    "if not warn:\n",
    "    print(\"âœ… All classes ready for deployment: F1 >= 0.8 and entropy <= 0.4\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some classes not deployment-ready! Address above issues before production.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9e53267-5a68-47f7-b0c9-babcecdcfe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up Model Ensembling Analysis ---\n",
      "âœ… Dynamically selected models for ensembling:\n",
      "   - Model 1: V21_20250630_142520\n",
      "   - Model 2: V20_20250629_134956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V21_20250630_142520 were not used when initializing ViTForImageClassification: ['vit.pooler.attention_net.0.bias', 'vit.pooler.attention_net.0.weight', 'vit.pooler.attention_net.2.bias', 'vit.pooler.attention_net.2.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Ensemble Analysis on Hard Cases ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ensemble_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m true_label \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     87\u001b[0m single_model_pred \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 89\u001b[0m ensemble_pred, ensemble_conf, individual_preds \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_predict\u001b[49m(\n\u001b[1;32m     90\u001b[0m     ensemble_models, processor, image_path, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(image_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - True Label:      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrue_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ensemble_predict' is not defined"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 24. Model Ensembling Analysis\n",
    "# ===================================================================\n",
    "\n",
    "print(\"--- Setting up Model Ensembling Analysis ---\")\n",
    "\n",
    "# --- Part A: Dynamic Model Loading ---\n",
    "# This section dynamically finds the two most recent, successfully saved model versions.\n",
    "\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "def extract_version_from_path(path):\n",
    "    match = re.search(r\"V(\\d+)\", os.path.basename(path))\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Find all valid model directories that contain saved model weights\n",
    "all_model_dirs = [\n",
    "    os.path.join(MODEL_ROOT, d)\n",
    "    for d in os.listdir(MODEL_ROOT)\n",
    "    if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))\n",
    "       and (os.path.exists(os.path.join(MODEL_ROOT, d, \"model.safetensors\")) or \n",
    "            os.path.exists(os.path.join(MODEL_ROOT, d, \"pytorch_model.bin\")))\n",
    "]\n",
    "\n",
    "# Exclude the current run's directory\n",
    "all_model_dirs = [d for d in all_model_dirs if os.path.abspath(d) != os.path.abspath(SAVE_DIR)]\n",
    "\n",
    "# Sort by version number and select the two most recent\n",
    "if len(all_model_dirs) < 2:\n",
    "    print(\"âš ï¸ Found fewer than two previous model versions. Skipping ensembling.\")\n",
    "else:\n",
    "    sorted_models = sorted(all_model_dirs, key=extract_version_from_path, reverse=True)\n",
    "    model_path_1 = sorted_models[0] # Most recent (e.g., V20)\n",
    "    model_path_2 = sorted_models[1] # Second most recent (e.g., V19)\n",
    "\n",
    "    print(f\"âœ… Dynamically selected models for ensembling:\")\n",
    "    print(f\"   - Model 1: {os.path.basename(model_path_1)}\")\n",
    "    print(f\"   - Model 2: {os.path.basename(model_path_2)}\")\n",
    "\n",
    "    # Load the models\n",
    "    model_1 = AutoModelForImageClassification.from_pretrained(model_path_1).to(device).eval()\n",
    "    model_2 = AutoModelForImageClassification.from_pretrained(model_path_2).to(device).eval()\n",
    "    ensemble_models = [model_1, model_2]\n",
    "\n",
    "\n",
    "    # Returns a boolean tensor: True if the prediction is low-confidence\n",
    "    def is_uncertain(probs, threshold=0.85, entropy_thresh=1.3):\n",
    "        conf, _ = torch.max(probs, dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1)\n",
    "        return (conf < threshold) | (entropy > entropy_thresh)\n",
    "    \n",
    "        # --- Part B: Ensemble Prediction Function ---\n",
    "        def ensemble_predict(models, processor, image_path, device=\"cpu\"):\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "            softmaxes = []\n",
    "            individual_preds = []\n",
    "    \n",
    "            for m in models:\n",
    "                with torch.no_grad():\n",
    "                    logits = m(**inputs).logits\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    individual_preds.append(id2label[torch.argmax(probs, dim=-1).item()])\n",
    "                    softmaxes.append(probs.cpu().numpy())\n",
    "            \n",
    "            avg_probs = np.mean(softmaxes, axis=0)\n",
    "            ensemble_pred_idx = np.argmax(avg_probs)\n",
    "            ensemble_conf = avg_probs[0, ensemble_pred_idx]\n",
    "            \n",
    "            return id2label[ensemble_pred_idx], ensemble_conf, individual_preds\n",
    "    \n",
    "\n",
    "    # --- Part C: Run Analysis on Misclassified Images ---\n",
    "    # Load the audit file from the most recent run to find challenging images\n",
    "    audit_csv_path = os.path.join(SAVE_DIR, \"review_assignment_audit.csv\")\n",
    "    if os.path.exists(audit_csv_path):\n",
    "        print(\"\\n--- Running Ensemble Analysis on Hard Cases ---\")\n",
    "        audit_df = pd.read_csv(audit_csv_path)\n",
    "        \n",
    "        # Filter for images that were misclassified by the single model\n",
    "        misclassified_df = audit_df[audit_df['true_label'] != audit_df['pred_label']]\n",
    "        \n",
    "        # Analyze the top 5 hardest cases\n",
    "        for _, row in misclassified_df.head(5).iterrows():\n",
    "            image_path = row['image_path']\n",
    "            true_label = row['true_label']\n",
    "            single_model_pred = row['pred_label']\n",
    "            \n",
    "            ensemble_pred, ensemble_conf, individual_preds = ensemble_predict(\n",
    "                ensemble_models, processor, image_path, device=device\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nImage: {os.path.basename(image_path)}\")\n",
    "            print(f\"  - True Label:      {true_label}\")\n",
    "            print(f\"  - Model 1 ({os.path.basename(model_path_1)}) Pred: {individual_preds[0]}\")\n",
    "            print(f\"  - Model 2 ({os.path.basename(model_path_2)}) Pred: {individual_preds[1]}\")\n",
    "            print(f\"  - ENSEMBLE Pred:   {ensemble_pred} (Confidence: {ensemble_conf:.2f})\")\n",
    "            \n",
    "            if ensemble_pred == true_label and single_model_pred != true_label:\n",
    "                print(\"  - âœ… SUCCESS: Ensemble corrected the misclassification!\")\n",
    "            elif ensemble_pred != true_label:\n",
    "                print(\"  - âŒ FAILURE: Ensemble also failed to classify correctly.\")\n",
    "            else:\n",
    "                print(\"  - âš ï¸ NOTE: Ensemble agreed with a correct single-model prediction (edge case).\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Could not find audit CSV at {audit_csv_path}. Skipping ensemble analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
