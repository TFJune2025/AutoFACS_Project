{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2025-05-03 16:33:02.288342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746289982.310914    4593 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746289982.318394    4593 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746289982.337410    4593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746289982.337437    4593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746289982.337446    4593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746289982.337454    4593 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 16:33:02.342620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, log_loss\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import LBFGS\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de48cd2-aba6-46e2-8340-aeffa93211ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # GPU Environment Setup for Multi-GPU Optimization (GPUs 0-n)\n",
    "# # --------------------------\n",
    "# # Limit process to specific GPUs\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" #0, 1, 2, 3,...n\n",
    "# print(\"Process restricted to GPUs:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# # Ensure pip executables are available\n",
    "# os.environ[\"PATH\"] = f\"{os.path.expanduser('~/.local/bin')}:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# # Enable memory growth for TensorFlow\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         print(\"Memory growth enabled on GPUs.\")\n",
    "#     except RuntimeError as e:\n",
    "#         print(\"Error configuring GPUs:\", e)\n",
    "# print(\"GPUs available to this process (as seen by TensorFlow):\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# # Optional: Monitor current GPU usage\n",
    "# gpu_usage = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\")\n",
    "# print(\"Current GPU usage:\\n\", gpu_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /home/ubuntu/MLexpressionsStorage/V8_20250503_190001\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 0. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/home/ubuntu/MLexpressionsStorage/img_datasets/ferckjalf_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"happiness\",\n",
    "    \"sadness\", \"surprise\", \"neutral\", \"questioning\"\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in glob.glob(os.path.join(base_dir, \"V*_vit_final_independent\"))\n",
    "    ]\n",
    "    versions = [int(d[1:].split(\"_\")[0]) for d in existing if d.startswith(\"V\") and \"_\" in d]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/home/ubuntu/MLexpressionsStorage\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/home/ubuntu/MLexpressionsStorage\", VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto-loaded model from: /home/ubuntu/MLexpressionsStorage/V8_20250503_190001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Auto-Load Latest Pretrained Model and Processor\n",
    "# --------------------------\n",
    "\n",
    "# Automatically load latest model path\n",
    "MODEL_ROOT = \"/home/ubuntu/MLexpressionsStorage\"\n",
    "model_dirs = sorted(\n",
    "    [os.path.join(MODEL_ROOT, d) for d in os.listdir(MODEL_ROOT) if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))],\n",
    "    key=os.path.getmtime,\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "if not model_dirs:\n",
    "    raise FileNotFoundError(\"‚ùå No model folders found matching pattern 'V*' in MODEL_ROOT.\")\n",
    "model_path = model_dirs[0]\n",
    "print(f\"‚úÖ Auto-loaded model from: {model_path}\")\n",
    "\n",
    "# Load base model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Replace classification head to match current label schema\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, len(id2label))\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c378153-1b3b-495f-8e64-17031286a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # Rescue & Save from Last Checkpoint\n",
    "# # --------------------------\n",
    "\n",
    "# # in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"‚úÖ Processor manually re-saved.\")\n",
    "\n",
    "# # ‚úÖ Set correct version path (edit when needed)\n",
    "# SAVE_DIR = \"/home/ubuntu/MLexpressionsStorage/V8_20250427_155958\"  # replace with correct version\n",
    "\n",
    "# # üîç Locate latest checkpoint inside SAVE_DIR\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(SAVE_DIR, d) for d in os.listdir(SAVE_DIR)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(SAVE_DIR, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"‚ùå No checkpoint found to recover from.\")\n",
    "\n",
    "# # to set checkpoint manually:    \n",
    "# # latest_checkpiont = os.path.join(SAVE_DIR, \"checkpoint-22500\")  # or checkpoint-18000\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # üß† Load model & processor from checkpoint\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(SAVE_DIR)\n",
    "\n",
    "# # üßØ Move model to CPU (to avoid GPU I/O hangs)\n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "# # üíæ Save model state_dict\n",
    "# torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"final_model.pth\"))\n",
    "# print(f\"‚úÖ State dict saved to: {SAVE_DIR}/final_model.pth\")\n",
    "\n",
    "# # üíæ Save full model and processor in Hugging Face-style\n",
    "# model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(f\"‚úÖ Model and processor saved to: {SAVE_DIR}\")\n",
    "\n",
    "# # üíæ (Optional) Save Trainer backup\n",
    "# try:\n",
    "#     trainer.save_model(os.path.join(SAVE_DIR, \"backup_trainer_model\"))\n",
    "#     print(f\"‚úÖ Trainer backup saved to: {os.path.join(SAVE_DIR, 'backup_trainer_model')}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "\n",
    "# # üßπ Clean up memory\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"‚úÖ Memory cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples after filtering: 32041\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2. Load and Prepare Dataset\n",
    "# --------------------------\n",
    "features = Features({\n",
    "    'image': DatasetsImage(),\n",
    "    'label': ClassLabel(names=LABEL_NAMES)\n",
    "})\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=\"/home/ubuntu/MLexpressionsStorage/img_datasets/ferckjalf_dataset\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/tmp/hf_cache\",\n",
    "    features=features  # ‚úÖ manually define all 8 labels\n",
    ")\n",
    "\n",
    "counter = {\"n\": 0}\n",
    "\n",
    "def reconcile_labels(example):\n",
    "    counter[\"n\"] += 1\n",
    "    if counter[\"n\"] % 1000 == 0:\n",
    "        print(f\"Processed {counter['n']} images...\")\n",
    "        \n",
    "    # If the label is already an integer, convert it to a string using the dataset features.\n",
    "    if isinstance(example[\"label\"], int):\n",
    "        # Use dataset.features[\"label\"].int2str to get the string label.\n",
    "        original_label = dataset.features[\"label\"].int2str(example[\"label\"]).strip().lower()\n",
    "    else:\n",
    "        original_label = example[\"label\"].strip().lower()\n",
    "\n",
    "    # Map the lowercased label to the pre-trained model's expected label.\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    # If not recognized, mark it for filtering.\n",
    "    # Covert the mapped label to its corresponding integer.\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example\n",
    "\n",
    "# Apply reconciliation function to dataset.\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset\", with_indices=False, num_proc=8)\n",
    "# Filter out any examples that were marked as unrecognized.\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "print(\"Total examples after filtering:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'neutral', 'questioning'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 4702 examples\n",
      "  disgust: 607 examples\n",
      "  fear: 4671 examples\n",
      "  happiness: 8869 examples\n",
      "  sadness: 1221 examples\n",
      "  surprise: 4491 examples\n",
      "  neutral: 5996 examples\n",
      "  questioning: 1484 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes: ['disgust', 'sadness', 'questioning']\n",
      "\n",
      "üìÇ Image count per label folder:\n",
      "  .ipynb_checkpoints: 0 images\n",
      "  anger: 4702 images\n",
      "  disgust: 607 images\n",
      "  fear: 4671 images\n",
      "  happiness: 8869 images\n",
      "  neutral: 5996 images\n",
      "  questioning: 1484 images\n",
      "  sadness: 1221 images\n",
      "  surprise: 4491 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 2A. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 3. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# Stronger augmentation for minority classes\n",
    "minority_aug = T.Compose([\n",
    "    T.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.8),\n",
    "    T.RandomRotation(30),\n",
    "    T.ColorJitter(0.4, 0.4, 0.4, 0.2),\n",
    "    T.RandomGrayscale(p=0.3),\n",
    "    T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "])\n",
    "\n",
    "#factory function that returns another function -> tranform_function\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function\n",
    "\n",
    "#returned transform_function is applied to each dataset example inside .map()\n",
    "#each time the dataset runs transform_function(example), it receives:\n",
    "#inputs = {\n",
    "#     'pixel_values': tensor,\n",
    "#     'labels': label_int\n",
    "# }\n",
    "dataset = dataset.map(make_transform_function(processor, minority_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "# # in case above code doesn't run:\n",
    "# split_dataset = dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "# train_dataset = split_dataset[\"train\"]\n",
    "# eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({3: 7110, 6: 4850, 0: 3761, 2: 3680, 5: 3594, 7: 1171, 4: 985, 1: 481})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:18<00:00, 80.55 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:15<00:00, 81.20 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:15<00:00, 81.12 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:17<00:00, 80.75 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:14<00:00, 81.51 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:13<00:00, 81.83 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:15<00:00, 81.17 examples/s] \n",
      "Filter (num_proc=8): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25632/25632 [05:15<00:00, 81.32 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({1: 4500, 6: 4500, 3: 4500, 5: 4500, 7: 4500, 0: 4500, 4: 4500, 2: 4500})\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Balance Dataset\n",
    "# --------------------------\n",
    "label_target = 4500\n",
    "balanced_subsets = []\n",
    "\n",
    "# Dynamically calculate label counts\n",
    "label_counts = Counter(train_dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = train_dataset.filter(lambda x: x['label'] == label, num_proc=8)\n",
    "    if count > label_target:\n",
    "        subset = subset.select(random.sample(range(len(subset)), label_target))\n",
    "    elif count < label_target:\n",
    "        multiplier = label_target // len(subset)\n",
    "        remainder = label_target % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "    balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a5ab27-d5e2-4340-8450-23693f34b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # 6 Oversample Underrepresented Classes\n",
    "# # --------------------------\n",
    "# def oversample_dataset(dataset):\n",
    "#     label_counts = Counter(dataset['label'])\n",
    "#     max_count = max(label_counts.values())\n",
    "#     label_datasets = []\n",
    "\n",
    "#     for label in sorted(label_counts):\n",
    "#         subset = dataset.filter(lambda x: x['label'] == label, num_proc=8)\n",
    "#         multiplier = max_count // len(subset)\n",
    "#         remainder = max_count % len(subset)\n",
    "#         oversampled = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "#         label_datasets.append(oversampled)\n",
    "\n",
    "#     return concatenate_datasets(label_datasets).shuffle(seed=42)\n",
    "\n",
    "# train_dataset = oversample_dataset(train_dataset)\n",
    "# print(\"After oversampling:\", Counter(train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 7. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # ‚úÖ (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # ‚úÖ Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # ‚úÖ Log once per epoch\n",
    "    save_safetensors=True                  # ‚úÖ Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 8. Define Compute and Confusion Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Define a compute_metrics function w/ confusion matrix logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw values for further use if needed\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)    \n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify top 3 confused class pairs (excluding diagonal)\n",
    "    confusion_pairs = []\n",
    "    for i in range(len(LABEL_NAMES)):\n",
    "        for j in range(len(LABEL_NAMES)):\n",
    "            if i != j:\n",
    "                confusion_pairs.append(((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j]))\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22500' max='22500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22500/22500 7:04:29, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>0.330817</td>\n",
       "      <td>0.917304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.375850</td>\n",
       "      <td>0.925417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.458280</td>\n",
       "      <td>0.921829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.419278</td>\n",
       "      <td>0.934935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.424627</td>\n",
       "      <td>0.939148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.85      0.93      0.89       941\n",
      "     disgust       0.80      0.85      0.82       126\n",
      "        fear       0.90      0.86      0.88       991\n",
      "   happiness       0.97      0.94      0.96      1759\n",
      "     sadness       0.94      0.86      0.90       236\n",
      "    surprise       0.93      0.94      0.94       897\n",
      "     neutral       0.90      0.92      0.91      1146\n",
      " questioning       0.95      0.95      0.95       313\n",
      "\n",
      "    accuracy                           0.92      6409\n",
      "   macro avg       0.91      0.91      0.90      6409\n",
      "weighted avg       0.92      0.92      0.92      6409\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.91      0.90      0.91       941\n",
      "     disgust       0.88      0.78      0.82       126\n",
      "        fear       0.93      0.87      0.90       991\n",
      "   happiness       0.97      0.94      0.96      1759\n",
      "     sadness       0.93      0.91      0.92       236\n",
      "    surprise       0.90      0.97      0.93       897\n",
      "     neutral       0.89      0.95      0.92      1146\n",
      " questioning       0.93      0.93      0.93       313\n",
      "\n",
      "    accuracy                           0.93      6409\n",
      "   macro avg       0.92      0.91      0.91      6409\n",
      "weighted avg       0.93      0.93      0.93      6409\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.85      0.94      0.89       941\n",
      "     disgust       0.77      0.85      0.81       126\n",
      "        fear       0.90      0.88      0.89       991\n",
      "   happiness       0.98      0.96      0.97      1759\n",
      "     sadness       0.95      0.83      0.88       236\n",
      "    surprise       0.95      0.92      0.93       897\n",
      "     neutral       0.93      0.90      0.91      1146\n",
      " questioning       0.88      0.96      0.92       313\n",
      "\n",
      "    accuracy                           0.92      6409\n",
      "   macro avg       0.90      0.91      0.90      6409\n",
      "weighted avg       0.92      0.92      0.92      6409\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.91      0.92       941\n",
      "     disgust       0.88      0.80      0.84       126\n",
      "        fear       0.90      0.90      0.90       991\n",
      "   happiness       0.97      0.96      0.97      1759\n",
      "     sadness       0.93      0.91      0.92       236\n",
      "    surprise       0.95      0.95      0.95       897\n",
      "     neutral       0.92      0.94      0.93      1146\n",
      " questioning       0.93      0.96      0.94       313\n",
      "\n",
      "    accuracy                           0.93      6409\n",
      "   macro avg       0.93      0.92      0.92      6409\n",
      "weighted avg       0.94      0.93      0.93      6409\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.95      0.90      0.92       941\n",
      "     disgust       0.92      0.75      0.83       126\n",
      "        fear       0.91      0.91      0.91       991\n",
      "   happiness       0.97      0.97      0.97      1759\n",
      "     sadness       0.91      0.93      0.92       236\n",
      "    surprise       0.93      0.96      0.95       897\n",
      "     neutral       0.92      0.96      0.94      1146\n",
      " questioning       0.94      0.95      0.95       313\n",
      "\n",
      "    accuracy                           0.94      6409\n",
      "   macro avg       0.93      0.92      0.92      6409\n",
      "weighted avg       0.94      0.94      0.94      6409\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22500, training_loss=0.07624199735853407, metrics={'train_runtime': 25472.2209, 'train_samples_per_second': 7.067, 'train_steps_per_second': 0.883, 'total_flos': 1.394930822971392e+19, 'train_loss': 0.07624199735853407, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Trainer with Class-Weighted Loss\n",
    "# --------------------------\n",
    "\n",
    "# Compute class weights from training set\n",
    "label_freqs = Counter(train_dataset['label'])\n",
    "total = sum(label_freqs.values())\n",
    "class_weights = torch.tensor([total / label_freqs[i] for i in range(len(label_freqs))], dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# trainer initialization\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # 10. Save Final Independent Model  - STUCK!\n",
    "# # --------------------------\n",
    "# model = model.to(\"cpu\")   #move to CPU first to avoid GPU I/O freeze\n",
    "\n",
    "# torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'final_model.pth'))\n",
    "# model.save_pretrained(SAVE_DIR)\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(f\"‚úÖ Model and processor saved to {SAVE_DIR}\")\n",
    "\n",
    "# # Save full Trainer backup (optimizer state, scheduler, RNGs etc.)\n",
    "# trainer.save_model(os.path.join(SAVE_DIR, \"backup_trainer_model\"))\n",
    "# print(f\"‚úÖ Trainer backup saved to {os.path.join(SAVE_DIR, 'backup_trainer_model')}\")\n",
    "\n",
    "# # --------------------------\n",
    "# # 10B. Fast Recovery Save (Optional Safety Net)\n",
    "# # --------------------------\n",
    "# from transformers import AutoModelForImageClassification\n",
    "\n",
    "# # Locate the last checkpoint inside SAVE_DIR\n",
    "# latest_checkpoint = max(\n",
    "#     (os.path.join(SAVE_DIR, d) for d in os.listdir(SAVE_DIR) if d.startswith(\"checkpoint-\")),\n",
    "#     key=os.path.getmtime,\n",
    "#     default=None\n",
    "# )\n",
    "\n",
    "# if latest_checkpoint:\n",
    "#     print(f\"‚úÖ Found latest checkpoint at: {latest_checkpoint}\")\n",
    "#     recovered_model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "#     recovered_model.save_pretrained(SAVE_DIR)  # Save cleanly\n",
    "#     processor.save_pretrained(SAVE_DIR)        # Save processor again just to be safe\n",
    "#     print(f\"‚úÖ Re-saved model and processor from checkpoint to: {SAVE_DIR}\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No checkpoint found for recovery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12eaaf8a-eb3d-43b5-a99c-b513d20621c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8915/2482864176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move to CPU first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Save state dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Save Final Independent Model (Safe Save Mode)\n",
    "# --------------------------\n",
    "\n",
    "model = model.to(\"cpu\")  # move to CPU first\n",
    "\n",
    "# Save processor\n",
    "processor.save_pretrained(SAVE_DIR)\n",
    "print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "\n",
    "# Save state dict\n",
    "final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "# Save full model\n",
    "model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "# Save trainer state (if defined)\n",
    "if 'trainer' in globals():\n",
    "    trainer.save_model(os.path.join(SAVE_DIR, \"backup_trainer_model\"))\n",
    "    print(\"‚úÖ Trainer backup saved.\")\n",
    "    \n",
    "# Free memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Memory cleanup complete after save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 11. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 501/501 [04:04<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "Distribution plot saved to: /home/ubuntu/MLexpressionsStorage/V8_20250503_190001/V8_distribution_plot_20250503_190001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 12. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 13. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Simple calibration-only function for analysis and storage\n",
    "def apply_temperature_scaling(logits_path=f\"{VERSION}_logits_eval.npy\", labels_path=f\"{VERSION}_labels_eval.npy\"):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(\"‚ùå Logits or labels not found. Skipping temperature scaling.\")\n",
    "        return None\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().numpy()\n",
    "    logloss = log_loss(labels.numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(torch.tensor([temperature_value]), os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\"))\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "\n",
    "    return temperature_value\n",
    "\n",
    "# Reliability diagram\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\"))\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {VERSION}_reliability_diagram_calibrated.png\")\n",
    "\n",
    "# üî• Run hybrid calibration only if logits exist\n",
    "if os.path.exists(f\"{VERSION}_logits_eval.npy\") and os.path.exists(f\"{VERSION}_labels_eval.npy\"):\n",
    "    temp_value = apply_temperature_scaling()\n",
    "    if temp_value is not None:\n",
    "        logits = torch.tensor(np.load(f\"{VERSION}_logits_eval.npy\"), dtype=torch.float32)\n",
    "        labels = torch.tensor(np.load(f\"{VERSION}_labels_eval.npy\"), dtype=torch.long)\n",
    "        plot_reliability_diagram(logits, labels, torch.tensor(temp_value))\n",
    "        # To use later: wrap model if needed\n",
    "        # temp_model = ModelWithTemperature(model)\n",
    "        # temp_model.temperature.data = torch.tensor([temp_value])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2463842-900f-4d94-ba12-5717928fa8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
