{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  V39  -  Stability and diagnostics run.\n",
    "#  Summary: Implemented memory management to prevent crashes. Overhauled hard-negative\n",
    "#           mining and inference logging to produce accurate curation data.\n",
    "# ==============================================================================\n",
    "\n",
    "# V38 to V39 changes:\n",
    "    # overview: A stability and diagnostics run. Implemented memory management fixes to\n",
    "    #           prevent OS crashes during training. Overhauled the hard-negative mining\n",
    "    #           logic and inference logging to produce accurate data for the next\n",
    "    #           curation cycle.\n",
    "\n",
    "    # section #5 (TrainingArguments - Stability):\n",
    "    #   - Reduced `per_device_train_batch_size` for both Stage 1 (16‚Üí8) and Stage 2 (8‚Üí4).\n",
    "    #   - Added `gradient_accumulation_steps` to compensate for the smaller batch sizes\n",
    "    #     and maintain effective batch throughput.\n",
    "    #   - Rationale: Prevents excessive memory consumption that was causing the\n",
    "    #     operating system (macOS jetsam) to terminate the script during long training runs.\n",
    "\n",
    "    # section #7 (Hierarchical Inference):\n",
    "    #   - Updated the `hierarchical_predict` function to log the model's raw guess as\n",
    "    #     `top1_label` in addition to the final `prediction` (which is post-thresholding).\n",
    "    #   - Rationale: The previous version overwrote predictions that fell below the\n",
    "    #     confidence threshold, making it impossible to find all misclassifications. This\n",
    "    #     change ensures the `full_inference_log.csv` contains the complete data needed\n",
    "    #     for accurate mining.\n",
    "\n",
    "    # section #8 (Post-Training ‚Äî Mining):\n",
    "    #   - Overhauled the hard-negative mining logic.\n",
    "    #   - The script now pre-filters the inference log to only mine for confusions\n",
    "    #     among truly `RELEVANT_CLASSES`, preventing images that slipped past the S1\n",
    "    #     filter from contaminating the results.\n",
    "    #   - The mining logic now correctly uses the new `top1_label` column for analysis.\n",
    "    #   - Updated the `confusion_pairs_to_mine` list to explicitly target the\n",
    "    #     `neutral_speech` vs. `speech_action` and `sadness` vs. `speech_action` confusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "# Alias json to prevent scope conflicts with local variables.\n",
    "import json as json_mod\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageStat, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamically loading latest checkpoint: V38_20251021_123355\n",
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- üìÇ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- ‚öôÔ∏è Run Configuration ---\n",
    "# default safer for daily dev runs; flip to True when you want full-corpus inference\n",
    "RUN_INFERENCE = True\n",
    "# default safer; run once when dataset layout changes\n",
    "PREPARE_DATASETS = False\n",
    "\n",
    "# Curation/Artifacts policy\n",
    "USE_EXTERNAL_CURATIONS = True\n",
    "CURATION_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V34_20251013_211825\"\n",
    "EXTERNAL_PATCH = os.path.join(CURATION_DIR, \"patch_V35.csv\")  # produced by the curation nb\n",
    "\n",
    "# --- Smoke test mode (no training) ---\n",
    "# Set to True to load the latest model and test a few images.\n",
    "# Set to False to run the full training pipeline.\n",
    "SMOKE_TEST_ONLY = True\n",
    "\n",
    "# Will be dynamically found if left as None.\n",
    "# Or, you can hardcode a path like \"/path/to/V35_...\" to test a specific version.\n",
    "SMOKE_CHECKPOINT_PATH = None\n",
    "\n",
    "# Finds the most recent V* model directory based on modification time.\n",
    "# VERSION_TAG is the folder name the script just created for this run, e.g. \"V35_20251014_161418\"\n",
    "# Ensure VERSION_TAG is defined where you compose SAVE_DIR / OUTPUT_ROOT_DIR.\n",
    "# Example: VERSION_TAG = os.path.basename(SAVE_DIR)\n",
    "def find_latest_checkpoint(root_dir, current_run_basename=None):\n",
    "    \"\"\"\n",
    "    Return the path to the most recent *completed* run by semantic version + timestamp,\n",
    "    excluding the current run directory. Ignores folders that don't contain model artifacts.\n",
    "    Folder name pattern: V<version>_<YYYYMMDD>_<HHMMSS>  (e.g., V34_20251013_211825)\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    pat = re.compile(r\"^V(\\d+)_(\\d{8}_\\d{6})$\")  # V<num>_YYYYMMDD_HHMMSS\n",
    "\n",
    "    for d in os.listdir(root_dir):\n",
    "        full = os.path.join(root_dir, d)\n",
    "        if not (os.path.isdir(full) and d.startswith(\"V\")):\n",
    "            continue\n",
    "        if current_run_basename and d == current_run_basename:\n",
    "            continue\n",
    "\n",
    "        m = pat.match(d)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        ver = int(m.group(1))\n",
    "        ts  = m.group(2)  # sortable string\n",
    "\n",
    "        # Treat as \"completed\" only if it contains known artifacts\n",
    "        has_model = any(\n",
    "            os.path.isdir(os.path.join(full, p))\n",
    "            for p in (\n",
    "                \"emotion_classifier_model\",\n",
    "                \"relevance_filter_model\",\n",
    "                \"stage_2_emotion_model_training\",\n",
    "            )\n",
    "        )\n",
    "        if not has_model:\n",
    "            continue\n",
    "\n",
    "        candidates.append((ver, ts, full))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Sort by (version, timestamp) descending: highest V, then latest time\n",
    "    candidates.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "# --- ü§ñ Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# --- üè∑Ô∏è Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Weakest-label targeting \n",
    "WEAKEST_LABEL = \"sadness\"   # <‚Äî change ONLY if a different label is sub-0.80\n",
    "WEAK_BOOST   = 1.8          # LOWERED to 1.8 to lessen oversampling effect\n",
    "SKIP_ERASE_WEAK = True      # leave True to protect fine cues; set False if you want occlusion\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# single source of truth for review gating\n",
    "REVIEW_CONF_THRESHOLD = 0.85  \n",
    "\n",
    "# --- üñºÔ∏è File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- üî¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Dynamically find the latest checkpoint to train from\n",
    "# Resolve checkpoint path (AFTER you define VERSION_TAG and OUTPUT_ROOT_DIR)\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR, current_run_basename=VERSION_TAG)\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = latest_checkpoint\n",
    "    print(f\"‚úÖ Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(\"‚ö†Ô∏è No previous checkpoint found ‚Äî falling back to base model.\")\n",
    "\n",
    "\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"üóÇÔ∏è Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\n‚úÖ Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Smoke Test ---\n",
    "\n",
    "#  Normalizes model config maps so we always get:\n",
    "      # id2label: Dict[int, str]\n",
    "      # label2id: Dict[str, int]\n",
    "def _normalize_label_maps_from_config(cfg):\n",
    "    id2label = {int(k): str(v) for k, v in getattr(cfg, \"id2label\", {}).items()}\n",
    "    label2id = {str(k): int(v) for k, v in getattr(cfg, \"label2id\", {}).items()}\n",
    "    if not id2label and label2id:\n",
    "        id2label = {vi: k for k, vi in label2id.items()}\n",
    "    if not label2id and id2label:\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "    return id2label, label2id\n",
    "\n",
    "# Loads the processor, S1, and S2 models from a completed training run folder.\n",
    "def _load_exports_for_smoke(checkpoint_path: str, device: torch.device):\n",
    "    \n",
    "    s2_dir = os.path.join(checkpoint_path, \"emotion_classifier_model\")\n",
    "    s1_dir = os.path.join(checkpoint_path, \"relevance_filter_model\")\n",
    "\n",
    "    if not os.path.isdir(s1_dir) or not os.path.isdir(s2_dir):\n",
    "        raise FileNotFoundError(f\"Valid S1 or S2 model not found in: {checkpoint_path}\")\n",
    "\n",
    "    processor = AutoImageProcessor.from_pretrained(s2_dir)\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(s1_dir).to(device).eval()\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(s2_dir).to(device).eval()\n",
    "\n",
    "    # Make label maps globally available for the inference function\n",
    "    globals()[\"id2label_s2\"], globals()[\"label2id_s2\"] = _normalize_label_maps_from_config(model_s2.config)\n",
    "    globals()[\"id2label_s1\"], globals()[\"label2id_s1\"] = _normalize_label_maps_from_config(model_s1.config)\n",
    "\n",
    "    return model_s1, model_s2, processor\n",
    "    \n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# üì¶ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self,\n",
    "                 processor,\n",
    "                 augment_dict=None,\n",
    "                 base_augment=None,\n",
    "                 # --- : tensor-level erasing controls (applied after processor) ---\n",
    "                 random_erasing_prob: float = 0.10,\n",
    "                 random_erasing_scale = (0.02, 0.08),\n",
    "                 skip_erasing_label_ids=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processor: HF image processor that yields pixel_value tensors\n",
    "            augment_dict: dict[int label_id -> PIL transform], class-specific\n",
    "            base_augment: fallback PIL transform when class-specific not found\n",
    "            random_erasing_prob: probability for applying tensor-level RandomErasing\n",
    "            random_erasing_scale: area range for erasing region\n",
    "            skip_erasing_label_ids: iterable of label ids to skip erasing for\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict or {}\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = base_augment or T.Compose([T.Resize((224, 224))])\n",
    "\n",
    "        # --- : tensor-level RandomErasing (applied AFTER processor) ---\n",
    "        # Keep None to disable; expects CHW tensors in [0,1]\n",
    "        self.random_erasing = (\n",
    "            T.RandomErasing(p=random_erasing_prob, scale=random_erasing_scale, value=\"random\")\n",
    "            if random_erasing_prob and random_erasing_prob > 0.0 else None\n",
    "        )\n",
    "                \n",
    "        # --- : define tensor <-> PIL helpers used in __call__ ---\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.to_pil = T.ToPILImage()\n",
    "        \n",
    "        # Labels to skip erasing for (can be overridden when constructing the collator)\n",
    "        self.skip_erasing_label_ids = set(skip_erasing_label_ids or [])\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "\n",
    "            # 1) apply class-specific PIL pipeline if present; else base PIL pipeline\n",
    "            pil_aug = self.augment_dict.get(label, self.base_augment)\n",
    "\n",
    "            img = pil_aug(rgb_image)\n",
    "\n",
    "            # ‚¨áÔ∏è INSERT THE  LINES HERE\n",
    "            # --- Tensor-level RandomErasing ---\n",
    "            img_t = self.to_tensor(img)                 # PIL ‚Üí Tensor [C,H,W]\n",
    "            if self.random_erasing is not None and label not in self.skip_erasing_label_ids:\n",
    "                img_t = self.random_erasing(img_t)      # RandomErasing on tensor\n",
    "            img = self.to_pil(img_t)  \n",
    "        \n",
    "            processed_images.append(img)\n",
    "\n",
    "        batch = self.processor(images=processed_images, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- normalize any image-like object to 3-channel RGB (PIL) ---\n",
    "def _ensure_rgb(img):\n",
    "    # If already PIL, force RGB mode\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert(\"RGB\")\n",
    "    # Else coerce to array and expand grayscale to 3 channels\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr, arr, arr], axis=-1)\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# üèãÔ∏è Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# üîÑ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None, focal_gamma=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.focal_gamma = focal_gamma  #  (None disables focal scaling)\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        ce_per_sample = -(smooth_labels * log_probs).sum(dim=1)\n",
    "\n",
    "        # : optional focal scaling\n",
    "        if self.focal_gamma is not None and self.focal_gamma > 0:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pt = (probs * smooth_labels).sum(dim=1).clamp_min(1e-6)\n",
    "            ce_per_sample = ((1 - pt) ** self.focal_gamma) * ce_per_sample\n",
    "\n",
    "        return ce_per_sample.mean()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1 loss function: focal-modulated cross-entropy (relevant-only)\n",
    "#   - We keep class weights for imbalance handling.\n",
    "#   - We add focal modulation ONLY when the ground truth is \"relevant\"\n",
    "#     to emphasize difficult positives without exploding FP on easy negatives.\n",
    "# ------------------------------------------------------------------------------\n",
    "class RelevantFocalCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, class_weights: torch.Tensor, gamma: float = 2.0, relevant_id: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            class_weights: Tensor of per-class weights (size 2 for S1)\n",
    "            gamma: focal exponent (higher -> more emphasis on hard examples)\n",
    "            relevant_id: integer id for the 'relevant' class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=class_weights, reduction=\"none\")\n",
    "        self.gamma = gamma\n",
    "        self.relevant_id = relevant_id\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes cross-entropy per-sample, then applies focal scaling only\n",
    "        for samples whose target == 'relevant'. Non-relevant samples keep vanilla CE.\n",
    "        \"\"\"\n",
    "        # base cross-entropy (per-sample)\n",
    "        ce = self.ce(logits, targets)  # shape: [B]\n",
    "\n",
    "        # compute p_t = softmax(logits)[range(B), targets]\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            p_t = probs[torch.arange(probs.size(0)), targets]  # [B]\n",
    "\n",
    "        # mask: 1 for relevant targets, 0 otherwise\n",
    "        mask = (targets == self.relevant_id).float()\n",
    "\n",
    "        # focal factor: (1 - p_t)^gamma for relevant samples; 1.0 for others\n",
    "        focal = (1.0 - p_t).pow(self.gamma) * mask + (1.0 - mask)\n",
    "\n",
    "        # mean reduced loss\n",
    "        return (focal * ce).mean()\n",
    "\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# üìä Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(\n",
    "    eval_pred,\n",
    "    label_names,\n",
    "    stage_name=\"Stage2\",\n",
    "    s2_temperature: float = 1.0,\n",
    "):\n",
    "    logits, labels = eval_pred  # logits: np.ndarray, labels: np.ndarray\n",
    "\n",
    "    # ---- Stage-2 temperature (calibrated probabilities) ----\n",
    "    if stage_name.lower().startswith(\"stage2\") and (s2_temperature is not None) and (s2_temperature != 1.0):\n",
    "        logits = logits / max(1e-6, float(s2_temperature))\n",
    "\n",
    "    # softmax ‚Üí probs, preds\n",
    "    probs = torch.softmax(torch.from_numpy(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "\n",
    "    print(f\"\\nüìà Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw eval tensors for post-hoc analysis\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Per-class metrics for CSV\n",
    "    f1s        = [report[name][\"f1-score\"]   for name in label_names]\n",
    "    recalls    = [report[name][\"recall\"]     for name in label_names]\n",
    "    precisions = [report[name][\"precision\"]  for name in label_names]\n",
    "\n",
    "    # Entropy per class\n",
    "    softmax_probs = F.softmax(torch.from_numpy(logits), dim=-1)\n",
    "    entropies     = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    labels_np = np.asarray(labels)\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (labels_np == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging (append)\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names},\n",
    "    })\n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # Confusion matrix figure\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [((label_names[i], label_names[j]), cm[i][j])\n",
    "                       for i in range(len(label_names)) for j in range(len(label_names))\n",
    "                       if i != j and cm[i][j] > 0]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nüîç Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    return {\"accuracy\": float((preds == labels).mean())}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "#   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "#   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "#   - Persist T and œÑ for hierarchical inference.\n",
    "# ------------------------------------------------------------------------------\n",
    "def fit_temperature(model, eval_ds, processor, device):\n",
    "    \"\"\"\n",
    "    Fits a single temperature scalar T by minimizing NLL on eval set.\n",
    "    Returns:\n",
    "        float: learned temperature T (>= ~1e-3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        #Normalize every eval image to 3-channel RGB in fit_temperature\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "        \n",
    "            # --- Ensure 3-channel RGB for the processor ---\n",
    "            # If PIL: convert directly; if numpy/other: coerce to array and expand gray to 3-channels\n",
    "            if isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "            else:\n",
    "                arr = np.array(img)\n",
    "                if arr.ndim == 2:                      # grayscale -> stack to RGB\n",
    "                    arr = np.stack([arr, arr, arr], axis=-1)\n",
    "                img = Image.fromarray(arr.astype(np.uint8))  # ensure PIL RGB\n",
    "        \n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            logits_list.append(logits.cpu())\n",
    "            labels_list.append(lab)\n",
    "\n",
    "    logits = torch.cat(logits_list, dim=0)  # [N, 2]\n",
    "    labels = torch.tensor(labels_list)\n",
    "\n",
    "    T = torch.nn.Parameter(torch.ones(1))\n",
    "    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def _closure():\n",
    "        \"\"\"\n",
    "        LBFGS closure for temperature scaling:\n",
    "        Scales logits by 1/T, computes CE loss, backprops to adjust T.\n",
    "        \"\"\"\n",
    "        opt.zero_grad()\n",
    "        scaled = logits / T.clamp(min=1e-3)\n",
    "        loss = ce(scaled, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    opt.step(_closure)\n",
    "    return float(T.data.item())\n",
    "\n",
    "def sweep_tau(model, eval_ds, processor, device, T=1.0):\n",
    "    \"\"\"\n",
    "    Sweep œÑ on P(relevant) over [0.28, 0.55] (0.01 steps) to maximize F1(relevant).\n",
    "    Keeps œÑ near the historically stable 0.30, but allows a slight reduction when it\n",
    "    meaningfully lifts F1 on eval.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "            img = _ensure_rgb(img)  # your helper\n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits / max(T, 1e-3)\n",
    "            prob_rel = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(prob_rel)\n",
    "\n",
    "    y_true = np.asarray(y_true, dtype=bool)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "\n",
    "    pos_rate = float(y_true.mean()) if len(y_true) > 0 else 0.0\n",
    "    print(f\"‚ÑπÔ∏è S1 calib eval prevalence (relevant rate): {pos_rate:.3f}\")\n",
    "\n",
    "    best = {\"tau\": None, \"f1\": -1.0, \"prec\": None, \"rec\": None}\n",
    "    for tau in np.round(np.arange(0.28, 0.55 + 1e-9, 0.01), 2):\n",
    "        pred = (y_prob >= tau)\n",
    "        tp = ((pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((pred == 1) & (y_true == 0)).sum()\n",
    "        fn = ((pred == 0) & (y_true == 1)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"tau\": float(tau),\n",
    "                    \"f1\": round(float(f1), 3),\n",
    "                    \"prec\": round(float(prec), 3),\n",
    "                    \"rec\": round(float(rec), 3)}\n",
    "    return best\n",
    "    \n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# Minimal file router for ad-hoc runs from the training script.\n",
    "def _route_image_to_fs(src_path: str, out_dir: Path, review_dir: Path, decision: dict):\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import shutil, os\n",
    "\n",
    "    if decision.get(\"route_reason\") == \"thresholds\":\n",
    "        dest = review_dir / \"review_lowconf\"\n",
    "    elif decision.get(\"final_label\") == \"irrelevant\":\n",
    "        dest = review_dir / \"irrelevant_or_lowS1\"\n",
    "    else:\n",
    "        dest = out_dir / decision[\"final_label\"]\n",
    "\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    shutil.copy2(src_path, dest / Path(src_path).name)\n",
    "\n",
    "\n",
    "# üíæ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"üíæ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"‚úÖ {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "# ==========================================================================\n",
    "#   POST-TRAINING ANALYSIS UTILITIES (OFFLINE / OPTIONAL)\n",
    "#   - Qualitative error bucketing (QE)\n",
    "#   - Attention rollout (XAI) for S1 inspection\n",
    "#   - Ablation helpers\n",
    "# ==========================================================================\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"‚ö†Ô∏è Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - ‚ùå {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - ‚úÖ {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")\n",
    "\n",
    "# --- Qualitative Error Bucketing (Stage 1) ---\n",
    "# Scans an inference CSV and tags each row with simple visual heuristics:\n",
    "# blur/shadow/occlusion/low-res. Outputs a QE report CSV for targeting data fixes.\n",
    "def variance_of_laplacian(gray):\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def is_dark(img_pil, thresh=40):\n",
    "    stat = ImageStat.Stat(img_pil.convert(\"L\"))\n",
    "    return stat.mean[0] < thresh\n",
    "\n",
    "def qualitative_buckets_s1(inference_csv, out_csv):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(inference_csv)\n",
    "    # consider only S1 mistakes if you logged them; otherwise filter low conf or S2 mismatches\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        path = r['filepath']\n",
    "        if not os.path.exists(path): continue\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        arr = np.array(img)\n",
    "        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        blur = variance_of_laplacian(gray) < 60         # motion blur proxy\n",
    "        dark = is_dark(img, thresh=45)                  # shadows proxy\n",
    "        lowres = min(img.size) < 80\n",
    "        # Cheap occlusion proxy: large random erasing candidate on face area would help, but without faces we use entropy\n",
    "        ent = cv2.calcHist([gray],[0],None,[256],[0,256]).flatten()\n",
    "        ent = -np.sum((ent/ent.sum()+1e-9)*np.log2(ent/ent.sum()+1e-9))\n",
    "        occl = ent < 4.5                                 # low entropy proxy\n",
    "        rows.append([path, r.get('true_label','?'), r.get('predicted_label','?'), r.get('confidence',np.nan),\n",
    "                     int(blur), int(dark), int(occl), int(lowres)])\n",
    "    with open(out_csv, \"w\", line=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"filepath\",\"true\",\"pred\",\"conf\",\"blur\",\"shadow\",\"occlusion\",\"lowres\"])\n",
    "        w.writerows(rows)\n",
    "    return out_csv\n",
    "\n",
    "# --- Ablation summary utility for Stage 1 ---\n",
    "# Summarizes precision/recall/F1 for S1 given (T, tau).\n",
    "def summarize_s1(eval_ds, model, processor, device, T: float, tau: float):\n",
    "    import numpy as np\n",
    "    y_true, y_prob = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "    \n",
    "            # Normalize to 3-channel RGB to avoid ndim==2 errors\n",
    "            img = _ensure_rgb(img)\n",
    "    \n",
    "            logits = model(**processor(images=img, return_tensors=\"pt\").to(device)).logits\n",
    "            logits = logits / max(T, 1e-3)\n",
    "            p = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(p)\n",
    "\n",
    "    y_true = np.array(y_true, bool); y_prob = np.array(y_prob, float)\n",
    "    pred = (y_prob >= tau)\n",
    "    tp = ((pred==1)&(y_true==1)).sum(); fp=((pred==1)&(y_true==0)).sum(); fn=((pred==0)&(y_true==1)).sum()\n",
    "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
    "    return {\"precision\":round(prec,3), \"recall\":round(rec,3), \"f1\":round(f1,3), \"tau\":tau, \"T\":T}\n",
    "\n",
    "\n",
    "# --- Attention Rollout heatmaps for ViT (offline) ---\n",
    "def vit_attention_rollout(model, inputs, discard_ratio=0.9):\n",
    "    # returns a [H,W] mask normalized 0..1; you can overlay it\n",
    "    # (Implementation omitted for brevity; use a standard attention-rollout snippet for ViT)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # --- Define specific model paths from the latest checkpoint ---\n",
    "    s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "    s2_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"emotion_classifier_model\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"‚úÖ Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        s1_checkpoint_path, # <-- Use the specific path for the Stage 1 model\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"‚öñÔ∏è Stage 1 Class Weights: {class_weights_s1}\")\n",
    "\n",
    "    # --- Define Early Stopping ---\n",
    "    # Stops training if validation loss doesn't improve for 2 consecutive epochs\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=8,      # Halved from 16 to reduce memory\n",
    "        per_device_eval_batch_size=8,       # Also reduce eval batch size\n",
    "        gradient_accumulation_steps=2,      # Compensate for the smaller batch size\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    # The complex discriminative learning rate and layer freezing strategy in \n",
    "        # V31 caused a severe performance drop. This change reverts Stage 1 to \n",
    "        # V30's simpler and more effective approach of using a single, uniform \n",
    "        # learning rate for the entire model, which is managed by the Hugging \n",
    "        # Face Trainer's default optimizer.\n",
    "    training_args_s1.learning_rate = 3e-5 # Set learning rate directly\n",
    "    \n",
    "    loss_fct_s1 = RelevantFocalCrossEntropy(\n",
    "        class_weights=class_weights_s1,   # <-- we KEEP and USE class_weights here\n",
    "        gamma=2.0,\n",
    "        relevant_id=label2id_s1['relevant']\n",
    "    )\n",
    "\n",
    "    strong_pos_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "        T.RandomPerspective(distortion_scale=0.05, p=0.2),\n",
    "        # NEW: small pose/tilt tolerance to reduce false \"irrelevant\" on near-frontal faces\n",
    "        T.RandomAffine(degrees=6, translate=(0.03, 0.03), scale=(0.97, 1.03)),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Map label id -> transform. 1 == relevant\n",
    "    augment_map_s1 = { label2id_s1['relevant']: strong_pos_aug }\n",
    "\n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    # Apply stronger augmentation ONLY to the \"relevant\" class to expand coverage\n",
    "        # near the decision boundary (lighting, small occlusions, slight perspective).\n",
    "        # Keep \"irrelevant\" mild as before to avoid over-creating near-face artifacts.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=list(id2label_s1.values()), \n",
    "                                stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(\n",
    "            processor=processor,\n",
    "            augment_dict=augment_map_s1,                # your existing class‚ÜíPIL map\n",
    "            random_erasing_prob=0.10,                   # enable erasing\n",
    "            random_erasing_scale=(0.02, 0.08),\n",
    "            skip_erasing_label_ids=[]                   # or [label2id_s1['relevant']] to skip\n",
    "        ),\n",
    "        loss_fct=loss_fct_s1,         # <-- : custom loss uses class_weights + focal on relevant\n",
    "        callbacks=[early_stop_callback]\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"üöÄ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"‚åõ Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\n‚úÖ Stage 1 Training Complete.\")\n",
    "    \n",
    "    # Ensure the S1 return uses the trained model instance\n",
    "    model_s1 = trainer_s1.model\n",
    " \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "    #   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "    #   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "    #   - Persist T and œÑ for hierarchical inference.\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\nüß™ Calibrating Stage 1...\")\n",
    "    T_s1 = fit_temperature(trainer_s1.model, eval_dataset_s1, processor, device)\n",
    "    best_s1 = sweep_tau(trainer_s1.model, eval_dataset_s1, processor, device, T=T_s1)\n",
    "    print(f\"‚úÖ S1 calibration done: T={T_s1:.3f} | best œÑ={best_s1['tau']} | F1={best_s1['f1']} (P={best_s1['prec']}, R={best_s1['rec']})\")\n",
    "\n",
    "    # ---- Fail-fast sanity for S1 calibration ----\n",
    "    if not isinstance(best_s1, dict) or \"tau\" not in best_s1:\n",
    "        raise RuntimeError(\"S1 calibration failed: best_s1 missing 'tau' key.\")\n",
    "    if not (0.0 <= float(best_s1[\"tau\"]) <= 1.0):\n",
    "        raise RuntimeError(f\"S1 calibration produced invalid tau: {best_s1['tau']}\")\n",
    "    \n",
    "    if not isinstance(T_s1, (float, int)) or not (0.1 <= float(T_s1) <= 100.0):\n",
    "        raise RuntimeError(f\"S1 temperature T looks suspicious: {T_s1}\")\n",
    "\n",
    "    \n",
    "    # Persist calibration for inference\n",
    "    calib_out = os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "    with open(calib_out, \"w\") as f:\n",
    "        json_mod.dump({\"T\": float(T_s1), \"tau\": float(best_s1[\"tau\"])}, f)\n",
    "    print(f\"‚úÖ Wrote S1 calibration to {calib_out}\")\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(train_dataset_s2['label']))\n",
    "\n",
    "    # --- Optional: inject curated patch into TRAIN ONLY (no eval leak) ---\n",
    "    if USE_EXTERNAL_CURATIONS and os.path.exists(EXTERNAL_PATCH):\n",
    "        from datasets import Dataset, Features, ClassLabel, Image as DatasetsImage\n",
    "        \n",
    "        patch_df = pd.read_csv(EXTERNAL_PATCH)\n",
    "        patch_df = patch_df[patch_df[\"label\"].isin(RELEVANT_CLASSES)]\n",
    "    \n",
    "        # Define the features for the patch, REUSING the ClassLabel from the main dataset\n",
    "        patch_features = Features({\n",
    "            'image': DatasetsImage(),\n",
    "            'label': train_dataset_s2.features['label']  # This is the key fix\n",
    "        })\n",
    "    \n",
    "        def _open_img(p):\n",
    "            try:\n",
    "                return Image.open(p).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                return None\n",
    "                \n",
    "        patch_hf = Dataset.from_dict({\n",
    "            \"image\": [ _open_img(p) for p in patch_df[\"filepath\"] ],\n",
    "            \"label\": [ label2id_s2[l] for l in patch_df[\"label\"] ],\n",
    "        }, features=patch_features).filter(lambda ex: ex[\"image\"] is not None)\n",
    "    \n",
    "        # Concatenation will now succeed because the features match\n",
    "        train_dataset_s2 = concatenate_datasets([train_dataset_s2, patch_hf]).shuffle(seed=42)\n",
    "        print(f\"üìå Injected curated patch into TRAIN: +{len(patch_hf)} samples\")\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        s2_checkpoint_path, # <-- Use the specific path for the Stage 2 model\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=11),  \n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "    minority_classes_s2 = [label2id_s2[n] for n in ['disgust','questioning','contempt','fear']]\n",
    "    minority_augment_map_s2 = {lid: minority_aug for lid in minority_classes_s2}\n",
    "    \n",
    "    # very mild, targeted aug ONLY for the weakest classes\n",
    "    mild_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.95, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.05, 0.05, 0.05, 0.02),\n",
    "        T.RandomAffine(degrees=3, translate=(0.02, 0.02), scale=(0.98, 1.02)),\n",
    "    ])\n",
    "\n",
    "    # targeted mild augmentation for fragile classes\n",
    "    #     - Keep 'sadness' and 'speech_action' on very mild pipeline (no RandAug)\n",
    "    #     - Extend to 'neutral_speech' to preserve subtle mouth/phoneme cues\n",
    "    targeted_mild_classes = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['speech_action'],\n",
    "    ]\n",
    "    targeted_mild_map_s2 = {label_id: mild_aug for label_id in targeted_mild_classes}\n",
    "\n",
    "    # MERGE: single mapping passed to the collator (class id -> transform)\n",
    "    augment_dict = {**minority_augment_map_s2, **targeted_mild_map_s2}\n",
    "\n",
    "    # --- Section E: Tiny loss tweak for the weakest label (minimal & safe) ----\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,                      # keep global smoothing\n",
    "        target_class_names=[WEAKEST_LABEL],  # sharpen ONLY the weak class\n",
    "        label2id_map=label2id_s2,\n",
    "        focal_gamma=1.6                      # mild focal emphasis\n",
    "    )\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001  # tiny but non-zero improvement required\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # Adding weight decay, cosine scheduler + warmup, grad accumulation improves stability \n",
    "        # (especially on CPU/small batch) without altering your high-level flow.\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=4,      # Halved from 8 to reduce memory\n",
    "        per_device_eval_batch_size=4,       # Also reduce eval batch size\n",
    "        gradient_accumulation_steps=4,      # Increased from 2 to compensate\n",
    "        num_train_epochs=6,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        dataloader_num_workers=0,\n",
    "        overwrite_output_dir=True,\n",
    "        remove_unused_columns=False,\n",
    "        learning_rate=4e-5,\n",
    "        weight_decay=0.05,                         \n",
    "        lr_scheduler_type=\"cosine\",                \n",
    "        warmup_ratio=0.10,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # As with Stage 1, the complex fine-tuning strategy implemented in V31 failed. \n",
    "        # This change reverts the Stage 2 training process to V30's more effective \n",
    "        # uniform learning rate strategy to restore model performance.\n",
    "    training_args_s2.learning_rate = 4e-5 # Set learning rate directly\n",
    "\n",
    "    # skip erasing for fragile classes: sadness and neutral_speech\n",
    "    # NEW added speech_action\n",
    "    fragile_ids = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['speech_action'],\n",
    "        label2id_s2['neutral_speech']\n",
    "    ]\n",
    "\n",
    "    # ensure weakest label is included once (idempotent)\n",
    "    weak_id = label2id_s2[WEAKEST_LABEL]\n",
    "    if SKIP_ERASE_WEAK and weak_id not in fragile_ids:\n",
    "        fragile_ids.append(weak_id)\n",
    "    \n",
    "    # Single collator instance used by the trainer\n",
    "    data_collator = DataCollatorWithAugmentation(\n",
    "        processor=processor,\n",
    "        augment_dict=augment_dict,           # your merged S2 map\n",
    "        random_erasing_prob=0.10,\n",
    "        random_erasing_scale=(0.02, 0.08),\n",
    "        skip_erasing_label_ids=fragile_ids\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Hard guard: refuse to start S2 if S1 artifacts missing / not readable\n",
    "    required_s1 = [\n",
    "        os.path.join(SAVE_DIR, \"relevance_filter_model\"),\n",
    "        os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "    ]\n",
    "    for p in required_s1:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing required S1 artifact before S2: {p}\")\n",
    "    \n",
    "    # Optional: timing checkpoints to avoid silent 10h stalls\n",
    "    from time import perf_counter as _t\n",
    "    _t0_s2 = _t()\n",
    "\n",
    "    # --- Step A: Define the Sampler (using the final training data) ---\n",
    "    labels_np     = np.array(train_dataset_s2[\"label\"])\n",
    "    num_classes_s2 = len(label2id_s2)\n",
    "    class_counts  = np.bincount(labels_np, minlength=num_classes_s2)\n",
    "    class_weights = 1.0 / np.clip(class_counts, 1, None)\n",
    "    weak_id = label2id_s2[WEAKEST_LABEL]\n",
    "    class_weights[weak_id] *= WEAK_BOOST\n",
    "    sample_weights = class_weights[labels_np]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.as_tensor(sample_weights, dtype=torch.float),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    # --- Step B: Define the Trainer and Data Collator ---\n",
    "    # (data_collator was already defined earlier, this just creates the trainer)\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=RELEVANT_CLASSES, \n",
    "                                stage_name=\"Stage2\"),\n",
    "        loss_fct=loss_fct_s2,\n",
    "        callbacks=[early_stop_callback],\n",
    "    )\n",
    "\n",
    "    # --- Step C: Define and Override the Trainer's Data Loader ---\n",
    "    # This function is now defined LAST, so it can safely see both `sampler` and `trainer_s2`.\n",
    "    def _custom_train_loader():\n",
    "        return DataLoader(\n",
    "            train_dataset_s2,\n",
    "            batch_size=training_args_s2.per_device_train_batch_size,\n",
    "            sampler=sampler,                # Use the sampler defined in Step A\n",
    "            collate_fn=trainer_s2.data_collator, # Use the collator from the trainer in Step B\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    trainer_s2.get_train_dataloader = _custom_train_loader\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"üöÄ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time()\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()\n",
    "    \n",
    "    _t1_s2 = _t()\n",
    "    print(f\"‚åõ Stage 2 training took: {(_t1_s2 - _t0_s2)/3600:.2f} hours\")\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"‚åõ Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\n‚úÖ Stage 2 Training Complete.\")\n",
    "\n",
    "    # --- Calibrate Stage 2 (scalar temperature on eval) ---\n",
    "    print(\"\\nüß™ Calibrating Stage 2 (scalar T on eval set)...\")\n",
    "    \n",
    "    pred_eval = trainer_s2.predict(eval_dataset_s2)\n",
    "    # The calibration function expects numpy arrays on the CPU\n",
    "    logits_s2_numpy = pred_eval.predictions\n",
    "    labels_s2_numpy = pred_eval.label_ids\n",
    "    \n",
    "    # Call the existing function to find the optimal temperature\n",
    "    T_s2 = apply_temperature_scaling(logits_s2_numpy, labels_s2_numpy)\n",
    "    \n",
    "    s2_calib_path = os.path.join(SAVE_DIR, \"emotion_classifier_model\", \"stage2_calibration.json\")\n",
    "    os.makedirs(os.path.dirname(s2_calib_path), exist_ok=True)\n",
    "    with open(s2_calib_path, \"w\") as f:\n",
    "        # Use the aliased import 'json_mod' here as well\n",
    "        json_mod.dump({\n",
    "            \"T\": float(max(1e-6, T_s2)),\n",
    "            \"val_size\": int(labels_s2_numpy.size),\n",
    "            \"notes\": \"Scalar temperature via NLL on eval; seed=42\"\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ S2 calibration done: T={T_s2:.3f} ‚Üí {s2_calib_path}\")\n",
    "    \n",
    "    # (Optional) Re-run final eval with calibrated temperature for more accurate on-screen metrics\n",
    "    trainer_s2.compute_metrics = partial(\n",
    "        compute_metrics_with_confusion,\n",
    "        label_names=RELEVANT_CLASSES,\n",
    "        stage_name=\"Stage2_Calibrated\",\n",
    "        s2_temperature=float(T_s2),\n",
    "    )\n",
    "    print(\"\\nüìä Re-running evaluation with calibrated temperature:\")\n",
    "    _ = trainer_s2.evaluate(eval_dataset_s2)\n",
    "    \n",
    "    print(\"\\nüéâ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "    \n",
    "    return model_s1, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for  images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "\n",
    "    @dataclass\n",
    "    class _Thresh:\n",
    "        base_conf: float = 0.65\n",
    "        entropy_max: float = 1.60\n",
    "        minority_classes: tuple = (\"sadness\", \"speech_action\")\n",
    "        minority_conf: float = 0.90\n",
    "    \n",
    "    thr_cfg = _Thresh()\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"üî¨ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        calib_path = os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "        T_s1, tau = 1.0, 0.30\n",
    "        \n",
    "        try:\n",
    "            with open(calib_path, \"r\") as f:\n",
    "                _c = json_mod.load(f)\n",
    "            T_s1 = float(_c[\"T\"])\n",
    "            tau  = float(_c[\"tau\"])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è Missing {calib_path}; using defaults T={T_s1}, œÑ={tau}.\")\n",
    "        except (KeyError, Exception) as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not read {calib_path} ({e!s}); using defaults.\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits / max(T_s1, 1e-3)\n",
    "            probs_s1 = F.softmax(logits_s1, dim=-1)\n",
    "        \n",
    "        relevant_mask = (probs_s1[:, label2id_s1['relevant']] >= tau)\n",
    "        dev = logits_s1.device\n",
    "        preds_s1 = torch.where(\n",
    "            relevant_mask,\n",
    "            torch.tensor(label2id_s1['relevant'], device=dev, dtype=torch.long),\n",
    "            torch.tensor(label2id_s1['irrelevant'], device=dev, dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        if relevant_mask.any():\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "            _eps = 1e-12\n",
    "            entropies_s2 = (-probs_s2 * torch.log(probs_s2 + _eps)).sum(dim=1)\n",
    "\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            original_prediction = None\n",
    "            if relevant_mask[j]:\n",
    "                label_idx   = preds_s2[s2_idx].item()\n",
    "                label_name  = id2label_s2[label_idx]\n",
    "                original_prediction = label_name\n",
    "                confidence  = float(confs_s2[s2_idx].item())\n",
    "                entropy_val = float(entropies_s2[s2_idx].item())\n",
    "                s2_idx += 1\n",
    "\n",
    "                thr = thr_cfg.minority_conf if label_name in thr_cfg.minority_classes else thr_cfg.base_conf\n",
    "                if (confidence < thr) or (entropy_val > thr_cfg.entropy_max):\n",
    "                    final_label  = \"review_lowconf\"\n",
    "                    route_reason = \"thresholds\"\n",
    "                else:\n",
    "                    final_label  = label_name\n",
    "                    route_reason = \"passed\"\n",
    "            else:\n",
    "                final_label  = \"irrelevant\"\n",
    "                confidence   = float(torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item())\n",
    "                entropy_val  = float('nan')\n",
    "                route_reason = \"stage1_gate\"\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": final_label,\n",
    "                \"top1_label\": original_prediction, # The model's raw guess\n",
    "                \"confidence\": confidence,\n",
    "                \"route_reason\": route_reason,\n",
    "                \"entropy\": entropy_val\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    import pandas as pd   # ensure pd is local; prevents UnboundLocalError in notebooks\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\n‚úÖ Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # at top of the function (after building df)\n",
    "    GENERATE_TRAINING_SHORTLISTS = False   # training script should not rebuild these\n",
    "    GENERATE_MINING_PAIRS       = False    # keep mining in the curation notebook\n",
    "\n",
    "    if GENERATE_TRAINING_SHORTLISTS:\n",
    "        # ... (your existing shortlist + curated_additions code)\n",
    "        pass\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipping shortlist/curated_additions creation here (use curation notebook artifacts).\")\n",
    "    \n",
    "    if GENERATE_MINING_PAIRS:\n",
    "        # ... (your existing hard-negative mining code)\n",
    "        pass\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipping hard-negative mining here (handled in curation notebook).\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = REVIEW_CONF_THRESHOLD\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"üìÇ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- : Generate shortlist and curated patch CSVs for THIS run ---\n",
    "    #     - Shortlist: low-confidence items in focus classes (for targeted manual review)\n",
    "    #     - Curated patch: template CSV for corrected labels to be fed back into VNext\n",
    "    focus_classes = ['sadness','speech_action','neutral','neutral_speech','happiness']\n",
    "    \n",
    "    # Defensive: ensure the expected columns exist\n",
    "    has_pred = 'prediction' in df.columns or 'predicted_label' in df.columns\n",
    "    pred_col = 'prediction' if 'prediction' in df.columns else ('predicted_label' if 'predicted_label' in df.columns else None)\n",
    "    if pred_col is not None:\n",
    "        # Sort by confidence ascending (uncertain first)\n",
    "        df_focus = df[df[pred_col].isin(focus_classes)].copy()\n",
    "        if 'confidence' in df_focus.columns:\n",
    "            df_focus = df_focus.sort_values('confidence', ascending=True)\n",
    "    \n",
    "        short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "        patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "        # Write shortlist with a stable set of columns\n",
    "        keep_cols = [c for c in ['image_path','filepath','true_label',pred_col,'confidence'] if c in df_focus.columns]\n",
    "        df_focus[keep_cols].to_csv(short_csv, index=False)\n",
    "        print(f\"‚úÖ Shortlist written: {short_csv}\")\n",
    "    \n",
    "        # Create empty curated patch template\n",
    "        src_path_col = 'image_path' if 'image_path' in df_focus.columns else 'filepath'\n",
    "        patch_df = pd.DataFrame({\n",
    "            \"filepath\": df_focus[src_path_col],\n",
    "            \"correct_label\": \"\",\n",
    "            \"notes\": \"\"\n",
    "        })\n",
    "        patch_df.to_csv(patch_csv, index=False)\n",
    "        print(f\"‚úÖ Curated patch template written: {patch_csv}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped shortlist/patch CSVs: no predicted label column found in full log.\")\n",
    "\n",
    "    # --- : Merge this run's shortlist/patch with V32 to create canonical merged artifacts ---\n",
    "    def _merge_csvs(csv_list, key_cols, out_csv):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "    \n",
    "        # Normalize common column name variants so we can dedupe safely\n",
    "        def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            colmap = {}\n",
    "            # path columns\n",
    "            if \"image_path\" not in df.columns:\n",
    "                if \"filepath\" in df.columns:\n",
    "                    colmap[\"filepath\"] = \"image_path\"\n",
    "                elif \"path\" in df.columns:\n",
    "                    colmap[\"path\"] = \"image_path\"\n",
    "            # predicted label columns\n",
    "            if \"predicted_label\" not in df.columns:\n",
    "                if \"prediction\" in df.columns:\n",
    "                    colmap[\"prediction\"] = \"predicted_label\"\n",
    "                elif \"predicted\" in df.columns:\n",
    "                    colmap[\"predicted\"] = \"predicted_label\"\n",
    "            return df.rename(columns=colmap)\n",
    "    \n",
    "        frames = []\n",
    "        for p in csv_list:\n",
    "            if os.path.exists(p):\n",
    "                try:\n",
    "                    df = pd.read_csv(p)\n",
    "                    df = _normalize_cols(df)\n",
    "                    frames.append(df)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "        if not frames:\n",
    "            return\n",
    "    \n",
    "        merged = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "        # Keep only keys that actually exist after normalization\n",
    "        available_keys = [k for k in key_cols if k in merged.columns]\n",
    "        if not available_keys:\n",
    "            print(f\"‚ÑπÔ∏è Skipped merge for {out_csv}: none of the key columns {key_cols} exist in merged data.\")\n",
    "            return\n",
    "    \n",
    "        merged = merged.drop_duplicates(subset=available_keys, keep=\"first\")\n",
    "        merged.to_csv(out_csv, index=False)\n",
    "        print(f\"‚úÖ Merged: {out_csv} ({len(merged)} rows)\")\n",
    "\n",
    "    \n",
    "    # Paths for this run (already defined above)\n",
    "    short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "    patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "    # V32 paths (if present)\n",
    "    v32_short = os.path.join(save_dir, \"curation_shortlist_V32.csv\")\n",
    "    v32_patch = os.path.join(save_dir, \"curated_additions_V32.csv\")\n",
    "    \n",
    "    # Canonical merged outputs\n",
    "    short_merged = os.path.join(save_dir, \"curation_shortlist_merged.csv\")\n",
    "    patch_merged = os.path.join(save_dir, \"curated_additions_merged.csv\")\n",
    "    \n",
    "    # Merge (shortlist merges on [filepath, predicted_label]; patch merges on [filepath])\n",
    "    if pred_col is not None:\n",
    "        # Figure out the filepath column available\n",
    "        avail_path_cols = ['image_path','filepath']\n",
    "        path_col = next((c for c in avail_path_cols if c in df.columns), None)\n",
    "    \n",
    "        if path_col is not None:\n",
    "            _merge_csvs([v32_short, short_csv], key_cols=[path_col, pred_col], out_csv=short_merged)\n",
    "            _merge_csvs([v32_patch, patch_csv], key_cols=[path_col], out_csv=patch_merged)\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Skipped merge: no filepath column present in full log.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped merge: no predicted label column present in full log.\")\n",
    "\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs ---\n",
    "    MINING_HARD_NEGATIVES = True\n",
    "\n",
    "    if MINING_HARD_NEGATIVES:\n",
    "        print(\"\\n‚õèÔ∏è  Mining for hard negative confusion pairs...\")\n",
    "        \n",
    "        # FIX: First, filter the DataFrame to only include true relevant images\n",
    "        # This prevents images from 'hard_case' that slipped past S1 from being mined.\n",
    "        relevant_df = df[df['true_label'].isin(RELEVANT_CLASSES)].copy()\n",
    "        print(f\"   - Analyzing confusions within {len(relevant_df)} truly relevant images.\")\n",
    "\n",
    "        col_true = 'true_label'\n",
    "        col_pred = 'top1_label' # Use the raw model prediction before thresholding\n",
    "        \n",
    "        if col_pred not in relevant_df.columns:\n",
    "             raise RuntimeError(f\"Could not find '{col_pred}' column. Ensure hierarchical_predict is logging it.\")\n",
    "\n",
    "        # FIX: Add the speech_action pairs to the mining list\n",
    "        confusion_pairs_to_mine = [\n",
    "            ('contempt', 'questioning'),\n",
    "            ('contempt', 'neutral'),\n",
    "            ('fear', 'surprise'),\n",
    "            ('neutral_speech', 'speech_action'),\n",
    "            ('sadness', 'speech_action')\n",
    "        ]\n",
    "\n",
    "        for c1, c2 in confusion_pairs_to_mine:\n",
    "            # The mask now operates on the pre-filtered 'relevant_df'\n",
    "            mask = ((relevant_df[col_true] == c1) & (relevant_df[col_pred] == c2)) | \\\n",
    "                   ((relevant_df[col_true] == c2) & (relevant_df[col_pred] == c1))\n",
    "            hard_negatives = relevant_df.loc[mask]\n",
    "\n",
    "            if not hard_negatives.empty:\n",
    "                out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "                hard_negatives.to_csv(out_path, index=False)\n",
    "                print(f\"  - Found {len(hard_negatives)} hard negatives for ({c1} ‚Üî {c2}). Saved: {out_path}\")\n",
    "            else:\n",
    "                print(f\"  - No hard negatives found for ({c1} ‚Üî {c2}).\")\n",
    "    else:\n",
    "        print(\"‚è© Hard-negative mining disabled.\")\n",
    "    \n",
    "\n",
    "    # --- Part D: Generate Final End-to-End Performance Report ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  END-TO-END PIPELINE PERFORMANCE REPORT (S1+S2)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # For a clean report, we'll consider 'review_lowconf' as a misclassification\n",
    "    # and filter the dataframe to only include the original, known labels.\n",
    "    report_df = df[df['true_label'].isin(RELEVANT_CLASSES + IRRELEVANT_CLASSES)].copy()\n",
    "    \n",
    "    # We also need to map the S1 'irrelevant' ground truth to its own class\n",
    "    report_df.loc[report_df['true_label'].isin(IRRELEVANT_CLASSES), 'true_label'] = 'irrelevant'\n",
    "    \n",
    "    # Get all labels that appear in either the true or predicted columns for the report\n",
    "    all_labels = sorted(list(set(report_df['true_label'].unique()) | set(report_df['prediction'].unique())))\n",
    "    \n",
    "    # Generate and print the final report\n",
    "    end_to_end_report = classification_report(\n",
    "        report_df['true_label'], \n",
    "        report_df['prediction'], \n",
    "        labels=all_labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(\"This report reflects the true performance of the entire two-stage system.\")\n",
    "    print(\"It accounts for errors made by both the Stage 1 filter and the Stage 2 classifier.\\n\")\n",
    "    print(end_to_end_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"üìä Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üß™ RUNNING PRE-FLIGHT SMOKE TEST...\n",
      "============================================================\n",
      "   - Loading latest model from: V38_20251021_123355\n",
      "   - Running inference on 5 random images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Running Hierarchical Inference:   0%|                  | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Missing /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/stage1_calibration.json; using defaults T=1.0, œÑ=0.3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Running Hierarchical Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Missing /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/stage1_calibration.json; using defaults T=1.0, œÑ=0.3.\n",
      "     - OK: 'Training_15203635.jpg' -> 'irrelevant'\n",
      "     - OK: 'PublicTest_96387819.jpg' -> 'irrelevant'\n",
      "     - OK: 'Tom_Welch_0001.jpg' -> 'irrelevant'\n",
      "     - OK: 'Colin_Powell_0179.jpg' -> 'irrelevant'\n",
      "     - OK: 'Atal_Bihari_Vajpayee_0009.jpg_face1.jpg' -> 'irrelevant'\n",
      "\n",
      "‚úÖ Smoke test passed successfully.\n",
      "\n",
      "============================================================\n",
      "üöÄ PROCEEDING TO FULL TRAINING PIPELINE...\n",
      "============================================================\n",
      "\n",
      "üñ•Ô∏è Using device: cpu\n",
      "‚úÖ Skipping dataset preparation, using existing directories.\n",
      "\n",
      "============================================================\n",
      "  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d486fac2174ac5a00174d3266e096c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: 21504 training samples, 5377 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions_v5/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Stage 1 Class Weights: tensor([0.6492, 2.1761])\n",
      "üöÄ Starting Stage 1 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6720' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6720/6720 4:26:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.050759</td>\n",
       "      <td>0.992003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.043872</td>\n",
       "      <td>0.987725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.019275</td>\n",
       "      <td>0.990701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>0.990887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029885</td>\n",
       "      <td>0.984564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.99      1.00      0.99      4132\n",
      "    relevant       0.99      0.98      0.98      1245\n",
      "\n",
      "    accuracy                           0.99      5377\n",
      "   macro avg       0.99      0.99      0.99      5377\n",
      "weighted avg       0.99      0.99      0.99      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - relevant ‚Üí irrelevant: 30 instances\n",
      "  - irrelevant ‚Üí relevant: 13 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0241\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.0926\n",
      "  - irrelevant: entropy = 0.0035\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       1.00      0.99      0.99      4132\n",
      "    relevant       0.96      0.99      0.97      1245\n",
      "\n",
      "    accuracy                           0.99      5377\n",
      "   macro avg       0.98      0.99      0.98      5377\n",
      "weighted avg       0.99      0.99      0.99      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant ‚Üí relevant: 52 instances\n",
      "  - relevant ‚Üí irrelevant: 14 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0215\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.0641\n",
      "  - irrelevant: entropy = 0.0087\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       1.00      0.99      0.99      4132\n",
      "    relevant       0.97      0.99      0.98      1245\n",
      "\n",
      "    accuracy                           0.99      5377\n",
      "   macro avg       0.98      0.99      0.99      5377\n",
      "weighted avg       0.99      0.99      0.99      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant ‚Üí relevant: 43 instances\n",
      "  - relevant ‚Üí irrelevant: 7 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0224\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.0676\n",
      "  - irrelevant: entropy = 0.0088\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       1.00      0.99      0.99      4132\n",
      "    relevant       0.97      0.99      0.98      1245\n",
      "\n",
      "    accuracy                           0.99      5377\n",
      "   macro avg       0.98      0.99      0.99      5377\n",
      "weighted avg       0.99      0.99      0.99      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant ‚Üí relevant: 38 instances\n",
      "  - relevant ‚Üí irrelevant: 11 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0169\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.0493\n",
      "  - irrelevant: entropy = 0.0071\n",
      "\n",
      "üìà Classification Report for Stage1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       1.00      0.98      0.99      4132\n",
      "    relevant       0.94      1.00      0.97      1245\n",
      "\n",
      "    accuracy                           0.98      5377\n",
      "   macro avg       0.97      0.99      0.98      5377\n",
      "weighted avg       0.99      0.98      0.98      5377\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - irrelevant ‚Üí relevant: 81 instances\n",
      "  - relevant ‚Üí irrelevant: 2 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.0189\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - relevant: entropy = 0.0452\n",
      "  - irrelevant: entropy = 0.0109\n",
      "‚åõ Stage 1 training took: 04:26:50\n",
      "üíæ Saving relevance_filter_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948\n",
      "‚úÖ relevance_filter_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 1 Training Complete.\n",
      "\n",
      "üß™ Calibrating Stage 1...\n",
      "‚ÑπÔ∏è S1 calib eval prevalence (relevant rate): 0.232\n",
      "‚úÖ S1 calibration done: T=5.821 | best œÑ=0.28 | F1=0.364 (P=0.755, R=0.24)\n",
      "‚úÖ Wrote S1 calibration to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/stage1_calibration.json\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db35ebc20ff94367ba7731cd307266ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n",
      "üöÄ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='926' max='1848' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 926/1848 38:37 < 38:32, 0.40 it/s, Epoch 2/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.237258</td>\n",
       "      <td>0.917409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.203355</td>\n",
       "      <td>0.910931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.206216</td>\n",
       "      <td>0.917409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.94      0.87      0.90        85\n",
      "      contempt       0.80      0.68      0.74        60\n",
      "       disgust       0.86      0.69      0.77        26\n",
      "          fear       0.87      0.96      0.91        71\n",
      "     happiness       0.91      1.00      0.95       167\n",
      "       neutral       0.97      0.99      0.98       135\n",
      "   questioning       0.81      0.95      0.87        92\n",
      "       sadness       0.91      0.78      0.84        40\n",
      "      surprise       0.98      0.95      0.96       147\n",
      "neutral_speech       0.96      0.90      0.93       381\n",
      " speech_action       0.69      1.00      0.82        31\n",
      "\n",
      "      accuracy                           0.92      1235\n",
      "     macro avg       0.88      0.89      0.88      1235\n",
      "  weighted avg       0.92      0.92      0.92      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech ‚Üí happiness: 14 instances\n",
      "  - contempt ‚Üí questioning: 10 instances\n",
      "  - neutral_speech ‚Üí speech_action: 10 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4220\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.6597\n",
      "  - disgust: entropy = 0.5689\n",
      "  - sadness: entropy = 0.5308\n",
      "  - anger: entropy = 0.4664\n",
      "  - fear: entropy = 0.4630\n",
      "  - neutral_speech: entropy = 0.4522\n",
      "  - questioning: entropy = 0.4054\n",
      "  - surprise: entropy = 0.3819\n",
      "  - neutral: entropy = 0.3539\n",
      "  - speech_action: entropy = 0.3244\n",
      "  - happiness: entropy = 0.2966\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.87      0.96      0.92        85\n",
      "      contempt       0.79      0.70      0.74        60\n",
      "       disgust       0.87      0.77      0.82        26\n",
      "          fear       0.90      0.90      0.90        71\n",
      "     happiness       0.95      0.98      0.96       167\n",
      "       neutral       0.90      0.99      0.94       135\n",
      "   questioning       0.83      0.93      0.88        92\n",
      "       sadness       0.82      0.80      0.81        40\n",
      "      surprise       0.98      0.89      0.94       147\n",
      "neutral_speech       0.96      0.90      0.93       381\n",
      " speech_action       0.68      0.90      0.78        31\n",
      "\n",
      "      accuracy                           0.91      1235\n",
      "     macro avg       0.87      0.88      0.87      1235\n",
      "  weighted avg       0.91      0.91      0.91      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech ‚Üí speech_action: 9 instances\n",
      "  - contempt ‚Üí questioning: 8 instances\n",
      "  - neutral_speech ‚Üí happiness: 8 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4794\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.7058\n",
      "  - sadness: entropy = 0.6413\n",
      "  - disgust: entropy = 0.6121\n",
      "  - questioning: entropy = 0.5713\n",
      "  - happiness: entropy = 0.5449\n",
      "  - fear: entropy = 0.4961\n",
      "  - neutral_speech: entropy = 0.4833\n",
      "  - surprise: entropy = 0.3931\n",
      "  - anger: entropy = 0.3904\n",
      "  - speech_action: entropy = 0.3532\n",
      "  - neutral: entropy = 0.3212\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.92      0.93      0.92        85\n",
      "      contempt       0.78      0.77      0.77        60\n",
      "       disgust       0.78      0.81      0.79        26\n",
      "          fear       0.79      0.96      0.87        71\n",
      "     happiness       0.96      0.95      0.96       167\n",
      "       neutral       0.98      0.94      0.96       135\n",
      "   questioning       0.87      0.93      0.90        92\n",
      "       sadness       0.93      0.65      0.76        40\n",
      "      surprise       0.99      0.94      0.97       147\n",
      "neutral_speech       0.94      0.93      0.94       381\n",
      " speech_action       0.71      0.97      0.82        31\n",
      "\n",
      "      accuracy                           0.92      1235\n",
      "     macro avg       0.88      0.89      0.88      1235\n",
      "  weighted avg       0.92      0.92      0.92      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 9 instances\n",
      "  - happiness ‚Üí neutral_speech: 8 instances\n",
      "  - neutral_speech ‚Üí speech_action: 8 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4460\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.5950\n",
      "  - sadness: entropy = 0.5933\n",
      "  - disgust: entropy = 0.5666\n",
      "  - questioning: entropy = 0.4763\n",
      "  - neutral: entropy = 0.4761\n",
      "  - anger: entropy = 0.4470\n",
      "  - neutral_speech: entropy = 0.4306\n",
      "  - happiness: entropy = 0.4182\n",
      "  - fear: entropy = 0.3947\n",
      "  - surprise: entropy = 0.3897\n",
      "  - speech_action: entropy = 0.3650\n",
      "‚åõ Stage 2 training took: 0.64 hours\n",
      "‚åõ Stage 2 training took: 00:38:40\n",
      "üíæ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948\n",
      "‚úÖ emotion_classifier_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 2 Training Complete.\n",
      "\n",
      "üß™ Calibrating Stage 2 (scalar T on eval set)...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.94      0.86      0.90        85\n",
      "      contempt       0.85      0.73      0.79        60\n",
      "       disgust       0.78      0.69      0.73        26\n",
      "          fear       0.90      0.93      0.92        71\n",
      "     happiness       0.91      1.00      0.95       167\n",
      "       neutral       0.97      0.98      0.97       135\n",
      "   questioning       0.84      0.91      0.88        92\n",
      "       sadness       0.90      0.65      0.75        40\n",
      "      surprise       0.97      0.95      0.96       147\n",
      "neutral_speech       0.93      0.90      0.92       381\n",
      " speech_action       0.65      0.97      0.78        31\n",
      "\n",
      "      accuracy                           0.91      1235\n",
      "     macro avg       0.88      0.87      0.87      1235\n",
      "  weighted avg       0.91      0.91      0.91      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech ‚Üí happiness: 14 instances\n",
      "  - neutral_speech ‚Üí speech_action: 10 instances\n",
      "  - anger ‚Üí neutral_speech: 8 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4289\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.7045\n",
      "  - sadness: entropy = 0.5606\n",
      "  - disgust: entropy = 0.5060\n",
      "  - anger: entropy = 0.4900\n",
      "  - fear: entropy = 0.4677\n",
      "  - neutral_speech: entropy = 0.4522\n",
      "  - questioning: entropy = 0.4401\n",
      "  - surprise: entropy = 0.3900\n",
      "  - neutral: entropy = 0.3531\n",
      "  - speech_action: entropy = 0.3114\n",
      "  - happiness: entropy = 0.2968\n",
      "‚úÖ S2 calibration done: T=1.187 ‚Üí /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/emotion_classifier_model/stage2_calibration.json\n",
      "\n",
      "üìä Re-running evaluation with calibrated temperature:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='309' max='309' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [309/309 01:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage2_Calibrated:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.94      0.88      0.91        85\n",
      "      contempt       0.80      0.72      0.75        60\n",
      "       disgust       0.86      0.69      0.77        26\n",
      "          fear       0.92      0.96      0.94        71\n",
      "     happiness       0.91      1.00      0.95       167\n",
      "       neutral       0.96      0.98      0.97       135\n",
      "   questioning       0.83      0.96      0.89        92\n",
      "       sadness       0.91      0.78      0.84        40\n",
      "      surprise       0.97      0.93      0.95       147\n",
      "neutral_speech       0.96      0.90      0.93       381\n",
      " speech_action       0.68      0.97      0.80        31\n",
      "\n",
      "      accuracy                           0.92      1235\n",
      "     macro avg       0.88      0.89      0.88      1235\n",
      "  weighted avg       0.92      0.92      0.92      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech ‚Üí happiness: 14 instances\n",
      "  - contempt ‚Üí questioning: 10 instances\n",
      "  - neutral_speech ‚Üí speech_action: 10 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.6768\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.9073\n",
      "  - disgust: entropy = 0.8838\n",
      "  - sadness: entropy = 0.7695\n",
      "  - anger: entropy = 0.7646\n",
      "  - fear: entropy = 0.7194\n",
      "  - neutral_speech: entropy = 0.6834\n",
      "  - questioning: entropy = 0.6637\n",
      "  - surprise: entropy = 0.6365\n",
      "  - neutral: entropy = 0.6185\n",
      "  - speech_action: entropy = 0.6172\n",
      "  - happiness: entropy = 0.5623\n",
      "\n",
      "üéâ Hierarchical Training Pipeline Finished Successfully.\n",
      "\n",
      "============================================================\n",
      "  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\n",
      "============================================================\n",
      "Found 26755 images to process for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Running Hierarchical Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 837/837 [23:22<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Full inference log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/V39_full_inference_log.csv\n",
      "‚ÑπÔ∏è Skipping shortlist/curated_additions creation here (use curation notebook artifacts).\n",
      "‚ÑπÔ∏è Skipping hard-negative mining here (handled in curation notebook).\n",
      "\n",
      "Found 3375 images below 0.85 confidence for review.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting review images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3375/3375 [00:01<00:00, 2797.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Sorted review images into folders at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/review_candidates_by_predicted_class\n",
      "‚úÖ Shortlist written: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/curation_shortlist_V39.csv\n",
      "‚úÖ Curated patch template written: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/curated_additions_V39.csv\n",
      "‚úÖ Merged: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/curation_shortlist_merged.csv (1364 rows)\n",
      "‚úÖ Merged: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/curated_additions_merged.csv (1364 rows)\n",
      "\n",
      "‚õèÔ∏è  Mining for hard negative confusion pairs...\n",
      "   - Analyzing confusions within 6174 truly relevant images.\n",
      "  - No hard negatives found for (contempt ‚Üî questioning).\n",
      "  - Found 2 hard negatives for (contempt ‚Üî neutral). Saved: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/hard_negatives_contempt_vs_neutral.csv\n",
      "  - No hard negatives found for (fear ‚Üî surprise).\n",
      "  - Found 998 hard negatives for (neutral_speech ‚Üî speech_action). Saved: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/hard_negatives_neutral_speech_vs_speech_action.csv\n",
      "  - Found 1 hard negatives for (sadness ‚Üî speech_action). Saved: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V39_20251022_093948/hard_negatives_sadness_vs_speech_action.csv\n",
      "\n",
      "============================================================\n",
      "  END-TO-END PIPELINE PERFORMANCE REPORT (S1+S2)\n",
      "============================================================\n",
      "This report reflects the true performance of the entire two-stage system.\n",
      "It accounts for errors made by both the Stage 1 filter and the Stage 2 classifier.\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.95      0.04      0.07       472\n",
      "      contempt       0.95      0.06      0.11       311\n",
      "       disgust       1.00      0.02      0.05       126\n",
      "          fear       1.00      0.05      0.09       311\n",
      "     happiness       0.93      0.10      0.18       818\n",
      "    irrelevant       0.00      0.00      0.00         0\n",
      "       neutral       0.98      0.07      0.13       665\n",
      "neutral_speech       0.13      0.32      0.19       472\n",
      "   questioning       0.05      0.01      0.01       139\n",
      "review_lowconf       0.00      0.00      0.00         0\n",
      "       sadness       0.00      0.00      0.00       701\n",
      " speech_action       0.00      0.00      0.00      1993\n",
      "      surprise       0.01      0.01      0.01       166\n",
      "\n",
      "      accuracy                           0.05      6174\n",
      "     macro avg       0.46      0.05      0.06      6174\n",
      "  weighted avg       0.43      0.05      0.07      6174\n",
      "\n",
      "\n",
      "============================================================\n",
      "  DEPLOYMENT READINESS CHECK\n",
      "============================================================\n",
      "Threshold: F1-Score >= 0.8\n",
      "\n",
      "  - ‚úÖ anger           | F1-Score: 0.90\n",
      "  - ‚ùå contempt        | F1-Score: 0.79 (Below Threshold)\n",
      "  - ‚ùå disgust         | F1-Score: 0.73 (Below Threshold)\n",
      "  - ‚úÖ fear            | F1-Score: 0.92\n",
      "  - ‚úÖ happiness       | F1-Score: 0.95\n",
      "  - ‚úÖ neutral         | F1-Score: 0.97\n",
      "  - ‚úÖ questioning     | F1-Score: 0.88\n",
      "  - ‚ùå sadness         | F1-Score: 0.75 (Below Threshold)\n",
      "  - ‚úÖ surprise        | F1-Score: 0.96\n",
      "  - ‚úÖ neutral_speech  | F1-Score: 0.92\n",
      "  - ‚ùå speech_action   | F1-Score: 0.78 (Below Threshold)\n",
      "\n",
      " Model is NOT ready for production.\n",
      "\n",
      "============================================================\n",
      "  CALIBRATING STAGE 2 MODEL\n",
      "============================================================\n",
      "‚úÖ Optimal temperature for Stage 2 model: 1.1871\n",
      "\n",
      "============================================================\n",
      "  RUNNING ENSEMBLE ANALYSIS (Current: V39_20251022_093948 vs. Previous: V38_20251021_123355)\n",
      "============================================================\n",
      "Ensemble prediction for 'posed_happiness_photo_771.jpg': irrelevant (Confidence: N/A)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point (with Integrated Smoke Test and Full Analysis)\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # --- Configuration for Pre-flight Smoke Test ---\n",
    "    RUN_SMOKE_TEST = True\n",
    "\n",
    "    if RUN_SMOKE_TEST:\n",
    "        print(\"=\"*60)\n",
    "        print(\"üß™ RUNNING PRE-FLIGHT SMOKE TEST...\")\n",
    "        print(\"=\"*60)\n",
    "        try:\n",
    "            checkpoint_path = find_latest_checkpoint(OUTPUT_ROOT_DIR)\n",
    "            if not checkpoint_path or not os.path.isdir(checkpoint_path):\n",
    "                raise FileNotFoundError(\n",
    "                    \"No previous model found for smoke test. \"\n",
    "                    \"To train from scratch, set RUN_SMOKE_TEST = False.\"\n",
    "                )\n",
    "\n",
    "            print(f\"   - Loading latest model from: {os.path.basename(checkpoint_path)}\")\n",
    "            model_s1, model_s2, processor = _load_exports_for_smoke(checkpoint_path, device)\n",
    "            \n",
    "            image_paths = [str(p) for p in Path(BASE_DATASET_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    "            if not image_paths:\n",
    "                raise FileNotFoundError(f\"No images found in {BASE_DATASET_PATH} for smoke test.\")\n",
    "            \n",
    "            test_images = random.sample(image_paths, min(len(image_paths), 5))\n",
    "            print(f\"   - Running inference on {len(test_images)} random images...\")\n",
    "\n",
    "            predictions = hierarchical_predict(\n",
    "                image_paths=test_images, model_s1=model_s1, model_s2=model_s2,\n",
    "                processor=processor, device=device, batch_size=4\n",
    "            )\n",
    "\n",
    "            if len(predictions) != len(test_images):\n",
    "                raise RuntimeError(f\"Inference failed. Expected {len(test_images)} results, got {len(predictions)}.\")\n",
    "\n",
    "            for result in predictions:\n",
    "                print(f\"     - OK: '{os.path.basename(result['image_path'])}' -> '{result['prediction']}'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå SMOKE TEST FAILED: {e}\")\n",
    "            print(\"   - Halting script. Please resolve the issue or set RUN_SMOKE_TEST = False.\")\n",
    "            raise\n",
    "\n",
    "        print(\"\\n‚úÖ Smoke test passed successfully.\")\n",
    "\n",
    "    # --- Step 1: Execute Training Pipeline ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üöÄ PROCEEDING TO FULL TRAINING PIPELINE...\")\n",
    "    print(\"=\"*60)\n",
    "    model_s1, model_s2, processor = main(device)\n",
    "\n",
    "    # --- Step 2: Run Post-Training Analysis & Curation ---\n",
    "    if RUN_INFERENCE:\n",
    "        run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "\n",
    "    # --- Step 3: Run Final Model Checks ---\n",
    "    stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "    check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "\n",
    "    # --- Step 4: Calibrate the Stage 2 Model (Restored) ---\n",
    "    logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "    labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "    \n",
    "    if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  CALIBRATING STAGE 2 MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        logits_s2 = np.load(logits_s2_path)\n",
    "        labels_s2 = np.load(labels_s2_path)\n",
    "        \n",
    "        optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "        print(f\"‚úÖ Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping Stage 2 calibration: log files not found.\")\n",
    "\n",
    "    # --- Step 5: Run Ensemble Analysis (Restored) ---\n",
    "    # Note: This uses a hardcoded path to a previous version for comparison.\n",
    "    v_prev_path = find_latest_checkpoint(OUTPUT_ROOT_DIR, current_run_basename=VERSION_TAG)\n",
    "    \n",
    "    if v_prev_path:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"  RUNNING ENSEMBLE ANALYSIS (Current: {VERSION_TAG} vs. Previous: {os.path.basename(v_prev_path)})\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        s1_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"relevance_filter_model\")\n",
    "        ).to(device).eval()\n",
    "        s2_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"emotion_classifier_model\")\n",
    "        ).to(device).eval()\n",
    "        \n",
    "        s1_models_ensemble = [model_s1, s1_model_prev]\n",
    "        s2_models_ensemble = [model_s2, s2_model_prev]\n",
    "\n",
    "        # Find a random image to test the ensemble\n",
    "        all_images = [str(p) for p in Path(BASE_DATASET_PATH).rglob(\"*\") if is_valid_image(p.name)]\n",
    "        if all_images:\n",
    "            example_image_path = random.choice(all_images)\n",
    "            prediction, confidence = hierarchical_ensemble_predict(\n",
    "                example_image_path, processor, s1_models_ensemble, s2_models_ensemble, device\n",
    "            )\n",
    "            if confidence is not None:\n",
    "                print(f\"Ensemble prediction for '{os.path.basename(example_image_path)}': {prediction} (Confidence: {confidence:.2f})\")\n",
    "            else:\n",
    "                print(f\"Ensemble prediction for '{os.path.basename(example_image_path)}': {prediction} (Confidence: N/A)\")\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è Skipping ensemble analysis: no previous model version found to compare against.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacac8a-70b8-4b4d-bdc6-d2f098b62df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
