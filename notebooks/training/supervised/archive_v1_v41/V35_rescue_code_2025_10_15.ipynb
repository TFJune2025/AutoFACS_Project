{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# V35 RESCUE CODE  -  One-off stabilization & rerun prep (non-standard VML flow)\n",
    "# Summary: Reuse existing V35 folder; load S1 from disk; de-dupe SAVE_DIR logic;\n",
    "    # ensure S2 trains cleanly without patch_V35.csv for this run.\n",
    "# ==============================================================================\n",
    "\n",
    "# overview: Stabilize runtime and pathing for an in-notebook rerun; skip S1 retrain;\n",
    "#           keep Stage-2 flow clean and spawn-safe on macOS; minimal edits to proceed.\n",
    "\n",
    "# section #0 (imports / housekeeping / paths):\n",
    "#   - Defined single source of truth:\n",
    "#       SAVE_DIR  = \".../V35_20251014_162112\"\n",
    "#       VERSION_TAG = os.path.basename(SAVE_DIR)\n",
    "#       VERSION     = VERSION_TAG\n",
    "#   - Consolidated SAVE_DIR setup (removed duplicate assignments/prints).\n",
    "#   - Ensured imports precede first use (os/glob/re/time/datetime).\n",
    "#   - Rationale: avoid accidental new-run folders and NameErrors.\n",
    "#   - Expected Impact: deterministic artifact locations; clean bootstrap.\n",
    "\n",
    "# section #2 (checkpoint discovery):\n",
    "#   - find_latest_checkpoint(...) sorts by (version, timestamp) and excludes current run.\n",
    "#   - Rationale: when reusing V35, latest-previous resolves to V34 by design.\n",
    "#   - Expected Impact: S2 initializes from the most recent completed baseline.\n",
    "\n",
    "\n",
    "# Stage 1 (relevance) ‚Äî loader path\n",
    "# section #3 (S1 load instead of train):\n",
    "#   - Replaced training with disk load from SAVE_DIR/relevance_filter_model.\n",
    "#   - Loaded processor from same folder for S2 parity.\n",
    "#   - Rationale: skip multi-hour S1 retrain.\n",
    "#   - Expected Impact: immediate availability of model_s1 + processor.\n",
    "\n",
    "# section #3 (S1 calibration block removal):\n",
    "#   - Disabled the calibration block referencing trainer_s1/eval_dataset_s1.\n",
    "#   - Rationale: those objects do not exist in loader-only mode.\n",
    "#   - Expected Impact: prevent NameError; optional calibration deferred.\n",
    "\n",
    "\n",
    "# Stage 2 (emotion) ‚Äî training safety & cleanliness:\n",
    "# section #5 (TrainingArguments):\n",
    "#   - Set overwrite_output_dir=True to allow reruns into the same directory.\n",
    "#   - Rationale: avoid ‚Äúdirectory not empty‚Äù conflicts.\n",
    "#   - Expected Impact: idempotent reruns.\n",
    "\n",
    "# section #5 (early stopping):\n",
    "#   - Defined EarlyStoppingCallback(patience=2, threshold=0.0).\n",
    "#   - Rationale: previously referenced but undefined.\n",
    "#   - Expected Impact: stable convergence guard (no NameError).\n",
    "\n",
    "# section #5 (sampler bind order):\n",
    "#   - Bound WeightedRandomSampler only AFTER trainer_s2 construction.\n",
    "#   - Rationale: prevent early reference to trainer_s2.\n",
    "#   - Expected Impact: valid sampler override; no shuffle conflicts.\n",
    "\n",
    "\n",
    "# Stage 2 (emotion) ‚Äî DataLoader stability (macOS spawn-safe):\n",
    "# section #5 (TrainingArguments ‚Äì workers):\n",
    "#   - Added: dataloader_num_workers=0\n",
    "#   - Rationale: prevent multiprocessing from pickling DataCollatorWithAugmentation\n",
    "#     (defined in __main__), which raised AttributeError in worker processes.\n",
    "#   - Expected Impact: stable training loop; minor I/O slowdown acceptable.\n",
    "\n",
    "# section #5 (custom train DataLoader override):\n",
    "#   - Updated override to: num_workers=0, pin_memory=False  (CPU-only).\n",
    "#   - Rationale: keep collation in main process to avoid spawn-time pickling.\n",
    "#   - Expected Impact: eliminates worker exit errors; deterministic batches.\n",
    "\n",
    "# notes (future optional):\n",
    "#   - To re-enable multi-worker loading, move DataCollatorWithAugmentation into\n",
    "#     an importable module (e.g., collators.py) and import it; then raise\n",
    "#     dataloader_num_workers > 0 safely.\n",
    "\n",
    "\n",
    "# Utilities / nits:\n",
    "# section #9 (CSV writing):\n",
    "#   - Fixed writer argument where applicable: newline=\"\" (not line=\"\").\n",
    "#   - Rationale: correct CSV semantics; avoid extra blank lines.\n",
    "#   - Expected Impact: cleaner artifact CSVs (minor).\n",
    "\n",
    "\n",
    "# Out-of-scope for this rescue run:\n",
    "# section #10 (dataset patching):\n",
    "#   - Intentionally not using patch_V35.csv (train-only injection) in this pass.\n",
    "#   - Rationale: expedite a clean S2 rerun first; revisit targeted patching later.\n",
    "#   - Expected Impact: standard 80/20 split for S2 in this run.\n",
    "\n",
    "\n",
    "# Operator Notes:\n",
    "# - This ‚ÄúRescue Code‚Äù changelog documents a one-off stabilization lane to complete V35\n",
    "#   safely without data prep or S1 retraining. Fold changes selectively into mainline\n",
    "#   once metrics are reviewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageStat, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using SAVE_DIR: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\n",
      "‚úÖ Dynamically loading latest checkpoint: V34_20251013_211825\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# (ensure these imports appear before this block in your notebook/file)\n",
    "# import os, glob, re, time\n",
    "# from datetime import datetime\n",
    "\n",
    "# --- üìÇ Core Paths ---\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "OUTPUT_ROOT_DIR   = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- ‚öôÔ∏è Run Configuration ---\n",
    "RUN_INFERENCE     = True          # safer default for dev\n",
    "PREPARE_DATASETS  = False         # set True only when layout changes\n",
    "\n",
    "# ‚îÄ‚îÄ Reuse existing V35 run directory (no new folder) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "USE_EXISTING_SAVE_DIR = True\n",
    "EXISTING_V35_DIR      = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\"\n",
    "\n",
    "# Resolve SAVE_DIR / VERSION_TAG / VERSION exactly once\n",
    "if USE_EXISTING_SAVE_DIR:\n",
    "    SAVE_DIR    = EXISTING_V35_DIR\n",
    "    VERSION_TAG = os.path.basename(SAVE_DIR)   # e.g., \"V35_20251014_162112\"\n",
    "else:\n",
    "    # (fallback: create a new run folder only if you ever flip the flag)\n",
    "    VERSION_TAG = f\"V35_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "    SAVE_DIR    = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "VERSION = VERSION_TAG\n",
    "print(f\"üìÅ Using SAVE_DIR: {SAVE_DIR}\")\n",
    "\n",
    "# --- ü§ñ Model Configuration ---\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# --- üè∑Ô∏è Dataset & Label Definitions ---\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger','contempt','disgust','fear','happiness',\n",
    "    'neutral','questioning','sadness','surprise',\n",
    "    'neutral_speech','speech_action'\n",
    "]\n",
    "IRRELEVANT_CLASSES = ['hard_case']  # 'unknown' is inside 'hard_case' recursively\n",
    "\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "REVIEW_CONF_THRESHOLD = 0.85\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- üî¢ Versioning helpers (optional) ---\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "def find_latest_checkpoint(root_dir, current_run_basename=None):\n",
    "    \"\"\"\n",
    "    Return the path to the most recent *completed* run by semantic version + timestamp,\n",
    "    excluding the current run directory. Ignores folders that don't contain model artifacts.\n",
    "    Pattern: V<num>_YYYYMMDD_HHMMSS (e.g., V34_20251013_211825)\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    pat = re.compile(r\"^V(\\d+)_(\\d{8}_\\d{6})$\")\n",
    "\n",
    "    for d in os.listdir(root_dir):\n",
    "        full = os.path.join(root_dir, d)\n",
    "        if not (os.path.isdir(full) and d.startswith(\"V\")):\n",
    "            continue\n",
    "        if current_run_basename and d == current_run_basename:\n",
    "            continue\n",
    "\n",
    "        m = pat.match(d)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        ver = int(m.group(1))\n",
    "        ts  = m.group(2)\n",
    "\n",
    "        # \"completed\" if it contains any known artifact folders\n",
    "        has_model = any(\n",
    "            os.path.isdir(os.path.join(full, p))\n",
    "            for p in (\"emotion_classifier_model\", \"relevance_filter_model\", \"stage_2_emotion_model_training\")\n",
    "        )\n",
    "        if not has_model:\n",
    "            continue\n",
    "\n",
    "        candidates.append((ver, ts, full))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Sort by (version, timestamp) descending\n",
    "    candidates.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "# Dynamically find the latest checkpoint (excluding the current run dir)\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR, current_run_basename=VERSION_TAG)\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = latest_checkpoint\n",
    "    print(f\"‚úÖ Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(\"‚ö†Ô∏è No previous checkpoint found ‚Äî falling back to base model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"üóÇÔ∏è Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\n‚úÖ Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# üì¶ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self,\n",
    "                 processor,\n",
    "                 augment_dict=None,\n",
    "                 base_augment=None,\n",
    "                 # --- : tensor-level erasing controls (applied after processor) ---\n",
    "                 random_erasing_prob: float = 0.10,\n",
    "                 random_erasing_scale = (0.02, 0.08),\n",
    "                 skip_erasing_label_ids=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processor: HF image processor that yields pixel_value tensors\n",
    "            augment_dict: dict[int label_id -> PIL transform], class-specific\n",
    "            base_augment: fallback PIL transform when class-specific not found\n",
    "            random_erasing_prob: probability for applying tensor-level RandomErasing\n",
    "            random_erasing_scale: area range for erasing region\n",
    "            skip_erasing_label_ids: iterable of label ids to skip erasing for\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict or {}\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = base_augment or T.Compose([T.Resize((224, 224))])\n",
    "\n",
    "        # --- : tensor-level RandomErasing (applied AFTER processor) ---\n",
    "        # Keep None to disable; expects CHW tensors in [0,1]\n",
    "        self.random_erasing = (\n",
    "            T.RandomErasing(p=random_erasing_prob, scale=random_erasing_scale, value=\"random\")\n",
    "            if random_erasing_prob and random_erasing_prob > 0.0 else None\n",
    "        )\n",
    "                \n",
    "        # --- : define tensor <-> PIL helpers used in __call__ ---\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.to_pil = T.ToPILImage()\n",
    "        \n",
    "        # Labels to skip erasing for (can be overridden when constructing the collator)\n",
    "        self.skip_erasing_label_ids = set(skip_erasing_label_ids or [])\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "\n",
    "            # 1) apply class-specific PIL pipeline if present; else base PIL pipeline\n",
    "            pil_aug = self.augment_dict.get(label, self.base_augment)\n",
    "\n",
    "            img = pil_aug(rgb_image)\n",
    "\n",
    "            # ‚¨áÔ∏è INSERT THE  LINES HERE\n",
    "            # --- Tensor-level RandomErasing ---\n",
    "            img_t = self.to_tensor(img)                 # PIL ‚Üí Tensor [C,H,W]\n",
    "            if self.random_erasing is not None and label not in self.skip_erasing_label_ids:\n",
    "                img_t = self.random_erasing(img_t)      # RandomErasing on tensor\n",
    "            img = self.to_pil(img_t)  \n",
    "        \n",
    "            processed_images.append(img)\n",
    "\n",
    "        batch = self.processor(images=processed_images, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- normalize any image-like object to 3-channel RGB (PIL) ---\n",
    "def _ensure_rgb(img):\n",
    "    # If already PIL, force RGB mode\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert(\"RGB\")\n",
    "    # Else coerce to array and expand grayscale to 3 channels\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr, arr, arr], axis=-1)\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# üèãÔ∏è Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# üîÑ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None, focal_gamma=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.focal_gamma = focal_gamma  #  (None disables focal scaling)\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        ce_per_sample = -(smooth_labels * log_probs).sum(dim=1)\n",
    "\n",
    "        # : optional focal scaling\n",
    "        if self.focal_gamma is not None and self.focal_gamma > 0:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pt = (probs * smooth_labels).sum(dim=1).clamp_min(1e-6)\n",
    "            ce_per_sample = ((1 - pt) ** self.focal_gamma) * ce_per_sample\n",
    "\n",
    "        return ce_per_sample.mean()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1 loss function: focal-modulated cross-entropy (relevant-only)\n",
    "#   - We keep class weights for imbalance handling.\n",
    "#   - We add focal modulation ONLY when the ground truth is \"relevant\"\n",
    "#     to emphasize difficult positives without exploding FP on easy negatives.\n",
    "# ------------------------------------------------------------------------------\n",
    "class RelevantFocalCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, class_weights: torch.Tensor, gamma: float = 2.0, relevant_id: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            class_weights: Tensor of per-class weights (size 2 for S1)\n",
    "            gamma: focal exponent (higher -> more emphasis on hard examples)\n",
    "            relevant_id: integer id for the 'relevant' class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=class_weights, reduction=\"none\")\n",
    "        self.gamma = gamma\n",
    "        self.relevant_id = relevant_id\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes cross-entropy per-sample, then applies focal scaling only\n",
    "        for samples whose target == 'relevant'. Non-relevant samples keep vanilla CE.\n",
    "        \"\"\"\n",
    "        # base cross-entropy (per-sample)\n",
    "        ce = self.ce(logits, targets)  # shape: [B]\n",
    "\n",
    "        # compute p_t = softmax(logits)[range(B), targets]\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            p_t = probs[torch.arange(probs.size(0)), targets]  # [B]\n",
    "\n",
    "        # mask: 1 for relevant targets, 0 otherwise\n",
    "        mask = (targets == self.relevant_id).float()\n",
    "\n",
    "        # focal factor: (1 - p_t)^gamma for relevant samples; 1.0 for others\n",
    "        focal = (1.0 - p_t).pow(self.gamma) * mask + (1.0 - mask)\n",
    "\n",
    "        # mean reduced loss\n",
    "        return (focal * ce).mean()\n",
    "\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# üìä Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(eval_pred, label_names, stage_name=\"\"):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(f\"\\nüìà Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw logits/labels for later analysis like temperature scaling\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Save per-class F1/precision/recall/entropy to CSV (append per epoch)\n",
    "    f1s = [report[name][\"f1-score\"] for name in label_names]\n",
    "    recalls = [report[name][\"recall\"] for name in label_names]\n",
    "    precisions = [report[name][\"precision\"] for name in label_names]\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    \n",
    "    # Create a dictionary for entropies in the correct order for the CSV\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    # Access the trainer instance through its global-like availability during compute_metrics call\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names}\n",
    "    })\n",
    "    \n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "    # --- End Re-integration ---\n",
    "\n",
    "    # Generate and save a heatmap of the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- Re-integrated from V28 ---\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [\n",
    "        ((label_names[i], label_names[j]), cm[i][j])\n",
    "        for i in range(len(label_names))\n",
    "        for j in range(len(label_names)) if i != j and cm[i][j] > 0\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute and print entropy metrics\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nüîç Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "    # --- End Re-integration ---\n",
    "    \n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "#   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "#   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "#   - Persist T and œÑ for hierarchical inference.\n",
    "# ------------------------------------------------------------------------------\n",
    "def fit_temperature(model, eval_ds, processor, device):\n",
    "    \"\"\"\n",
    "    Fits a single temperature scalar T by minimizing NLL on eval set.\n",
    "    Returns:\n",
    "        float: learned temperature T (>= ~1e-3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        #Normalize every eval image to 3-channel RGB in fit_temperature\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "        \n",
    "            # --- Ensure 3-channel RGB for the processor ---\n",
    "            # If PIL: convert directly; if numpy/other: coerce to array and expand gray to 3-channels\n",
    "            if isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "            else:\n",
    "                arr = np.array(img)\n",
    "                if arr.ndim == 2:                      # grayscale -> stack to RGB\n",
    "                    arr = np.stack([arr, arr, arr], axis=-1)\n",
    "                img = Image.fromarray(arr.astype(np.uint8))  # ensure PIL RGB\n",
    "        \n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            logits_list.append(logits.cpu())\n",
    "            labels_list.append(lab)\n",
    "\n",
    "    logits = torch.cat(logits_list, dim=0)  # [N, 2]\n",
    "    labels = torch.tensor(labels_list)\n",
    "\n",
    "    T = torch.nn.Parameter(torch.ones(1))\n",
    "    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def _closure():\n",
    "        \"\"\"\n",
    "        LBFGS closure for temperature scaling:\n",
    "        Scales logits by 1/T, computes CE loss, backprops to adjust T.\n",
    "        \"\"\"\n",
    "        opt.zero_grad()\n",
    "        scaled = logits / T.clamp(min=1e-3)\n",
    "        loss = ce(scaled, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    opt.step(_closure)\n",
    "    return float(T.data.item())\n",
    "\n",
    "def sweep_tau(model, eval_ds, processor, device, T=1.0):\n",
    "    \"\"\"\n",
    "    Sweeps œÑ (threshold on P(relevant)) over [0.30, 0.55] to maximize F1(relevant).\n",
    "    Returns:\n",
    "        dict: {'tau', 'f1', 'prec', 'rec'} with 3-decimal rounding for logging.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "    \n",
    "            # Normalize to 3-channel RGB to avoid ndim==2 errors\n",
    "            img = _ensure_rgb(img)\n",
    "    \n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits / max(T, 1e-3)  # robustness vs tiny T\n",
    "            prob_rel = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(prob_rel)\n",
    "    \n",
    "    y_true = np.array(y_true, dtype=bool)\n",
    "    y_prob = np.array(y_prob, dtype=float)\n",
    "\n",
    "\n",
    "    best = {\"tau\": None, \"f1\": -1.0, \"prec\": None, \"rec\": None}\n",
    "    for tau in np.linspace(0.30, 0.55, 26):\n",
    "        pred = (y_prob >= tau)\n",
    "        tp = ((pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((pred == 1) & (y_true == 0)).sum()\n",
    "        fn = ((pred == 0) & (y_true == 1)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"tau\": round(float(tau), 3),\n",
    "                    \"f1\": round(float(f1), 3),\n",
    "                    \"prec\": round(float(prec), 3),\n",
    "                    \"rec\": round(float(rec), 3)}\n",
    "    return best\n",
    "    \n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# üíæ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"üíæ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"‚úÖ {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "# ==========================================================================\n",
    "#   POST-TRAINING ANALYSIS UTILITIES (OFFLINE / OPTIONAL)\n",
    "#   - Qualitative error bucketing (QE)\n",
    "#   - Attention rollout (XAI) for S1 inspection\n",
    "#   - Ablation helpers\n",
    "# ==========================================================================\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"‚ö†Ô∏è Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - ‚ùå {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - ‚úÖ {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")\n",
    "\n",
    "# --- Qualitative Error Bucketing (Stage 1) ---\n",
    "# Scans an inference CSV and tags each row with simple visual heuristics:\n",
    "# blur/shadow/occlusion/low-res. Outputs a QE report CSV for targeting data fixes.\n",
    "def variance_of_laplacian(gray):\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def is_dark(img_pil, thresh=40):\n",
    "    stat = ImageStat.Stat(img_pil.convert(\"L\"))\n",
    "    return stat.mean[0] < thresh\n",
    "\n",
    "def qualitative_buckets_s1(inference_csv, out_csv):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(inference_csv)\n",
    "    # consider only S1 mistakes if you logged them; otherwise filter low conf or S2 mismatches\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        path = r['filepath']\n",
    "        if not os.path.exists(path): continue\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        arr = np.array(img)\n",
    "        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        blur = variance_of_laplacian(gray) < 60         # motion blur proxy\n",
    "        dark = is_dark(img, thresh=45)                  # shadows proxy\n",
    "        lowres = min(img.size) < 80\n",
    "        # Cheap occlusion proxy: large random erasing candidate on face area would help, but without faces we use entropy\n",
    "        ent = cv2.calcHist([gray],[0],None,[256],[0,256]).flatten()\n",
    "        ent = -np.sum((ent/ent.sum()+1e-9)*np.log2(ent/ent.sum()+1e-9))\n",
    "        occl = ent < 4.5                                 # low entropy proxy\n",
    "        rows.append([path, r.get('true_label','?'), r.get('predicted_label','?'), r.get('confidence',np.nan),\n",
    "                     int(blur), int(dark), int(occl), int(lowres)])\n",
    "    with open(out_csv, \"w\", line=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"filepath\",\"true\",\"pred\",\"conf\",\"blur\",\"shadow\",\"occlusion\",\"lowres\"])\n",
    "        w.writerows(rows)\n",
    "    return out_csv\n",
    "\n",
    "# --- Ablation summary utility for Stage 1 ---\n",
    "# Summarizes precision/recall/F1 for S1 given (T, tau).\n",
    "def summarize_s1(eval_ds, model, processor, device, T: float, tau: float):\n",
    "    import numpy as np\n",
    "    y_true, y_prob = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "    \n",
    "            # Normalize to 3-channel RGB to avoid ndim==2 errors\n",
    "            img = _ensure_rgb(img)\n",
    "    \n",
    "            logits = model(**processor(images=img, return_tensors=\"pt\").to(device)).logits\n",
    "            logits = logits / max(T, 1e-3)\n",
    "            p = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(p)\n",
    "\n",
    "    y_true = np.array(y_true, bool); y_prob = np.array(y_prob, float)\n",
    "    pred = (y_prob >= tau)\n",
    "    tp = ((pred==1)&(y_true==1)).sum(); fp=((pred==1)&(y_true==0)).sum(); fn=((pred==0)&(y_true==1)).sum()\n",
    "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
    "    return {\"precision\":round(prec,3), \"recall\":round(rec,3), \"f1\":round(f1,3), \"tau\":tau, \"T\":T}\n",
    "\n",
    "\n",
    "# --- Attention Rollout heatmaps for ViT (offline) ---\n",
    "def vit_attention_rollout(model, inputs, discard_ratio=0.9):\n",
    "    # returns a [H,W] mask normalized 0..1; you can overlay it\n",
    "    # (Implementation omitted for brevity; use a standard attention-rollout snippet for ViT)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # --- Define specific model paths from the latest checkpoint ---\n",
    "    s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "    s2_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"emotion_classifier_model\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"‚úÖ Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    #   STAGE 1: LOAD RELEVANCE FILTER FROM SAVED ARTIFACTS\n",
    "    # ============================================================\n",
    "\n",
    "    # We are NOT retraining S1 for this run\n",
    "    TRAIN_STAGE1 = False\n",
    "    CALIBRATE_STAGE1 = False\n",
    "    \n",
    "    # Reuse the already-created V35 directory as SAVE_DIR (must be defined earlier)\n",
    "    # SAVE_DIR should be: \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\"\n",
    "    # Ensure SAVE_DIR exists and you printed it earlier like: print(f\"Using SAVE_DIR: {SAVE_DIR}\")\n",
    "    \n",
    "    # Prefer loading S1 from the SAME V35 run; fallback to V34 if missing\n",
    "    S1_LOAD_DIR = os.path.join(SAVE_DIR, \"relevance_filter_model\")\n",
    "    if not os.path.isdir(S1_LOAD_DIR):\n",
    "        S1_LOAD_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V34_20251013_211825/relevance_filter_model\"\n",
    "    print(f\"üîé Stage-1 load path: {S1_LOAD_DIR}\")\n",
    "    \n",
    "    # Device should already be defined earlier; if not, uncomment:\n",
    "    # device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Load S1 model + processor from disk (no training)\n",
    "    model_s1  = ViTForImageClassification.from_pretrained(S1_LOAD_DIR).to(device)\n",
    "    processor = AutoImageProcessor.from_pretrained(S1_LOAD_DIR)\n",
    "    print(\"‚è≠Ô∏è Loaded Stage 1 model/processor from disk (training skipped).\")\n",
    "\n",
    "    trainer_s1 = SimpleNamespace(model=model_s1, args=None)\n",
    "\n",
    "    \n",
    "    # Optional: load S1 calibration so inference gate can use it later\n",
    "    s1_calib = None\n",
    "    calib_path = os.path.join(os.path.dirname(S1_LOAD_DIR), \"stage1_calibration.json\")\n",
    "    if os.path.isfile(calib_path):\n",
    "        with open(calib_path, \"r\") as f:\n",
    "            s1_calib = json.load(f)\n",
    "        try:\n",
    "            print(f\"üìè Loaded S1 calibration: T={s1_calib.get('T')} œÑ={s1_calib.get('tau')}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(train_dataset_s2['label']))\n",
    "\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        s2_checkpoint_path, # <-- Use the specific path for the Stage 2 model\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=11),  \n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "    minority_classes_s2 = [label2id_s2[n] for n in ['disgust','questioning','contempt','fear']]\n",
    "    minority_augment_map_s2 = {lid: minority_aug for lid in minority_classes_s2}\n",
    "    \n",
    "    # very mild, targeted aug ONLY for the weakest classes\n",
    "    mild_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.95, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.05, 0.05, 0.05, 0.02),\n",
    "        T.RandomAffine(degrees=3, translate=(0.02, 0.02), scale=(0.98, 1.02)),\n",
    "    ])\n",
    "\n",
    "    # targeted mild augmentation for fragile classes\n",
    "    #     - Keep 'sadness' and 'speech_action' on very mild pipeline (no RandAug)\n",
    "    #     - Extend to 'neutral_speech' to preserve subtle mouth/phoneme cues\n",
    "    targeted_mild_classes = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['speech_action'],\n",
    "    ]\n",
    "    targeted_mild_map_s2 = {label_id: mild_aug for label_id in targeted_mild_classes}\n",
    "\n",
    "    # MERGE: single mapping passed to the collator (class id -> transform)\n",
    "    augment_dict = {**minority_augment_map_s2, **targeted_mild_map_s2}\n",
    "\n",
    "    # Use the custom loss function to turn off label smoothing for historically difficult classes.\n",
    "        # Turn OFF smoothing for the hardest classes (sharper targets) and apply mild focal emphasis\n",
    "        # Stage 2 loss: slightly softer focal gamma for fragile classes\n",
    "        # Reduces over-focus; improves probability calibration a bit.\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,\n",
    "        target_class_names=['sadness', 'speech_action'],\n",
    "        label2id_map=label2id_s2,\n",
    "        focal_gamma=1.6   # <-- NEW restored 1.6 gamma like V33\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # Adding weight decay, cosine scheduler + warmup, grad accumulation improves stability \n",
    "        # (especially on CPU/small batch) without altering your high-level flow.\n",
    "    \n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        overwrite_output_dir=True,            # <‚Äî important: reuse the folder\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=6,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "        weight_decay=0.05,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.10,\n",
    "        gradient_accumulation_steps=2,\n",
    "        dataloader_num_workers=0,\n",
    "        learning_rate=4e-5,                   # you set this later; ok to put here\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # As with Stage 1, the complex fine-tuning strategy implemented in V31 failed. \n",
    "        # This change reverts the Stage 2 training process to V30's more effective \n",
    "        # uniform learning rate strategy to restore model performance.\n",
    "    training_args_s2.learning_rate = 4e-5 # Set learning rate directly\n",
    "\n",
    "    # skip erasing for fragile classes: sadness and neutral_speech\n",
    "    # NEW added speech_action\n",
    "    fragile_ids = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['neutral_speech'],\n",
    "        label2id_s2['speech_action']\n",
    "    ]\n",
    "\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.0\n",
    "    )\n",
    "\n",
    "    # Use the CustomLossTrainer again, passing the targeted loss function.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=RELEVANT_CLASSES, \n",
    "                                stage_name=\"Stage2\"),\n",
    "        data_collator=DataCollatorWithAugmentation(\n",
    "            processor=processor,\n",
    "            augment_dict=augment_dict,                  # your merged S2 map\n",
    "            random_erasing_prob=0.10,\n",
    "            random_erasing_scale=(0.02, 0.08),\n",
    "            skip_erasing_label_ids=fragile_ids          # <-- skip erasing for fragile classes\n",
    "        ),\n",
    "        loss_fct=loss_fct_s2, # Pass custom loss function\n",
    "        callbacks=[early_stop_callback] # Keep early stopping\n",
    "    )\n",
    "\n",
    "    # --- Single sampler override (bind AFTER trainer_s2 exists) ---\n",
    "    labels = np.array(train_dataset_s2[\"label\"])\n",
    "    num_classes = len(label2id_s2)\n",
    "    \n",
    "    class_counts  = np.bincount(labels, minlength=num_classes)\n",
    "    class_weights = 1.0 / np.clip(class_counts, 1, None)\n",
    "    class_weights[label2id_s2['sadness']]       *= 2.5\n",
    "    class_weights[label2id_s2['speech_action']]  *= 2.0\n",
    "    \n",
    "    sample_weights = torch.as_tensor(class_weights[labels], dtype=torch.float)\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    def _custom_train_loader():\n",
    "        return DataLoader(\n",
    "            train_dataset_s2,\n",
    "            batch_size=training_args_s2.per_device_train_batch_size,\n",
    "            sampler=sampler,                      # no shuffle when sampler is set\n",
    "            collate_fn=trainer_s2.data_collator,\n",
    "            num_workers=0,                        # ‚Üê avoid pickling collator in notebook\n",
    "            pin_memory=False                      # CPU run; pin not needed\n",
    "        )\n",
    "    \n",
    "    trainer_s2.get_train_dataloader = _custom_train_loader\n",
    "    # --- end override ---\n",
    "\n",
    "\n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"üöÄ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"‚åõ Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\n‚úÖ Stage 2 Training Complete.\")\n",
    "    print(\"\\nüéâ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "\n",
    "    \n",
    "    # Return the trained models and processor to be used by analysis functions\n",
    "    # replace the bad return\n",
    "    return model_s1, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for  images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "def hierarchical_predict(image_paths, model_s1, model_s2, processor, device, batch_size=32):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"üî¨ Running Hierarchical Inference\"):\n",
    "        batch_paths = image_paths[i:i+batch_size]\n",
    "        images = []\n",
    "        valid_paths = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(path)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # --- Stage 1 Prediction: \n",
    "        # apply learned T and œÑ; fall back safely if file missing)\n",
    "        # --- Apply Stage-1 temperature scaling + threshold from saved calibration ---\n",
    "        calib_path = os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "        T_s1, tau = 1.0, 0.45  # safe defaults\n",
    "        if os.path.exists(calib_path):\n",
    "            try:\n",
    "                with open(calib_path, \"r\") as f:\n",
    "                    _c = json.load(f)\n",
    "                    T_s1 = float(_c.get(\"T\", 1.0))\n",
    "                    tau  = float(_c.get(\"tau\", 0.45))\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits_s1 = model_s1(**inputs).logits / max(T_s1, 1e-3)  # temperature scaling\n",
    "            probs_s1 = F.softmax(logits_s1, dim=-1)\n",
    "        \n",
    "        # Create a mask of images that were classified as 'relevant'\n",
    "            # Gate on calibrated œÑ\n",
    "        relevant_mask = (probs_s1[:, label2id_s1['relevant']] >= tau)\n",
    "        dev = logits_s1.device\n",
    "        preds_s1 = torch.where(\n",
    "            relevant_mask,\n",
    "            torch.tensor(label2id_s1['relevant'], device=dev, dtype=torch.long),\n",
    "            torch.tensor(label2id_s1['irrelevant'], device=dev, dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        # --- Stage 2 Prediction (only on relevant images) ---\n",
    "        if relevant_mask.any():\n",
    "            # Filter the input tensors to only include the relevant images\n",
    "            relevant_inputs = {k: v[relevant_mask] for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_s2 = model_s2(**relevant_inputs).logits\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)\n",
    "                confs_s2, preds_s2 = torch.max(probs_s2, dim=-1)\n",
    "\n",
    "        # --- Aggregate Results ---\n",
    "        # Loop through the original batch and assign the correct prediction\n",
    "        s2_idx = 0\n",
    "        for j in range(len(valid_paths)):\n",
    "            if relevant_mask[j]:\n",
    "                # If relevant, get the prediction from the Stage 2 model\n",
    "                pred_label = id2label_s2[preds_s2[s2_idx].item()]\n",
    "                confidence = confs_s2[s2_idx].item()\n",
    "                s2_idx += 1\n",
    "            else:\n",
    "                # If not relevant, label it and stop\n",
    "                pred_label = \"irrelevant\"\n",
    "                confidence = torch.softmax(logits_s1[j], dim=-1)[preds_s1[j]].item()\n",
    "\n",
    "            results.append({\n",
    "                \"image_path\": valid_paths[j],\n",
    "                \"prediction\": pred_label,\n",
    "                \"confidence\": confidence\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    import pandas as pd   # ensure pd is local; prevents UnboundLocalError in notebooks\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\n‚úÖ Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = REVIEW_CONF_THRESHOLD\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"üìÇ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- : Generate shortlist and curated patch CSVs for THIS run ---\n",
    "    #     - Shortlist: low-confidence items in focus classes (for targeted manual review)\n",
    "    #     - Curated patch: template CSV for corrected labels to be fed back into VNext\n",
    "    focus_classes = ['sadness','speech_action','neutral','neutral_speech','happiness']\n",
    "    \n",
    "    # Defensive: ensure the expected columns exist\n",
    "    has_pred = 'prediction' in df.columns or 'predicted_label' in df.columns\n",
    "    pred_col = 'prediction' if 'prediction' in df.columns else ('predicted_label' if 'predicted_label' in df.columns else None)\n",
    "    if pred_col is not None:\n",
    "        # Sort by confidence ascending (uncertain first)\n",
    "        df_focus = df[df[pred_col].isin(focus_classes)].copy()\n",
    "        if 'confidence' in df_focus.columns:\n",
    "            df_focus = df_focus.sort_values('confidence', ascending=True)\n",
    "    \n",
    "        short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "        patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "        # Write shortlist with a stable set of columns\n",
    "        keep_cols = [c for c in ['image_path','filepath','true_label',pred_col,'confidence'] if c in df_focus.columns]\n",
    "        df_focus[keep_cols].to_csv(short_csv, index=False)\n",
    "        print(f\"‚úÖ Shortlist written: {short_csv}\")\n",
    "    \n",
    "        # Create empty curated patch template\n",
    "        src_path_col = 'image_path' if 'image_path' in df_focus.columns else 'filepath'\n",
    "        patch_df = pd.DataFrame({\n",
    "            \"filepath\": df_focus[src_path_col],\n",
    "            \"correct_label\": \"\",\n",
    "            \"notes\": \"\"\n",
    "        })\n",
    "        patch_df.to_csv(patch_csv, index=False)\n",
    "        print(f\"‚úÖ Curated patch template written: {patch_csv}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped shortlist/patch CSVs: no predicted label column found in full log.\")\n",
    "\n",
    "    # --- : Merge this run's shortlist/patch with V32 to create canonical merged artifacts ---\n",
    "    def _merge_csvs(csv_list, key_cols, out_csv):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "    \n",
    "        # Normalize common column name variants so we can dedupe safely\n",
    "        def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            colmap = {}\n",
    "            # path columns\n",
    "            if \"image_path\" not in df.columns:\n",
    "                if \"filepath\" in df.columns:\n",
    "                    colmap[\"filepath\"] = \"image_path\"\n",
    "                elif \"path\" in df.columns:\n",
    "                    colmap[\"path\"] = \"image_path\"\n",
    "            # predicted label columns\n",
    "            if \"predicted_label\" not in df.columns:\n",
    "                if \"prediction\" in df.columns:\n",
    "                    colmap[\"prediction\"] = \"predicted_label\"\n",
    "                elif \"predicted\" in df.columns:\n",
    "                    colmap[\"predicted\"] = \"predicted_label\"\n",
    "            return df.rename(columns=colmap)\n",
    "    \n",
    "        frames = []\n",
    "        for p in csv_list:\n",
    "            if os.path.exists(p):\n",
    "                try:\n",
    "                    df = pd.read_csv(p)\n",
    "                    df = _normalize_cols(df)\n",
    "                    frames.append(df)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "        if not frames:\n",
    "            return\n",
    "    \n",
    "        merged = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "        # Keep only keys that actually exist after normalization\n",
    "        available_keys = [k for k in key_cols if k in merged.columns]\n",
    "        if not available_keys:\n",
    "            print(f\"‚ÑπÔ∏è Skipped merge for {out_csv}: none of the key columns {key_cols} exist in merged data.\")\n",
    "            return\n",
    "    \n",
    "        merged = merged.drop_duplicates(subset=available_keys, keep=\"first\")\n",
    "        merged.to_csv(out_csv, index=False)\n",
    "        print(f\"‚úÖ Merged: {out_csv} ({len(merged)} rows)\")\n",
    "\n",
    "    \n",
    "    # Paths for this run (already defined above)\n",
    "    short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "    patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "    # V32 paths (if present)\n",
    "    v32_short = os.path.join(save_dir, \"curation_shortlist_V32.csv\")\n",
    "    v32_patch = os.path.join(save_dir, \"curated_additions_V32.csv\")\n",
    "    \n",
    "    # Canonical merged outputs\n",
    "    short_merged = os.path.join(save_dir, \"curation_shortlist_merged.csv\")\n",
    "    patch_merged = os.path.join(save_dir, \"curated_additions_merged.csv\")\n",
    "    \n",
    "    # Merge (shortlist merges on [filepath, predicted_label]; patch merges on [filepath])\n",
    "    if pred_col is not None:\n",
    "        # Figure out the filepath column available\n",
    "        avail_path_cols = ['image_path','filepath']\n",
    "        path_col = next((c for c in avail_path_cols if c in df.columns), None)\n",
    "    \n",
    "        if path_col is not None:\n",
    "            _merge_csvs([v32_short, short_csv], key_cols=[path_col, pred_col], out_csv=short_merged)\n",
    "            _merge_csvs([v32_patch, patch_csv], key_cols=[path_col], out_csv=patch_merged)\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Skipped merge: no filepath column present in full log.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped merge: no predicted label column present in full log.\")\n",
    "\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs (toggleable & robust) ---\n",
    "    MINING_HARD_NEGATIVES = True  # ‚Üê set False for deployment runs\n",
    "\n",
    "    if MINING_HARD_NEGATIVES:\n",
    "        import pandas as pd\n",
    "         \n",
    "        # Prefer the freshly generated full log from THIS run; fallback to prior runs only if missing.\n",
    "        inference_log_path = full_log_path\n",
    "        if not os.path.exists(inference_log_path):\n",
    "            v33_log = os.path.join(SAVE_DIR, \"V33_full_inference_log.csv\")\n",
    "            v32_log = os.path.join(SAVE_DIR, \"V32_full_inference_log.csv\")\n",
    "            inference_log_path = v33_log if os.path.exists(v33_log) else (v32_log if os.path.exists(v32_log) else None)\n",
    "\n",
    "    \n",
    "        if not os.path.exists(inference_log_path):\n",
    "            print(\"‚è© Skipping hard-negative mining: no full inference log found.\")\n",
    "        else:\n",
    "            print(\"\\n‚õèÔ∏è  Mining for hard negative confusion pairs...\")\n",
    "            print(f\"   using: {inference_log_path}\")\n",
    "            df = pd.read_csv(inference_log_path)\n",
    "    \n",
    "            # Normalize column names between runs (V32 used 'prediction', V33 uses 'predicted_label')\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            col_true = cols.get(\"true_label\", \"true_label\")\n",
    "            col_pred = cols.get(\"predicted_label\") or cols.get(\"prediction\")\n",
    "            if col_pred is None:\n",
    "                raise RuntimeError(f\"Could not find predicted label column in {df.columns.tolist()}\")\n",
    "    \n",
    "            # (Optional) keep a stable sort by confidence descending if available\n",
    "            col_conf = cols.get(\"confidence\")\n",
    "            if col_conf:\n",
    "                df = df.sort_values(col_conf, ascending=False)\n",
    "    \n",
    "            # Which pairs to mine\n",
    "            confusion_pairs_to_mine = [\n",
    "                ('contempt', 'questioning'),\n",
    "                ('contempt', 'neutral'),\n",
    "                ('fear', 'surprise')\n",
    "            ]\n",
    "    \n",
    "            # Save to the current run folder\n",
    "            save_dir = SAVE_DIR\n",
    "    \n",
    "            for c1, c2 in confusion_pairs_to_mine:\n",
    "                mask = ((df[col_true] == c1) & (df[col_pred] == c2)) | \\\n",
    "                       ((df[col_true] == c2) & (df[col_pred] == c1))\n",
    "                hard_negatives = df.loc[mask]\n",
    "    \n",
    "                if not hard_negatives.empty:\n",
    "                    out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "                    hard_negatives.to_csv(out_path, index=False)\n",
    "                    print(f\"  - Found {len(hard_negatives)} hard negatives for ({c1} ‚Üî {c2}). Saved: {out_path}\")\n",
    "                else:\n",
    "                    print(f\"  - No hard negatives found for ({c1} ‚Üî {c2}).\")\n",
    "    else:\n",
    "        print(\"‚è© Hard-negative mining disabled (set MINING_HARD_NEGATIVES=True to enable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"üìä Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üñ•Ô∏è Using device: cpu\n",
      "‚úÖ Skipping dataset preparation, using existing directories.\n",
      "üîé Stage-1 load path: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/relevance_filter_model\n",
      "‚è≠Ô∏è Loaded Stage 1 model/processor from disk (training skipped).\n",
      "üìè Loaded S1 calibration: T=3.8392443656921387 œÑ=0.3\n",
      "\n",
      "============================================================\n",
      "  STAGE 2: TRAINING EMOTION CLASSIFIER (11-CLASS)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8302ee45a55443a9ea95b8810564a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/6175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2: 4940 training samples, 1235 validation samples.\n",
      "Stage 2 Label Distribution (Train): Counter({9: 1608, 4: 651, 8: 554, 5: 530, 0: 388, 6: 382, 1: 251, 3: 240, 10: 135, 7: 101, 2: 100})\n",
      "üöÄ Starting Stage 2 training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1854' max='1854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1854/1854 1:15:37, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.288070</td>\n",
       "      <td>0.874494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.261487</td>\n",
       "      <td>0.892308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.209864</td>\n",
       "      <td>0.897166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.199238</td>\n",
       "      <td>0.910931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.216251</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.233040</td>\n",
       "      <td>0.893927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.82      0.99      0.90        85\n",
      "      contempt       0.68      0.80      0.73        60\n",
      "       disgust       0.88      0.81      0.84        26\n",
      "          fear       0.98      0.77      0.87        71\n",
      "     happiness       0.97      0.91      0.94       167\n",
      "       neutral       0.93      0.94      0.94       135\n",
      "   questioning       0.88      0.62      0.73        92\n",
      "       sadness       0.60      0.70      0.64        40\n",
      "      surprise       0.99      0.94      0.96       147\n",
      "neutral_speech       0.87      0.90      0.89       381\n",
      " speech_action       0.63      0.84      0.72        31\n",
      "\n",
      "      accuracy                           0.87      1235\n",
      "     macro avg       0.84      0.84      0.83      1235\n",
      "  weighted avg       0.88      0.87      0.87      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - questioning ‚Üí neutral_speech: 18 instances\n",
      "  - neutral_speech ‚Üí sadness: 16 instances\n",
      "  - happiness ‚Üí neutral_speech: 14 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4988\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - questioning: entropy = 0.7313\n",
      "  - disgust: entropy = 0.7060\n",
      "  - sadness: entropy = 0.6331\n",
      "  - contempt: entropy = 0.6298\n",
      "  - fear: entropy = 0.5640\n",
      "  - neutral_speech: entropy = 0.4796\n",
      "  - neutral: entropy = 0.4659\n",
      "  - happiness: entropy = 0.4245\n",
      "  - surprise: entropy = 0.4218\n",
      "  - anger: entropy = 0.4197\n",
      "  - speech_action: entropy = 0.4195\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.94      0.94      0.94        85\n",
      "      contempt       0.71      0.80      0.75        60\n",
      "       disgust       0.76      0.85      0.80        26\n",
      "          fear       0.91      0.87      0.89        71\n",
      "     happiness       0.97      0.93      0.95       167\n",
      "       neutral       0.97      0.90      0.93       135\n",
      "   questioning       0.74      0.88      0.81        92\n",
      "       sadness       0.84      0.53      0.65        40\n",
      "      surprise       0.99      0.96      0.97       147\n",
      "neutral_speech       0.88      0.91      0.89       381\n",
      " speech_action       0.82      0.74      0.78        31\n",
      "\n",
      "      accuracy                           0.89      1235\n",
      "     macro avg       0.87      0.85      0.85      1235\n",
      "  weighted avg       0.90      0.89      0.89      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 17 instances\n",
      "  - neutral_speech ‚Üí questioning: 15 instances\n",
      "  - happiness ‚Üí neutral_speech: 11 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.5247\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.7776\n",
      "  - disgust: entropy = 0.7284\n",
      "  - sadness: entropy = 0.6717\n",
      "  - neutral: entropy = 0.6271\n",
      "  - questioning: entropy = 0.6052\n",
      "  - fear: entropy = 0.5454\n",
      "  - neutral_speech: entropy = 0.5304\n",
      "  - anger: entropy = 0.4623\n",
      "  - speech_action: entropy = 0.3993\n",
      "  - surprise: entropy = 0.3988\n",
      "  - happiness: entropy = 0.3838\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.91      0.96      0.94        85\n",
      "      contempt       0.77      0.73      0.75        60\n",
      "       disgust       0.88      0.85      0.86        26\n",
      "          fear       0.85      0.90      0.88        71\n",
      "     happiness       0.97      0.93      0.95       167\n",
      "       neutral       0.96      0.94      0.95       135\n",
      "   questioning       0.79      0.86      0.82        92\n",
      "       sadness       0.80      0.70      0.75        40\n",
      "      surprise       0.98      0.96      0.97       147\n",
      "neutral_speech       0.90      0.90      0.90       381\n",
      " speech_action       0.66      0.81      0.72        31\n",
      "\n",
      "      accuracy                           0.90      1235\n",
      "     macro avg       0.86      0.87      0.86      1235\n",
      "  weighted avg       0.90      0.90      0.90      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - happiness ‚Üí neutral_speech: 12 instances\n",
      "  - contempt ‚Üí questioning: 10 instances\n",
      "  - sadness ‚Üí neutral_speech: 10 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.5390\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.7410\n",
      "  - disgust: entropy = 0.6709\n",
      "  - neutral_speech: entropy = 0.6344\n",
      "  - neutral: entropy = 0.5784\n",
      "  - sadness: entropy = 0.5610\n",
      "  - questioning: entropy = 0.5258\n",
      "  - fear: entropy = 0.4787\n",
      "  - happiness: entropy = 0.4449\n",
      "  - anger: entropy = 0.4191\n",
      "  - surprise: entropy = 0.4002\n",
      "  - speech_action: entropy = 0.3364\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.91      0.98      0.94        85\n",
      "      contempt       0.81      0.83      0.82        60\n",
      "       disgust       0.96      0.85      0.90        26\n",
      "          fear       0.85      0.90      0.88        71\n",
      "     happiness       0.95      0.98      0.96       167\n",
      "       neutral       0.96      0.97      0.96       135\n",
      "   questioning       0.90      0.84      0.87        92\n",
      "       sadness       0.81      0.62      0.70        40\n",
      "      surprise       0.99      0.95      0.97       147\n",
      "neutral_speech       0.91      0.90      0.91       381\n",
      " speech_action       0.70      0.84      0.76        31\n",
      "\n",
      "      accuracy                           0.91      1235\n",
      "     macro avg       0.88      0.88      0.88      1235\n",
      "  weighted avg       0.91      0.91      0.91      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 12 instances\n",
      "  - neutral_speech ‚Üí speech_action: 9 instances\n",
      "  - neutral_speech ‚Üí happiness: 8 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.4800\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.6369\n",
      "  - disgust: entropy = 0.6329\n",
      "  - sadness: entropy = 0.6181\n",
      "  - questioning: entropy = 0.5439\n",
      "  - neutral: entropy = 0.5224\n",
      "  - neutral_speech: entropy = 0.5146\n",
      "  - fear: entropy = 0.4654\n",
      "  - anger: entropy = 0.4067\n",
      "  - surprise: entropy = 0.4014\n",
      "  - happiness: entropy = 0.3597\n",
      "  - speech_action: entropy = 0.3257\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.92      0.93      0.92        85\n",
      "      contempt       0.69      0.85      0.76        60\n",
      "       disgust       0.88      0.81      0.84        26\n",
      "          fear       0.85      0.94      0.89        71\n",
      "     happiness       0.97      0.95      0.96       167\n",
      "       neutral       0.98      0.89      0.93       135\n",
      "   questioning       0.76      0.86      0.81        92\n",
      "       sadness       0.79      0.55      0.65        40\n",
      "      surprise       0.98      0.97      0.97       147\n",
      "neutral_speech       0.90      0.89      0.90       381\n",
      " speech_action       0.76      0.81      0.78        31\n",
      "\n",
      "      accuracy                           0.89      1235\n",
      "     macro avg       0.86      0.86      0.86      1235\n",
      "  weighted avg       0.90      0.89      0.90      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - sadness ‚Üí neutral_speech: 14 instances\n",
      "  - neutral_speech ‚Üí questioning: 12 instances\n",
      "  - neutral ‚Üí contempt: 10 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.5179\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - sadness: entropy = 0.7286\n",
      "  - disgust: entropy = 0.7022\n",
      "  - neutral: entropy = 0.6966\n",
      "  - contempt: entropy = 0.6067\n",
      "  - neutral_speech: entropy = 0.5362\n",
      "  - questioning: entropy = 0.5295\n",
      "  - anger: entropy = 0.4768\n",
      "  - surprise: entropy = 0.4404\n",
      "  - fear: entropy = 0.4069\n",
      "  - happiness: entropy = 0.3887\n",
      "  - speech_action: entropy = 0.3141\n",
      "\n",
      "üìà Classification Report for Stage2:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "         anger       0.92      0.93      0.92        85\n",
      "      contempt       0.69      0.80      0.74        60\n",
      "       disgust       0.81      0.81      0.81        26\n",
      "          fear       0.86      0.90      0.88        71\n",
      "     happiness       0.98      0.96      0.97       167\n",
      "       neutral       0.97      0.90      0.93       135\n",
      "   questioning       0.75      0.85      0.80        92\n",
      "       sadness       0.81      0.65      0.72        40\n",
      "      surprise       0.97      0.95      0.96       147\n",
      "neutral_speech       0.91      0.90      0.90       381\n",
      " speech_action       0.71      0.81      0.76        31\n",
      "\n",
      "      accuracy                           0.89      1235\n",
      "     macro avg       0.85      0.86      0.85      1235\n",
      "  weighted avg       0.90      0.89      0.90      1235\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral_speech ‚Üí questioning: 11 instances\n",
      "  - sadness ‚Üí neutral_speech: 10 instances\n",
      "  - neutral ‚Üí contempt: 8 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.5065\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - neutral: entropy = 0.6673\n",
      "  - disgust: entropy = 0.6530\n",
      "  - contempt: entropy = 0.6387\n",
      "  - sadness: entropy = 0.6136\n",
      "  - neutral_speech: entropy = 0.5201\n",
      "  - questioning: entropy = 0.4865\n",
      "  - fear: entropy = 0.4686\n",
      "  - anger: entropy = 0.4538\n",
      "  - surprise: entropy = 0.4440\n",
      "  - happiness: entropy = 0.3854\n",
      "  - speech_action: entropy = 0.3602\n",
      "‚åõ Stage 2 training took: 01:15:39\n",
      "üíæ Saving emotion_classifier_model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\n",
      "‚úÖ emotion_classifier_model saved successfully.\n",
      "\n",
      "‚úÖ Stage 2 Training Complete.\n",
      "\n",
      "üéâ Hierarchical Training Pipeline Finished Successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer_s1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Step 1: Execute Training Pipeline ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# The main function now returns the trained models and processor\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model_s1, model_s2, processor \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# --- Step 2: Run Post-Training Analysis & Curation ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RUN_INFERENCE:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# This function runs the full inference pass and generates logs for review.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# It uses the in-memory models returned from main().\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 248\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müéâ Hierarchical Training Pipeline Finished Successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Return the trained models and processor to be used by analysis functions\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_s1\u001b[49m\u001b[38;5;241m.\u001b[39mmodel, trainer_s2\u001b[38;5;241m.\u001b[39mmodel, processor\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer_s1' is not defined"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the device once for the entire script run.\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # --- Step 1: Execute Training Pipeline ---\n",
    "    # The main function now returns the trained models and processor\n",
    "    model_s1, model_s2, processor = main(device)\n",
    "    \n",
    "    # --- Step 2: Run Post-Training Analysis & Curation ---\n",
    "    if RUN_INFERENCE:\n",
    "        # This function runs the full inference pass and generates logs for review.\n",
    "        # It uses the in-memory models returned from main().\n",
    "        run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "    \n",
    "    # --- Step 3: Run Final Model Checks ---\n",
    "    # Check if the model is ready for \"deployment\" based on F1 scores\n",
    "    stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "    check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "    \n",
    "    # --- Step 4: Calibrate the Stage 2 Model ---\n",
    "    logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "    labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "    \n",
    "    if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  CALIBRATING STAGE 2 MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        logits_s2 = np.load(logits_s2_path)\n",
    "        labels_s2 = np.load(labels_s2_path)\n",
    "        \n",
    "        optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "        print(f\"‚úÖ Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "        # plot_reliability_diagram(logits_s2, labels_s2, optimal_temp, SAVE_DIR, VERSION, \"Stage2\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Skipping calibration, logits/labels files for Stage 2 not found.\")\n",
    "\n",
    "    # COME BACK LATER TO MAKE DYNAMIC AND AUTOMATED LOADING OF PATH\n",
    "    # --- Step 5: (Hypothetical) Run Ensemble Analysis ---\n",
    "    # Use the saved V32 artifacts as the \"previous\" models for ensembling\n",
    "    v_prev_path = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V32_20251008_115114\"\n",
    "    \n",
    "    if os.path.exists(v_prev_path):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"  RUNNING HIERARCHICAL ENSEMBLE ANALYSIS (current + V32)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the older V32 models for the ensemble\n",
    "        s1_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"relevance_filter_model\")\n",
    "        ).to(device).eval()\n",
    "        s2_model_prev = AutoModelForImageClassification.from_pretrained(\n",
    "            os.path.join(v_prev_path, \"emotion_classifier_model\")\n",
    "        ).to(device).eval()\n",
    "        \n",
    "        # Use the in-memory models from THIS run (e.g., V33 when you launch it)\n",
    "        # Assumes you have model_s1 and model_s2 already defined in memory\n",
    "        s1_models_ensemble = [model_s1, s1_model_prev]\n",
    "        s2_models_ensemble = [model_s2, s2_model_prev]\n",
    "\n",
    "        # : auto-pick a real image from ANY non-empty predicted-class folder\n",
    "        review_root = os.path.join(v_prev_path, \"review_candidates_by_predicted_class\")\n",
    "        example_image_path = None\n",
    "        if os.path.isdir(review_root):\n",
    "            for cls in os.listdir(review_root):\n",
    "                cls_dir = os.path.join(review_root, cls)\n",
    "                if os.path.isdir(cls_dir):\n",
    "                    imgs = [f for f in os.listdir(cls_dir) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\"))]\n",
    "                    if imgs:\n",
    "                        example_image_path = os.path.join(cls_dir, imgs[0])\n",
    "                        break\n",
    "    \n",
    "        if example_image_path and os.path.exists(example_image_path):\n",
    "            prediction, confidence = hierarchical_ensemble_predict(\n",
    "                example_image_path, processor, s1_models_ensemble, s2_models_ensemble, device\n",
    "            )\n",
    "            print(f\"Ensemble prediction for {Path(example_image_path).name}: {prediction} (Confidence: {confidence:.2f})\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Skipping ensemble demo: no example image found under 'review_candidates_by_predicted_class'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c815005f-8389-4255-9bf7-052d0f2e5735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded S1 model/processor from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/relevance_filter_model\n",
      "üß™ Using S1 calibration: T=3.8392443656921387 œÑ=0.3\n"
     ]
    }
   ],
   "source": [
    "# --- Load Stage-1 model + processor from the V35 folder (no retrain) ---\n",
    "import os, json\n",
    "from types import SimpleNamespace\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "s1_dir = os.path.join(SAVE_DIR, \"relevance_filter_model\")\n",
    "\n",
    "model_s1 = ViTForImageClassification.from_pretrained(\n",
    "    s1_dir,\n",
    "    num_labels=2,\n",
    "    label2id=label2id_s1,\n",
    "    id2label=id2label_s1\n",
    ").to(device)\n",
    "\n",
    "# ensure we have a processor in scope (prefer S1‚Äôs export if needed)\n",
    "try:\n",
    "    processor\n",
    "except NameError:\n",
    "    processor = None\n",
    "if processor is None or not isinstance(processor, ViTImageProcessor):\n",
    "    processor = ViTImageProcessor.from_pretrained(s1_dir)\n",
    "\n",
    "# tiny shim to satisfy any downstream references to trainer_s1.model\n",
    "trainer_s1 = SimpleNamespace(model=model_s1, args=None)\n",
    "print(\"‚úÖ Loaded S1 model/processor from:\", s1_dir)\n",
    "\n",
    "# (optional) load S1 calibration (T, œÑ)\n",
    "calib_path = os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "with open(calib_path, \"r\") as f:\n",
    "    calib = json.load(f)\n",
    "T_cal = float(calib.get(\"T\", 1.0))\n",
    "tau   = float(calib.get(\"tau\", REVIEW_CONF_THRESHOLD))\n",
    "print(f\"üß™ Using S1 calibration: T={T_cal} œÑ={tau}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fbcc932-5a98-49f9-aafe-ed3771b695a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\n",
      "============================================================\n",
      "Found 26902 images to process for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üî¨ Running Hierarchical Inference: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 841/841 [23:55<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Full inference log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/V35_20251014_162112_full_inference_log.csv\n",
      "\n",
      "Found 5850 images below 0.85 confidence for review.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting review images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5850/5850 [00:02<00:00, 2857.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Sorted review images into folders at: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/review_candidates_by_predicted_class\n",
      "‚úÖ Shortlist written: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/curation_shortlist_V35_20251014_162112.csv\n",
      "‚úÖ Curated patch template written: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/curated_additions_V35_20251014_162112.csv\n",
      "‚úÖ Merged: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/curation_shortlist_merged.csv (2126 rows)\n",
      "‚úÖ Merged: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/curated_additions_merged.csv (2126 rows)\n",
      "\n",
      "‚õèÔ∏è  Mining for hard negative confusion pairs...\n",
      "   using: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/V35_20251014_162112_full_inference_log.csv\n",
      "  - No hard negatives found for (contempt ‚Üî questioning).\n",
      "  - Found 4 hard negatives for (contempt ‚Üî neutral). Saved: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/hard_negatives_contempt_vs_neutral.csv\n",
      "  - No hard negatives found for (fear ‚Üî surprise).\n",
      "‚úÖ Full inference log saved ‚Üí /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112/V35_full_inference_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Signature-safe call to run_post_training_analysis (no retrain) ===\n",
    "import os, inspect\n",
    "\n",
    "# ----- Inputs we already have in memory/files -----\n",
    "MODELS_KIT = {\n",
    "    \"model_s1\": model_s1,                    # loaded S1\n",
    "    \"model_s2\": trainer_s2.model,            # loaded S2\n",
    "    \"processor\": processor,                  # ViT processor\n",
    "    \"device\": device,                        # torch.device(\"cpu\") or cuda\n",
    "    \"base_dataset_path\": BASE_DATASET_PATH,  # your 14-folder dataset root\n",
    "    \"version\": VERSION,                      # e.g., \"V35_20251014_162112\"\n",
    "    # Preferred outputs/controls\n",
    "    \"save_dir\": SAVE_DIR,\n",
    "    \"full_log_filename\": \"V35_full_inference_log.csv\",\n",
    "    \"review_dir\": os.path.join(SAVE_DIR, \"review_candidates_by_predicted_class\"),\n",
    "    \"conf_threshold\": REVIEW_CONF_THRESHOLD,\n",
    "    \"temperature\": T_cal,                    # from stage1_calibration.json\n",
    "    \"tau\": tau,                              # from stage1_calibration.json\n",
    "    \"make_shortlists\": False,                # dedupe: keep in curation nb\n",
    "    \"make_mining_pairs\": False,\n",
    "}\n",
    "\n",
    "sig = inspect.signature(run_post_training_analysis)\n",
    "pos_args = []\n",
    "kw_args  = {}\n",
    "\n",
    "for name, param in sig.parameters.items():\n",
    "    if param.kind in (param.POSITIONAL_ONLY, param.POSITIONAL_OR_KEYWORD):\n",
    "        if name in MODELS_KIT:\n",
    "            pos_args.append(MODELS_KIT[name])\n",
    "        elif param.default is not inspect._empty:\n",
    "            pos_args.append(param.default)\n",
    "        else:\n",
    "            raise TypeError(f\"Missing required positional argument for {name}\")\n",
    "    elif param.kind == param.KEYWORD_ONLY:\n",
    "        if name in MODELS_KIT:\n",
    "            kw_args[name] = MODELS_KIT[name]\n",
    "    # (*args/**kwargs not needed here)\n",
    "\n",
    "result = run_post_training_analysis(*pos_args, **kw_args)\n",
    "\n",
    "# Confirm where the log landed; fall back to our expected default name\n",
    "out_name = kw_args.get(\"full_log_filename\", MODELS_KIT[\"full_log_filename\"])\n",
    "print(f\"‚úÖ Full inference log saved ‚Üí {os.path.join(SAVE_DIR, out_name)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25187923-dc05-4f30-9dbb-fc199c33c811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
