{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c218a024-f8fe-4354-a15e-85a55fadeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#  V36  -  Loader-order fixes; notebook-stable training; curation I/O de-dup\n",
    "#  Summary: bind sampler after trainer; set num_workers=0; keep overwrite_output_dir=True;\n",
    "#           remove in-training shortlist/mining; add gentle S1 pose tolerance;\n",
    "#           keep œÑ-calibration flow; tighten inference policy hooks.\n",
    "# ==============================================================================\n",
    "\n",
    "# V35 to V36 changes:\n",
    "    # overview: Resolve residual S2 loader ordering issues & notebook worker crashes;\n",
    "    #           make training consume curation artifacts (don‚Äôt create them);\n",
    "    #           small S1 augmentation nudge for fewer false ‚Äúirrelevant‚Äù; keep calibration.\n",
    "\n",
    "    # section #0 (imports / housekeeping):\n",
    "    #   - Ensure dataclass & other imports are co-located; clean, DRY module list.\n",
    "\n",
    "    # section #5 (Stage 2 ‚Äî Trainer & Sampler order):\n",
    "    #   - Reaffirm single sampler override bound AFTER `trainer_s2 = CustomLossTrainer(...)`.\n",
    "    #     (Removes any early/duplicate sampler binds; prevents NameError and shuffle conflicts.)\n",
    "    #     Rationale: `trainer_s2` must exist before overriding `get_train_dataloader`.\n",
    "\n",
    "    # section #5 (TrainingArguments stability):\n",
    "    #   - Set/keep `dataloader_num_workers=0`, `overwrite_output_dir=True` for notebook safety\n",
    "    #     and clean reruns of `stage_2_emotion_model_training/`.\n",
    "    #     Expected: no worker-pickling crashes; deterministic reuse of the S2 output path.\n",
    "\n",
    "    # section #5 (Early stopping explicitness):\n",
    "    #   - Define `early_stop_callback = EarlyStoppingCallback(patience=2, threshold=0.0)`\n",
    "    #     explicitly in-script (no implicit references).\n",
    "\n",
    "    # section #4 (Stage 1 augmentation ‚Äì pose tolerance):\n",
    "    #   - Add a tiny `RandomAffine(degrees=6, translate=(0.03,0.03), scale=(0.97,1.03))`\n",
    "    #     to `strong_pos_aug` for S1‚Äôs relevant class only‚Äîreduces false ‚Äúirrelevant‚Äù\n",
    "    #     on near-frontal, slightly tilted faces.\n",
    "    #     Expected Impact: lower S1 false negatives at the gate without over-augmenting.\n",
    "\n",
    "    # section #4 (Stage 1 calibration / œÑ sweep):\n",
    "    #   - Keep the temperature scaling + œÑ-sweep block that persists `stage1_calibration.json`\n",
    "    #     (T, œÑ) for hierarchical inference. (Range and persistence intact.)\n",
    "\n",
    "    # section #6 (Curation IO policy):\n",
    "    #   - Do not generate shortlists/mining during training. Set:\n",
    "    #         GENERATE_TRAINING_SHORTLISTS = False\n",
    "    #         GENERATE_MINING_PAIRS       = False\n",
    "    #     and print informative messages; curation stays in the notebook.\n",
    "    #     Expected: single source of truth for curation artifacts; less duplication.\n",
    "\n",
    "    # section #6 (Post-inference organization):\n",
    "    #   - Retain ‚Äúreview_candidates_by_predicted_class‚Äù organization for low-confidence items,\n",
    "    #     driven by a configurable `REVIEW_CONF_THRESHOLD`.\n",
    "\n",
    "    # section #5/#6 (Hierarchical inference tightening hooks):\n",
    "    #   - Integrate entropy/threshold decision signals around Stage-2 predictions so\n",
    "    #     downstream routing can mark low-confidence/uncertain items for review (the\n",
    "    #     code path that returns per-item predictions is set up to carry those fields).\n",
    "\n",
    "# Sections intentionally unchanged (vs V35):\n",
    "    #   - Stage 2 targeted loss & sampler philosophy remain (single override, modest boosts),\n",
    "    #     preserving the V35 robustness improvements on fragile labels.\n",
    "    #   - Diagnostics: per-epoch metrics CSVs + confusion matrices preserved.\n",
    "    #   - Overall œÑ-calibration flow for S1 remains the same end-to-end.\n",
    "\n",
    "# Expected net effects (V36):\n",
    "    #   - Training is notebook-stable: no DataLoader worker crashes; no sampler ordering issues.\n",
    "    #   - Fewer false ‚Äúirrelevant‚Äù gates thanks to tiny S1 pose tolerance.\n",
    "    #   - Cleaner artifact discipline: training does not emit shortlist/mining; notebooks own curation.\n",
    "    #   - Inference path carries clearer review signals (confidence/entropy) for downstream routing.\n",
    "\n",
    "# Notes:\n",
    "    #   - Keep curation (shortlists, patches, hard negatives) in the dedicated notebook; V36‚Äôs\n",
    "    #     training script only consumes those artifacts for the next run.\n",
    "    #   - If a fallback (no `stage1_calibration.json`) is ever hit, ensure the default œÑ used in\n",
    "    #     your inference helper matches historical calibration (‚âà0.30) to avoid over-gating.\n",
    "\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0374c9bd-0bc9-4eac-b109-409c78b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# WORKAROUND for PyTorch MPS bug\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# Standard Library Imports\n",
    "import datasets\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import json as json_mod\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, cv2\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash, hex_to_hash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageStat, ExifTags, UnidentifiedImageError\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    RandAugment,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    ViTForImageClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaf9e9c4-f1cb-4d78-bf95-780b56f8268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dynamically loading latest checkpoint: V35_20251014_162112\n",
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V38_20251017_131749\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "\n",
    "# --- üìÇ Core Paths ---\n",
    "# This is the root directory containing your original 14-class dataset structure.\n",
    "BASE_DATASET_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfaga_dataset_14_labels\"\n",
    "# This is the root directory where all outputs (models, logs, prepared datasets) will be saved.\n",
    "OUTPUT_ROOT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "\n",
    "# --- ‚öôÔ∏è Run Configuration ---\n",
    "# default safer for daily dev runs; flip to True when you want full-corpus inference\n",
    "RUN_INFERENCE = True\n",
    "# default safer; run once when dataset layout changes\n",
    "PREPARE_DATASETS = False\n",
    "\n",
    "# Curation/Artifacts policy\n",
    "USE_EXTERNAL_CURATIONS = True\n",
    "CURATION_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V34_20251013_211825\"\n",
    "EXTERNAL_PATCH = os.path.join(CURATION_DIR, \"patch_V35.csv\")  # produced by the curation nb\n",
    "\n",
    "# --- Smoke test mode (no training) ---\n",
    "SMOKE_TEST_ONLY = True\n",
    "\n",
    "# Point smoke test to a finished export (V35 folder)\n",
    "SMOKE_CHECKPOINT_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\"\n",
    "\n",
    "# Finds the most recent V* model directory based on modification time.\n",
    "# VERSION_TAG is the folder name the script just created for this run, e.g. \"V35_20251014_161418\"\n",
    "# Ensure VERSION_TAG is defined where you compose SAVE_DIR / OUTPUT_ROOT_DIR.\n",
    "# Example: VERSION_TAG = os.path.basename(SAVE_DIR)\n",
    "def find_latest_checkpoint(root_dir, current_run_basename=None):\n",
    "    \"\"\"\n",
    "    Return the path to the most recent *completed* run by semantic version + timestamp,\n",
    "    excluding the current run directory. Ignores folders that don't contain model artifacts.\n",
    "    Folder name pattern: V<version>_<YYYYMMDD>_<HHMMSS>  (e.g., V34_20251013_211825)\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    pat = re.compile(r\"^V(\\d+)_(\\d{8}_\\d{6})$\")  # V<num>_YYYYMMDD_HHMMSS\n",
    "\n",
    "    for d in os.listdir(root_dir):\n",
    "        full = os.path.join(root_dir, d)\n",
    "        if not (os.path.isdir(full) and d.startswith(\"V\")):\n",
    "            continue\n",
    "        if current_run_basename and d == current_run_basename:\n",
    "            continue\n",
    "\n",
    "        m = pat.match(d)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        ver = int(m.group(1))\n",
    "        ts  = m.group(2)  # sortable string\n",
    "\n",
    "        # Treat as \"completed\" only if it contains known artifacts\n",
    "        has_model = any(\n",
    "            os.path.isdir(os.path.join(full, p))\n",
    "            for p in (\n",
    "                \"emotion_classifier_model\",\n",
    "                \"relevance_filter_model\",\n",
    "                \"stage_2_emotion_model_training\",\n",
    "            )\n",
    "        )\n",
    "        if not has_model:\n",
    "            continue\n",
    "\n",
    "        candidates.append((ver, ts, full))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Sort by (version, timestamp) descending: highest V, then latest time\n",
    "    candidates.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "# --- ü§ñ Model Configuration ---\n",
    "# The pretrained Vision Transformer model from Hugging Face to be used as a base.\n",
    "BASE_MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# --- üè∑Ô∏è Dataset & Label Definitions ---\n",
    "# These lists define the structure for the hierarchical pipeline.\n",
    "# All folders listed here will be grouped into the 'relevant' class for Stage 1\n",
    "# and used for training the final 11-class classifier in Stage 2.\n",
    "RELEVANT_CLASSES = [\n",
    "    'anger', 'contempt', 'disgust', 'fear', 'happiness',\n",
    "    'neutral', 'questioning', 'sadness', 'surprise',\n",
    "    'neutral_speech', 'speech_action'\n",
    "]\n",
    "# **IMPORTANT**: Since 'unknown' is a subfolder of 'hard_case', we only need to\n",
    "# list 'hard_case' here. The script will find all images inside it recursively.\n",
    "IRRELEVANT_CLASSES = ['hard_case']\n",
    "\n",
    "# Mappings for the Stage 2 (11-class Emotion) model\n",
    "id2label_s2 = dict(enumerate(RELEVANT_CLASSES))\n",
    "label2id_s2 = {v: k for k, v in id2label_s2.items()}\n",
    "\n",
    "# Weakest-label targeting \n",
    "WEAKEST_LABEL = \"sadness\"   # <‚Äî change ONLY if a different label is sub-0.80\n",
    "WEAK_BOOST   = 2.5          # modest extra weight in the sampler (2.5‚Äì3.0 is safe)\n",
    "SKIP_ERASE_WEAK = True      # leave True to protect fine cues; set False if you want occlusion\n",
    "\n",
    "# Mappings for the Stage 1 (binary Relevance) model\n",
    "id2label_s1 = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id_s1 = {v: k for k, v in id2label_s1.items()}\n",
    "\n",
    "# single source of truth for review gating\n",
    "REVIEW_CONF_THRESHOLD = 0.85  \n",
    "\n",
    "# --- üñºÔ∏è File Handling ---\n",
    "# Defines valid image extensions and provides a function to check them.\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "# --- üî¢ Versioning and Output Directory Setup ---\n",
    "# Automatically determines the next version number (e.g., V31) and creates a timestamped output folder.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(OUTPUT_ROOT_DIR)\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(OUTPUT_ROOT_DIR, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Dynamically find the latest checkpoint to train from\n",
    "# Resolve checkpoint path (AFTER you define VERSION_TAG and OUTPUT_ROOT_DIR)\n",
    "latest_checkpoint = find_latest_checkpoint(OUTPUT_ROOT_DIR, current_run_basename=VERSION_TAG)\n",
    "if latest_checkpoint:\n",
    "    PRETRAINED_CHECKPOINT_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\"\n",
    "    print(f\"‚úÖ Dynamically loading latest checkpoint: {os.path.basename(PRETRAINED_CHECKPOINT_PATH)}\")\n",
    "else:\n",
    "    PRETRAINED_CHECKPOINT_PATH = BASE_MODEL_NAME\n",
    "    print(\"‚ö†Ô∏è No previous checkpoint found ‚Äî falling back to base model.\")\n",
    "\n",
    "\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6f3e6-6f89-4a0f-b8ba-a6badcd91bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Hierarchical Dataset Preparation\n",
    "# ----------------------------------------------------\n",
    "# This function organizes the original multi-class dataset into two separate\n",
    "# folder structures required for the two-stage training process. It recursively\n",
    "# searches through subdirectories (no matter how deep) and is smart enough to\n",
    "# skip non-image files.\n",
    "def prepare_hierarchical_datasets(base_path, output_path):\n",
    "    \n",
    "    stage1_path = os.path.join(output_path, \"stage_1_relevance_dataset\")\n",
    "    stage2_path = os.path.join(output_path, \"stage_2_emotion_dataset\")\n",
    "\n",
    "    print(f\"üóÇÔ∏è Preparing hierarchical datasets at: {output_path}\")\n",
    "\n",
    "    # --- Create Stage 1 Dataset (Relevance Filter) ---\n",
    "    print(\"\\n--- Creating Stage 1 Dataset ---\")\n",
    "    irrelevant_dest = os.path.join(stage1_path, \"0_irrelevant\")\n",
    "    relevant_dest = os.path.join(stage1_path, \"1_relevant\")\n",
    "    os.makedirs(irrelevant_dest, exist_ok=True)\n",
    "    os.makedirs(relevant_dest, exist_ok=True)\n",
    "\n",
    "    # Copy irrelevant files recursively\n",
    "    print(\"Processing 'irrelevant' classes...\")\n",
    "    for class_name in IRRELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            # Here, rglob('*') finds every file in every sub-folder.\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, irrelevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # Copy relevant files recursively\n",
    "    print(\"Processing 'relevant' classes...\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Recursively copying from '{class_name}'...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, relevant_dest)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    # --- Create Stage 2 Dataset (Emotion Classifier) ---\n",
    "    print(\"\\n--- Creating Stage 2 Dataset ---\")\n",
    "    for class_name in RELEVANT_CLASSES:\n",
    "        src_dir = Path(os.path.join(base_path, class_name))\n",
    "        dest_dir = os.path.join(stage2_path, class_name)\n",
    "\n",
    "        # Ensure destination is clean before copying\n",
    "        if os.path.exists(dest_dir):\n",
    "            shutil.rmtree(dest_dir)\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "        if src_dir.is_dir():\n",
    "            print(f\"  Copying '{class_name}' to Stage 2 directory...\")\n",
    "            for file_path in src_dir.rglob('*'):\n",
    "                 if file_path.is_file() and is_valid_image(file_path.name):\n",
    "                    shutil.copy(file_path, dest_dir)\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Source directory not found for '{class_name}'\")\n",
    "\n",
    "    print(\"\\n‚úÖ Hierarchical dataset preparation complete.\")\n",
    "    return stage1_path, stage2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f78708-27e0-4716-bc2c-36f7a485477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 3. Utility Functions & Custom Classes\n",
    "# -----------------------------------------------\n",
    "\n",
    "# --- Part A: Data Augmentation ---\n",
    "\n",
    "# üì¶ Applies augmentations and processes images on-the-fly for each batch.\n",
    "# This is a more robust approach than pre-processing the entire dataset.\n",
    "class DataCollatorWithAugmentation:\n",
    "    def __init__(self,\n",
    "                 processor,\n",
    "                 augment_dict=None,\n",
    "                 base_augment=None,\n",
    "                 # --- : tensor-level erasing controls (applied after processor) ---\n",
    "                 random_erasing_prob: float = 0.10,\n",
    "                 random_erasing_scale = (0.02, 0.08),\n",
    "                 skip_erasing_label_ids=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processor: HF image processor that yields pixel_value tensors\n",
    "            augment_dict: dict[int label_id -> PIL transform], class-specific\n",
    "            base_augment: fallback PIL transform when class-specific not found\n",
    "            random_erasing_prob: probability for applying tensor-level RandomErasing\n",
    "            random_erasing_scale: area range for erasing region\n",
    "            skip_erasing_label_ids: iterable of label ids to skip erasing for\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.augment_dict = augment_dict or {}\n",
    "        # Baseline augmentation for majority classes.\n",
    "        self.base_augment = base_augment or T.Compose([T.Resize((224, 224))])\n",
    "\n",
    "        # --- : tensor-level RandomErasing (applied AFTER processor) ---\n",
    "        # Keep None to disable; expects CHW tensors in [0,1]\n",
    "        self.random_erasing = (\n",
    "            T.RandomErasing(p=random_erasing_prob, scale=random_erasing_scale, value=\"random\")\n",
    "            if random_erasing_prob and random_erasing_prob > 0.0 else None\n",
    "        )\n",
    "                \n",
    "        # --- : define tensor <-> PIL helpers used in __call__ ---\n",
    "        self.to_tensor = T.ToTensor()\n",
    "        self.to_pil = T.ToPILImage()\n",
    "        \n",
    "        # Labels to skip erasing for (can be overridden when constructing the collator)\n",
    "        self.skip_erasing_label_ids = set(skip_erasing_label_ids or [])\n",
    "        \n",
    "    def __call__(self, features):\n",
    "        processed_images = []\n",
    "        for x in features:\n",
    "            label = x[\"label\"]\n",
    "            rgb_image = x[\"image\"].convert(\"RGB\")\n",
    "\n",
    "            # 1) apply class-specific PIL pipeline if present; else base PIL pipeline\n",
    "            pil_aug = self.augment_dict.get(label, self.base_augment)\n",
    "\n",
    "            img = pil_aug(rgb_image)\n",
    "\n",
    "            # ‚¨áÔ∏è INSERT THE  LINES HERE\n",
    "            # --- Tensor-level RandomErasing ---\n",
    "            img_t = self.to_tensor(img)                 # PIL ‚Üí Tensor [C,H,W]\n",
    "            if self.random_erasing is not None and label not in self.skip_erasing_label_ids:\n",
    "                img_t = self.random_erasing(img_t)      # RandomErasing on tensor\n",
    "            img = self.to_pil(img_t)  \n",
    "        \n",
    "            processed_images.append(img)\n",
    "\n",
    "        batch = self.processor(images=processed_images, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = torch.tensor([x[\"label\"] for x in features], dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "# --- normalize any image-like object to 3-channel RGB (PIL) ---\n",
    "def _ensure_rgb(img):\n",
    "    # If already PIL, force RGB mode\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert(\"RGB\")\n",
    "    # Else coerce to array and expand grayscale to 3 channels\n",
    "    arr = np.array(img)\n",
    "    if arr.ndim == 2:\n",
    "        arr = np.stack([arr, arr, arr], axis=-1)\n",
    "    return Image.fromarray(arr.astype(np.uint8))\n",
    "\n",
    "\n",
    "# --- Part B: Model & Training Components ---\n",
    "\n",
    "# üèãÔ∏è Defines a custom Trainer that can use either a targeted loss function or class weights.\n",
    "class CustomLossTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_fct=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fct:\n",
    "            # Stage 2 uses the custom targeted smoothing loss\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "        else:\n",
    "            # Stage 1 uses standard CrossEntropyLoss with class weights (all on CPU)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# üîÑ Implements Cross-Entropy Loss with *Targeted* Label Smoothing.\n",
    "# Smoothing is turned OFF for specified classes to encourage confident predictions. This is used for Stage 2.\n",
    "class TargetedSmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, target_class_names=None, label2id_map=None, focal_gamma=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.focal_gamma = focal_gamma  #  (None disables focal scaling)\n",
    "        if target_class_names and label2id_map:\n",
    "            self.target_class_ids = [label2id_map[name] for name in target_class_names]\n",
    "        else:\n",
    "            self.target_class_ids = []\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "            if self.target_class_ids:\n",
    "                target_mask = torch.isin(target, torch.tensor(self.target_class_ids, device=target.device))\n",
    "                if target_mask.any():\n",
    "                    sharp_labels = F.one_hot(target[target_mask], num_classes=num_classes).float()\n",
    "                    smooth_labels[target_mask] = sharp_labels\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        ce_per_sample = -(smooth_labels * log_probs).sum(dim=1)\n",
    "\n",
    "        # : optional focal scaling\n",
    "        if self.focal_gamma is not None and self.focal_gamma > 0:\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pt = (probs * smooth_labels).sum(dim=1).clamp_min(1e-6)\n",
    "            ce_per_sample = ((1 - pt) ** self.focal_gamma) * ce_per_sample\n",
    "\n",
    "        return ce_per_sample.mean()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1 loss function: focal-modulated cross-entropy (relevant-only)\n",
    "#   - We keep class weights for imbalance handling.\n",
    "#   - We add focal modulation ONLY when the ground truth is \"relevant\"\n",
    "#     to emphasize difficult positives without exploding FP on easy negatives.\n",
    "# ------------------------------------------------------------------------------\n",
    "class RelevantFocalCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, class_weights: torch.Tensor, gamma: float = 2.0, relevant_id: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            class_weights: Tensor of per-class weights (size 2 for S1)\n",
    "            gamma: focal exponent (higher -> more emphasis on hard examples)\n",
    "            relevant_id: integer id for the 'relevant' class\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ce = torch.nn.CrossEntropyLoss(weight=class_weights, reduction=\"none\")\n",
    "        self.gamma = gamma\n",
    "        self.relevant_id = relevant_id\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes cross-entropy per-sample, then applies focal scaling only\n",
    "        for samples whose target == 'relevant'. Non-relevant samples keep vanilla CE.\n",
    "        \"\"\"\n",
    "        # base cross-entropy (per-sample)\n",
    "        ce = self.ce(logits, targets)  # shape: [B]\n",
    "\n",
    "        # compute p_t = softmax(logits)[range(B), targets]\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            p_t = probs[torch.arange(probs.size(0)), targets]  # [B]\n",
    "\n",
    "        # mask: 1 for relevant targets, 0 otherwise\n",
    "        mask = (targets == self.relevant_id).float()\n",
    "\n",
    "        # focal factor: (1 - p_t)^gamma for relevant samples; 1.0 for others\n",
    "        focal = (1.0 - p_t).pow(self.gamma) * mask + (1.0 - mask)\n",
    "\n",
    "        # mean reduced loss\n",
    "        return (focal * ce).mean()\n",
    "\n",
    "\n",
    "# --- Part C: Metrics & Evaluation ---\n",
    "\n",
    "# üìä Computes metrics and generates a confusion matrix plot for each evaluation step.\n",
    "def compute_metrics_with_confusion(\n",
    "    eval_pred,\n",
    "    label_names,\n",
    "    stage_name=\"Stage2\",\n",
    "    s2_temperature: float = 1.0,\n",
    "):\n",
    "    logits, labels = eval_pred  # logits: np.ndarray, labels: np.ndarray\n",
    "\n",
    "    # ---- Stage-2 temperature (calibrated probabilities) ----\n",
    "    if stage_name.lower().startswith(\"stage2\") and (s2_temperature is not None) and (s2_temperature != 1.0):\n",
    "        logits = logits / max(1e-6, float(s2_temperature))\n",
    "\n",
    "    # softmax ‚Üí probs, preds\n",
    "    probs = torch.softmax(torch.from_numpy(logits), dim=-1).numpy()\n",
    "    preds = probs.argmax(axis=-1)\n",
    "\n",
    "    print(f\"\\nüìà Classification Report for {stage_name}:\")\n",
    "    report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    print(classification_report(labels, preds, target_names=label_names, zero_division=0))\n",
    "\n",
    "    # Save raw eval tensors for post-hoc analysis\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{stage_name}_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{stage_name}_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Per-class metrics for CSV\n",
    "    f1s        = [report[name][\"f1-score\"]   for name in label_names]\n",
    "    recalls    = [report[name][\"recall\"]     for name in label_names]\n",
    "    precisions = [report[name][\"precision\"]  for name in label_names]\n",
    "\n",
    "    # Entropy per class\n",
    "    softmax_probs = F.softmax(torch.from_numpy(logits), dim=-1)\n",
    "    entropies     = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    entropy_per_class = []\n",
    "    labels_np = np.asarray(labels)\n",
    "    for idx, class_name in enumerate(label_names):\n",
    "        mask = (labels_np == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "        else:\n",
    "            entropy_per_class.append((class_name, 0.0))\n",
    "    entropy_dict = dict(entropy_per_class)\n",
    "\n",
    "    # CSV logging (append)\n",
    "    epoch_metrics_path = os.path.join(SAVE_DIR, f\"per_class_metrics_{stage_name}.csv\")\n",
    "    active_trainer = trainer_s1 if stage_name == \"Stage1\" else trainer_s2\n",
    "    epoch = getattr(active_trainer.state, \"epoch\", None)\n",
    "\n",
    "    df_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch],\n",
    "        **{f\"f1_{n}\": [f] for n, f in zip(label_names, f1s)},\n",
    "        **{f\"recall_{n}\": [r] for n, r in zip(label_names, recalls)},\n",
    "        **{f\"precision_{n}\": [p] for n, p in zip(label_names, precisions)},\n",
    "        **{f\"entropy_{n}\": [entropy_dict[n]] for n in label_names},\n",
    "    })\n",
    "    if os.path.exists(epoch_metrics_path):\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df_row.to_csv(epoch_metrics_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    # Confusion matrix figure\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix - {stage_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_{stage_name}_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Top confused pairs\n",
    "    confusion_pairs = [((label_names[i], label_names[j]), cm[i][j])\n",
    "                       for i in range(len(label_names)) for j in range(len(label_names))\n",
    "                       if i != j and cm[i][j] > 0]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    if top_confusions:\n",
    "        print(\"\\nTop 3 confused class pairs:\")\n",
    "        for (true_label, pred_label), count in top_confusions:\n",
    "            print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    sorted_entropy = sorted(entropy_per_class, key=lambda x: x[1], reverse=True)\n",
    "    if sorted_entropy:\n",
    "        print(\"\\nüîç Class entropies (sorted):\")\n",
    "        for class_name, entropy in sorted_entropy:\n",
    "            print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    return {\"accuracy\": float((preds == labels).mean())}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "#   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "#   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "#   - Persist T and œÑ for hierarchical inference.\n",
    "# ------------------------------------------------------------------------------\n",
    "def fit_temperature(model, eval_ds, processor, device):\n",
    "    \"\"\"\n",
    "    Fits a single temperature scalar T by minimizing NLL on eval set.\n",
    "    Returns:\n",
    "        float: learned temperature T (>= ~1e-3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    logits_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        #Normalize every eval image to 3-channel RGB in fit_temperature\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "        \n",
    "            # --- Ensure 3-channel RGB for the processor ---\n",
    "            # If PIL: convert directly; if numpy/other: coerce to array and expand gray to 3-channels\n",
    "            if isinstance(img, Image.Image):\n",
    "                img = img.convert(\"RGB\")\n",
    "            else:\n",
    "                arr = np.array(img)\n",
    "                if arr.ndim == 2:                      # grayscale -> stack to RGB\n",
    "                    arr = np.stack([arr, arr, arr], axis=-1)\n",
    "                img = Image.fromarray(arr.astype(np.uint8))  # ensure PIL RGB\n",
    "        \n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits\n",
    "            logits_list.append(logits.cpu())\n",
    "            labels_list.append(lab)\n",
    "\n",
    "    logits = torch.cat(logits_list, dim=0)  # [N, 2]\n",
    "    labels = torch.tensor(labels_list)\n",
    "\n",
    "    T = torch.nn.Parameter(torch.ones(1))\n",
    "    opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50)\n",
    "    ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def _closure():\n",
    "        \"\"\"\n",
    "        LBFGS closure for temperature scaling:\n",
    "        Scales logits by 1/T, computes CE loss, backprops to adjust T.\n",
    "        \"\"\"\n",
    "        opt.zero_grad()\n",
    "        scaled = logits / T.clamp(min=1e-3)\n",
    "        loss = ce(scaled, labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    opt.step(_closure)\n",
    "    return float(T.data.item())\n",
    "\n",
    "def sweep_tau(model, eval_ds, processor, device, T=1.0):\n",
    "    \"\"\"\n",
    "    Sweep œÑ on P(relevant) over [0.28, 0.55] (0.01 steps) to maximize F1(relevant).\n",
    "    Keeps œÑ near the historically stable 0.30, but allows a slight reduction when it\n",
    "    meaningfully lifts F1 on eval.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    pos_rate = float((np.array(y_true) == True).mean())\n",
    "    print(f\"‚ÑπÔ∏è S1 calib eval prevalence (relevant rate): {pos_rate:.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "            img = _ensure_rgb(img)  # your helper\n",
    "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputs).logits / max(T, 1e-3)\n",
    "            prob_rel = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(prob_rel)\n",
    "\n",
    "    y_true = np.asarray(y_true, dtype=bool)\n",
    "    y_prob = np.asarray(y_prob, dtype=float)\n",
    "\n",
    "    best = {\"tau\": None, \"f1\": -1.0, \"prec\": None, \"rec\": None}\n",
    "    for tau in np.round(np.arange(0.28, 0.55 + 1e-9, 0.01), 2):\n",
    "        pred = (y_prob >= tau)\n",
    "        tp = ((pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((pred == 1) & (y_true == 0)).sum()\n",
    "        fn = ((pred == 0) & (y_true == 1)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "        f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0.0\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\"tau\": float(tau),\n",
    "                    \"f1\": round(float(f1), 3),\n",
    "                    \"prec\": round(float(prec), 3),\n",
    "                    \"rec\": round(float(rec), 3)}\n",
    "    return best\n",
    "    \n",
    "\n",
    "# --- Part D: Model Saving ---\n",
    "\n",
    "# Minimal file router for ad-hoc runs from the training script.\n",
    "def _route_image_to_fs(src_path: str, out_dir: Path, review_dir: Path, decision: dict):\n",
    "    \n",
    "    from pathlib import Path\n",
    "    import shutil, os\n",
    "\n",
    "    if decision.get(\"route_reason\") == \"thresholds\":\n",
    "        dest = review_dir / \"review_lowconf\"\n",
    "    elif decision.get(\"final_label\") == \"irrelevant\":\n",
    "        dest = review_dir / \"irrelevant_or_lowS1\"\n",
    "    else:\n",
    "        dest = out_dir / decision[\"final_label\"]\n",
    "\n",
    "    os.makedirs(dest, exist_ok=True)\n",
    "    shutil.copy2(src_path, dest / Path(src_path).name)\n",
    "\n",
    "\n",
    "# üíæ Saves the model and its associated processor to a specified directory.\n",
    "def save_model_and_processor(model, processor, save_dir, model_name):\n",
    "    print(f\"üíæ Saving {model_name} and processor to: {save_dir}\")\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model = model.to(\"cpu\")\n",
    "    processor.save_pretrained(model_path)\n",
    "    model.save_pretrained(model_path, safe_serialization=True)\n",
    "    print(f\"‚úÖ {model_name} saved successfully.\")\n",
    "\n",
    "\n",
    "# --- Part E: Post-Training Analysis ---\n",
    "# ==========================================================================\n",
    "#   POST-TRAINING ANALYSIS UTILITIES (OFFLINE / OPTIONAL)\n",
    "#   - Qualitative error bucketing (QE)\n",
    "#   - Attention rollout (XAI) for S1 inspection\n",
    "#   - Ablation helpers\n",
    "# ==========================================================================\n",
    "\n",
    "def check_deployment_readiness(metrics_csv_path, f1_threshold=0.80):\n",
    "    \"\"\"Analyzes the final metrics CSV to check for production readiness.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  DEPLOYMENT READINESS CHECK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(metrics_csv_path):\n",
    "        print(f\"‚ö†Ô∏è Metrics file not found at: {metrics_csv_path}\")\n",
    "        return\n",
    "\n",
    "    metrics_df = pd.read_csv(metrics_csv_path)\n",
    "    last_epoch_metrics = metrics_df.iloc[-1]\n",
    "    \n",
    "    label_names = [col.replace(\"f1_\", \"\") for col in metrics_df.columns if col.startswith(\"f1_\")]\n",
    "    \n",
    "    print(f\"Threshold: F1-Score >= {f1_threshold}\\n\")\n",
    "    \n",
    "    issues_found = False\n",
    "    for label in label_names:\n",
    "        f1_score = last_epoch_metrics.get(f\"f1_{label}\", 0)\n",
    "        if f1_score < f1_threshold:\n",
    "            print(f\"  - ‚ùå {label:<15} | F1-Score: {f1_score:.2f} (Below Threshold)\")\n",
    "            issues_found = True\n",
    "        else:\n",
    "            print(f\"  - ‚úÖ {label:<15} | F1-Score: {f1_score:.2f}\")\n",
    "            \n",
    "    if issues_found:\n",
    "        print(\"\\n Model is NOT ready for production.\")\n",
    "    else:\n",
    "        print(\"\\n Model meets the minimum F1-score threshold for all classes.\")\n",
    "\n",
    "# --- Qualitative Error Bucketing (Stage 1) ---\n",
    "# Scans an inference CSV and tags each row with simple visual heuristics:\n",
    "# blur/shadow/occlusion/low-res. Outputs a QE report CSV for targeting data fixes.\n",
    "def variance_of_laplacian(gray):\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def is_dark(img_pil, thresh=40):\n",
    "    stat = ImageStat.Stat(img_pil.convert(\"L\"))\n",
    "    return stat.mean[0] < thresh\n",
    "\n",
    "def qualitative_buckets_s1(inference_csv, out_csv):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(inference_csv)\n",
    "    # consider only S1 mistakes if you logged them; otherwise filter low conf or S2 mismatches\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        path = r['filepath']\n",
    "        if not os.path.exists(path): continue\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        arr = np.array(img)\n",
    "        gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        blur = variance_of_laplacian(gray) < 60         # motion blur proxy\n",
    "        dark = is_dark(img, thresh=45)                  # shadows proxy\n",
    "        lowres = min(img.size) < 80\n",
    "        # Cheap occlusion proxy: large random erasing candidate on face area would help, but without faces we use entropy\n",
    "        ent = cv2.calcHist([gray],[0],None,[256],[0,256]).flatten()\n",
    "        ent = -np.sum((ent/ent.sum()+1e-9)*np.log2(ent/ent.sum()+1e-9))\n",
    "        occl = ent < 4.5                                 # low entropy proxy\n",
    "        rows.append([path, r.get('true_label','?'), r.get('predicted_label','?'), r.get('confidence',np.nan),\n",
    "                     int(blur), int(dark), int(occl), int(lowres)])\n",
    "    with open(out_csv, \"w\", line=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"filepath\",\"true\",\"pred\",\"conf\",\"blur\",\"shadow\",\"occlusion\",\"lowres\"])\n",
    "        w.writerows(rows)\n",
    "    return out_csv\n",
    "\n",
    "# --- Ablation summary utility for Stage 1 ---\n",
    "# Summarizes precision/recall/F1 for S1 given (T, tau).\n",
    "def summarize_s1(eval_ds, model, processor, device, T: float, tau: float):\n",
    "    import numpy as np\n",
    "    y_true, y_prob = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ex in eval_ds:\n",
    "            img, lab = ex[\"image\"], int(ex[\"label\"])\n",
    "    \n",
    "            # Normalize to 3-channel RGB to avoid ndim==2 errors\n",
    "            img = _ensure_rgb(img)\n",
    "    \n",
    "            logits = model(**processor(images=img, return_tensors=\"pt\").to(device)).logits\n",
    "            logits = logits / max(T, 1e-3)\n",
    "            p = torch.softmax(logits, dim=-1)[0, label2id_s1['relevant']].item()\n",
    "            y_true.append(lab == label2id_s1['relevant'])\n",
    "            y_prob.append(p)\n",
    "\n",
    "    y_true = np.array(y_true, bool); y_prob = np.array(y_prob, float)\n",
    "    pred = (y_prob >= tau)\n",
    "    tp = ((pred==1)&(y_true==1)).sum(); fp=((pred==1)&(y_true==0)).sum(); fn=((pred==0)&(y_true==1)).sum()\n",
    "    prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
    "    rec  = tp/(tp+fn) if tp+fn>0 else 0.0\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
    "    return {\"precision\":round(prec,3), \"recall\":round(rec,3), \"f1\":round(f1,3), \"tau\":tau, \"T\":T}\n",
    "\n",
    "\n",
    "# --- Attention Rollout heatmaps for ViT (offline) ---\n",
    "def vit_attention_rollout(model, inputs, discard_ratio=0.9):\n",
    "    # returns a [H,W] mask normalized 0..1; you can overlay it\n",
    "    # (Implementation omitted for brevity; use a standard attention-rollout snippet for ViT)\n",
    "    pass\n",
    "\n",
    "\n",
    "def _normalize_label_maps_from_config(cfg):\n",
    "    \"\"\"\n",
    "    Normalize model config maps so we always get:\n",
    "      id2label: Dict[int, str]\n",
    "      label2id: Dict[str, int]\n",
    "    Works whether the raw dicts have str/int keys/values.\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    for k, v in getattr(cfg, \"id2label\", {}).items():\n",
    "        ik = int(k) if not isinstance(k, int) else k\n",
    "        id2label[int(ik)] = str(v)\n",
    "\n",
    "    label2id = {}\n",
    "    for k, v in getattr(cfg, \"label2id\", {}).items():\n",
    "        # here k is usually a label name (str), v is an id (int or str)\n",
    "        name = str(k)\n",
    "        iv = int(v) if not isinstance(v, int) else v\n",
    "        label2id[name] = int(iv)\n",
    "\n",
    "    # Fallback if one mapping is empty: infer from the other\n",
    "    if not id2label and label2id:\n",
    "        id2label = {vi: k for k, vi in label2id.items()}\n",
    "    if not label2id and id2label:\n",
    "        label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def _load_exports_for_smoke(checkpoint_path: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Load processor + S1/S2 models from an export folder (e.g., V35/V36),\n",
    "    and normalize label maps so downstream code doesn't care about key types.\n",
    "    \"\"\"\n",
    "    s2_dir = os.path.join(checkpoint_path, \"emotion_classifier_model\")\n",
    "    s1_dir = os.path.join(checkpoint_path, \"relevance_filter_model\")\n",
    "\n",
    "    if not os.path.isdir(s1_dir):\n",
    "        raise FileNotFoundError(f\"Missing S1 export at: {s1_dir}\")\n",
    "    if not os.path.isdir(s2_dir):\n",
    "        raise FileNotFoundError(f\"Missing S2 export at: {s2_dir}\")\n",
    "\n",
    "    # Prefer processor from S2 export (has full label set); fallback to S1 if needed\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(s2_dir)\n",
    "    except Exception:\n",
    "        processor = AutoImageProcessor.from_pretrained(s1_dir)\n",
    "\n",
    "    # Load models\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(s1_dir).to(device).eval()\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(s2_dir).to(device).eval()\n",
    "\n",
    "    # Normalize and expose label maps for S2\n",
    "    id2label_s2, label2id_s2 = _normalize_label_maps_from_config(model_s2.config)\n",
    "    globals()[\"id2label_s2\"] = id2label_s2\n",
    "    globals()[\"label2id_s2\"] = label2id_s2\n",
    "\n",
    "    # (Optional) also normalize S1 maps if you log or use them\n",
    "    id2label_s1, label2id_s1 = _normalize_label_maps_from_config(model_s1.config)\n",
    "    globals()[\"id2label_s1\"] = id2label_s1\n",
    "    globals()[\"label2id_s1\"] = label2id_s1\n",
    "\n",
    "    return model_s1, model_s2, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51d056e-d3fa-4254-a287-df4a8e98d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 4. Main Training Script\n",
    "# --------------------------\n",
    "\n",
    "def main(device):\n",
    "    # Make trainer objects accessible to metrics function\n",
    "    global trainer_s1, trainer_s2\n",
    "    \n",
    "    # --- Sanity Check for Checkpoint Path ---\n",
    "    if not os.path.exists(PRETRAINED_CHECKPOINT_PATH):\n",
    "        raise FileNotFoundError(f\"Fatal: Pretrained checkpoint not found at {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "    # --- Define specific model paths from the latest checkpoint ---\n",
    "    s1_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"relevance_filter_model\")\n",
    "    s2_checkpoint_path = os.path.join(PRETRAINED_CHECKPOINT_PATH, \"emotion_classifier_model\")\n",
    "\n",
    "    # The device is now passed in, so the local definition is removed.\n",
    "    print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # --- Step 0: Prepare Datasets ---\n",
    "    # This function copies files into the required two-stage structure.\n",
    "    # It only needs to be run once.\n",
    "    prepared_data_path = os.path.join(OUTPUT_ROOT_DIR, \"prepared_datasets\")\n",
    "    if PREPARE_DATASETS:\n",
    "        stage1_dataset_path, stage2_dataset_path = prepare_hierarchical_datasets(BASE_DATASET_PATH, prepared_data_path)\n",
    "    else:\n",
    "        stage1_dataset_path = os.path.join(prepared_data_path, \"stage_1_relevance_dataset\")\n",
    "        stage2_dataset_path = os.path.join(prepared_data_path, \"stage_2_emotion_dataset\")\n",
    "        print(\"‚úÖ Skipping dataset preparation, using existing directories.\")\n",
    "    \n",
    "    # # --- Set hardware device ---\n",
    "    # # commented out due to present mps and pytorch incompatibilities\n",
    "    # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    # print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 1: TRAIN RELEVANCE FILTER (BINARY CLASSIFIER)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 1: TRAINING RELEVANCE FILTER (BINARY CLASSIFIER)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 1 data ---\n",
    "    stage1_output_dir = os.path.join(SAVE_DIR, \"stage_1_relevance_model_training\")\n",
    "    dataset_s1 = load_dataset(\"imagefolder\", data_dir=stage1_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s1 = dataset_s1[\"train\"]\n",
    "    eval_dataset_s1 = dataset_s1[\"test\"]\n",
    "    print(f\"Stage 1: {len(train_dataset_s1)} training samples, {len(eval_dataset_s1)} validation samples.\")\n",
    "\n",
    "    # --- Configure Stage 1 model ---\n",
    "    # We load the base processor once.\n",
    "    processor = AutoImageProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "    # Load the pretrained checkpoint but replace the final layer (classifier head)\n",
    "    # for our binary (2-label) task.\n",
    "    model_s1 = ViTForImageClassification.from_pretrained(\n",
    "        s1_checkpoint_path, # <-- Use the specific path for the Stage 1 model\n",
    "        num_labels=2,\n",
    "        label2id=label2id_s1,\n",
    "        id2label=id2label_s1,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Handle Extreme Class Imbalance in Stage 1 with Class Weights ---\n",
    "    # This is critical because the 'irrelevant' class is much larger than the 'relevant' class.\n",
    "    class_weights_s1 = compute_class_weight('balanced', classes=np.unique(train_dataset_s1['label']), y=train_dataset_s1['label'])\n",
    "    class_weights_s1 = torch.tensor(class_weights_s1, dtype=torch.float).to(device)\n",
    "    print(f\"‚öñÔ∏è Stage 1 Class Weights: {class_weights_s1}\")\n",
    "\n",
    "    # --- Define Early Stopping ---\n",
    "    # Stops training if validation loss doesn't improve for 2 consecutive epochs\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001\n",
    "    )\n",
    "    \n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    training_args_s1 = TrainingArguments(\n",
    "        output_dir=stage1_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        logging_dir=os.path.join(stage1_output_dir, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 1 Trainer ---\n",
    "    # The complex discriminative learning rate and layer freezing strategy in \n",
    "        # V31 caused a severe performance drop. This change reverts Stage 1 to \n",
    "        # V30's simpler and more effective approach of using a single, uniform \n",
    "        # learning rate for the entire model, which is managed by the Hugging \n",
    "        # Face Trainer's default optimizer.\n",
    "    training_args_s1.learning_rate = 3e-5 # Set learning rate directly\n",
    "    \n",
    "    loss_fct_s1 = RelevantFocalCrossEntropy(\n",
    "        class_weights=class_weights_s1,   # <-- we KEEP and USE class_weights here\n",
    "        gamma=2.0,\n",
    "        relevant_id=label2id_s1['relevant']\n",
    "    )\n",
    "\n",
    "    strong_pos_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "        T.RandomPerspective(distortion_scale=0.05, p=0.2),\n",
    "        # NEW: small pose/tilt tolerance to reduce false \"irrelevant\" on near-frontal faces\n",
    "        T.RandomAffine(degrees=6, translate=(0.03, 0.03), scale=(0.97, 1.03)),\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Map label id -> transform. 1 == relevant\n",
    "    augment_map_s1 = { label2id_s1['relevant']: strong_pos_aug }\n",
    "\n",
    "    # Use the flexible CustomLossTrainer, passing the class weights to it.\n",
    "    # Apply stronger augmentation ONLY to the \"relevant\" class to expand coverage\n",
    "        # near the decision boundary (lighting, small occlusions, slight perspective).\n",
    "        # Keep \"irrelevant\" mild as before to avoid over-creating near-face artifacts.\n",
    "    trainer_s1 = CustomLossTrainer(\n",
    "        model=model_s1,\n",
    "        args=training_args_s1,\n",
    "        train_dataset=train_dataset_s1,\n",
    "        eval_dataset=eval_dataset_s1,\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=list(id2label_s1.values()), \n",
    "                                stage_name=\"Stage1\"),\n",
    "        data_collator=DataCollatorWithAugmentation(\n",
    "            processor=processor,\n",
    "            augment_dict=augment_map_s1,                # your existing class‚ÜíPIL map\n",
    "            random_erasing_prob=0.10,                   # enable erasing\n",
    "            random_erasing_scale=(0.02, 0.08),\n",
    "            skip_erasing_label_ids=[]                   # or [label2id_s1['relevant']] to skip\n",
    "        ),\n",
    "        loss_fct=loss_fct_s1,         # <-- : custom loss uses class_weights + focal on relevant\n",
    "        callbacks=[early_stop_callback]\n",
    "    )\n",
    "\n",
    "    # --- Train Stage 1 model ---\n",
    "    print(\"üöÄ Starting Stage 1 training...\")\n",
    "    start_time_s1 = time.time() # Record start time\n",
    "    trainer_s1.train()\n",
    "    end_time_s1 = time.time()   # Record end time\n",
    "    \n",
    "    # Calculate and print the duration\n",
    "    duration_s1 = end_time_s1 - start_time_s1\n",
    "    print(f\"‚åõ Stage 1 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s1))}\")\n",
    "    save_model_and_processor(trainer_s1.model, processor, SAVE_DIR, model_name=\"relevance_filter_model\")\n",
    "    print(\"\\n‚úÖ Stage 1 Training Complete.\")\n",
    "    \n",
    "    # Ensure the S1 return uses the trained model instance\n",
    "    model_s1 = trainer_s1.model\n",
    " \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Stage 1: Temperature scaling + threshold (œÑ) sweep\n",
    "    #   - Fit a single scalar T on eval logits (minimize NLL) to calibrate probabilities.\n",
    "    #   - Sweep œÑ in [0.30, 0.55] to pick the value that maximizes F1(relevant).\n",
    "    #   - Persist T and œÑ for hierarchical inference.\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\nüß™ Calibrating Stage 1...\")\n",
    "    T_s1 = fit_temperature(trainer_s1.model, eval_dataset_s1, processor, device)\n",
    "    best_s1 = sweep_tau(trainer_s1.model, eval_dataset_s1, processor, device, T=T_s1)\n",
    "    print(f\"‚úÖ S1 calibration done: T={T_s1:.3f} | best œÑ={best_s1['tau']} | F1={best_s1['f1']} (P={best_s1['prec']}, R={best_s1['rec']})\")\n",
    "\n",
    "    # ---- Fail-fast sanity for S1 calibration ----\n",
    "    if not isinstance(best_s1, dict) or \"tau\" not in best_s1:\n",
    "        raise RuntimeError(\"S1 calibration failed: best_s1 missing 'tau' key.\")\n",
    "    if not (0.0 <= float(best_s1[\"tau\"]) <= 1.0):\n",
    "        raise RuntimeError(f\"S1 calibration produced invalid tau: {best_s1['tau']}\")\n",
    "    \n",
    "    if not isinstance(T_s1, (float, int)) or not (0.1 <= float(T_s1) <= 100.0):\n",
    "        raise RuntimeError(f\"S1 temperature T looks suspicious: {T_s1}\")\n",
    "\n",
    "    \n",
    "    # Persist calibration for inference\n",
    "    calib_out = os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "    with open(calib_out, \"w\") as f:\n",
    "        json_mod.dump({\"T\": float(T_s1), \"tau\": float(best_s1[\"tau\"])}, f)\n",
    "    print(f\"‚úÖ Wrote S1 calibration to {calib_out}\")\n",
    "\n",
    "\n",
    "    # ==========================================================================\n",
    "    #   STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\n",
    "    # ==========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"  STAGE 2: TRAINING EMOTION CLASSIFIER ({len(RELEVANT_CLASSES)}-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Load Stage 2 data ---\n",
    "    stage2_output_dir = os.path.join(SAVE_DIR, \"stage_2_emotion_model_training\")\n",
    "    dataset_s2 = load_dataset(\"imagefolder\", data_dir=stage2_dataset_path, split='train').train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset_s2 = dataset_s2[\"train\"]\n",
    "    eval_dataset_s2 = dataset_s2[\"test\"]\n",
    "    print(f\"Stage 2: {len(train_dataset_s2)} training samples, {len(eval_dataset_s2)} validation samples.\")\n",
    "    print(\"Stage 2 Label Distribution (Train):\", Counter(train_dataset_s2['label']))\n",
    "\n",
    "    \n",
    "    # --- Optional: inject curated patch into TRAIN ONLY (no eval leak) ---\n",
    "    if USE_EXTERNAL_CURATIONS and os.path.exists(EXTERNAL_PATCH):\n",
    "        import pandas as pd\n",
    "        from datasets import Dataset\n",
    "    \n",
    "        patch_df = pd.read_csv(EXTERNAL_PATCH)\n",
    "        patch_df = patch_df[patch_df[\"label\"].isin(RELEVANT_CLASSES)]  # safety\n",
    "    \n",
    "        # build a tiny HF dataset from filepaths/labels\n",
    "        def _open_img(p): \n",
    "            try: \n",
    "                return Image.open(p).convert(\"RGB\")\n",
    "            except Exception: \n",
    "                return None\n",
    "        patch_hf = Dataset.from_dict({\n",
    "            \"image\": [ _open_img(p) for p in patch_df[\"filepath\"] ],\n",
    "            \"label\": [ label2id_s2[l] for l in patch_df[\"label\"] ],\n",
    "        }).filter(lambda ex: ex[\"image\"] is not None)\n",
    "    \n",
    "        # concat into train only\n",
    "        train_dataset_s2 = concatenate_datasets([train_dataset_s2, patch_hf]).shuffle(seed=42)\n",
    "        print(f\"üìå Injected curated patch into TRAIN: +{len(patch_hf)} samples\")\n",
    "\n",
    "    # --- Configure Stage 2 model ---\n",
    "    # Load the pretrained checkpoint again, this time with a classifier head for our 11 emotion classes.\n",
    "    model_s2 = ViTForImageClassification.from_pretrained(\n",
    "        s2_checkpoint_path, # <-- Use the specific path for the Stage 2 model\n",
    "        num_labels=len(RELEVANT_CLASSES),\n",
    "        label2id=label2id_s2,\n",
    "        id2label=id2label_s2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    # --- Define Augmentation and Loss for Stage 2 ---\n",
    "    # Apply stronger augmentation to the minority classes to help the model learn them better.\n",
    "    minority_aug = T.Compose([\n",
    "        RandAugment(num_ops=2, magnitude=11),  \n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "        T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "    ])\n",
    "    minority_classes_s2 = [label2id_s2[n] for n in ['disgust','questioning','contempt','fear']]\n",
    "    minority_augment_map_s2 = {lid: minority_aug for lid in minority_classes_s2}\n",
    "    \n",
    "    # very mild, targeted aug ONLY for the weakest classes\n",
    "    mild_aug = T.Compose([\n",
    "        T.RandomResizedCrop(224, scale=(0.95, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ColorJitter(0.05, 0.05, 0.05, 0.02),\n",
    "        T.RandomAffine(degrees=3, translate=(0.02, 0.02), scale=(0.98, 1.02)),\n",
    "    ])\n",
    "\n",
    "    # targeted mild augmentation for fragile classes\n",
    "    #     - Keep 'sadness' and 'speech_action' on very mild pipeline (no RandAug)\n",
    "    #     - Extend to 'neutral_speech' to preserve subtle mouth/phoneme cues\n",
    "    targeted_mild_classes = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['speech_action'],\n",
    "    ]\n",
    "    targeted_mild_map_s2 = {label_id: mild_aug for label_id in targeted_mild_classes}\n",
    "\n",
    "    # MERGE: single mapping passed to the collator (class id -> transform)\n",
    "    augment_dict = {**minority_augment_map_s2, **targeted_mild_map_s2}\n",
    "\n",
    "    # --- Section E: Tiny loss tweak for the weakest label (minimal & safe) ----\n",
    "    loss_fct_s2 = TargetedSmoothedCrossEntropyLoss(\n",
    "        smoothing=0.05,                      # keep global smoothing\n",
    "        target_class_names=[WEAKEST_LABEL],  # sharpen ONLY the weak class\n",
    "        label2id_map=label2id_s2,\n",
    "        focal_gamma=1.6                      # mild focal emphasis\n",
    "    )\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=2,\n",
    "        early_stopping_threshold=0.001  # tiny but non-zero improvement required\n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # Adding weight decay, cosine scheduler + warmup, grad accumulation improves stability \n",
    "        # (especially on CPU/small batch) without altering your high-level flow.\n",
    "    training_args_s2 = TrainingArguments(\n",
    "        output_dir=stage2_output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        use_cpu=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=6,                       # +1 epoch for minorities\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_accuracy\",\n",
    "        greater_is_better=True,\n",
    "        logging_dir=os.path.join(stage2_output_dir, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        dataloader_num_workers=0,      # notebook-safe\n",
    "        overwrite_output_dir=True,            # <‚Äî safe reruns into same folder\n",
    "        remove_unused_columns=False,\n",
    "        learning_rate=4e-5,\n",
    "        weight_decay=0.05,                         \n",
    "        lr_scheduler_type=\"cosine\",                \n",
    "        warmup_ratio=0.10,                        \n",
    "        gradient_accumulation_steps=2,             \n",
    "    )\n",
    "\n",
    "    # --- Set up Stage 2 Trainer ---\n",
    "    # As with Stage 1, the complex fine-tuning strategy implemented in V31 failed. \n",
    "        # This change reverts the Stage 2 training process to V30's more effective \n",
    "        # uniform learning rate strategy to restore model performance.\n",
    "    training_args_s2.learning_rate = 4e-5 # Set learning rate directly\n",
    "\n",
    "    # skip erasing for fragile classes: sadness and neutral_speech\n",
    "    # NEW added speech_action\n",
    "    fragile_ids = [\n",
    "        label2id_s2['sadness'],\n",
    "        label2id_s2['speech_action']\n",
    "    ]\n",
    "\n",
    "    # ensure weakest label is included once (idempotent)\n",
    "    weak_id = label2id_s2[WEAKEST_LABEL]\n",
    "    if SKIP_ERASE_WEAK and weak_id not in fragile_ids:\n",
    "        fragile_ids.append(weak_id)\n",
    "    \n",
    "    # Single collator instance used by the trainer\n",
    "    data_collator = DataCollatorWithAugmentation(\n",
    "        processor=processor,\n",
    "        augment_dict=augment_dict,           # your merged S2 map\n",
    "        random_erasing_prob=0.10,\n",
    "        random_erasing_scale=(0.02, 0.08),\n",
    "        skip_erasing_label_ids=fragile_ids\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  STAGE 2: TRAIN EMOTION CLASSIFIER (11-CLASS)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Hard guard: refuse to start S2 if S1 artifacts missing / not readable\n",
    "    required_s1 = [\n",
    "        os.path.join(SAVE_DIR, \"relevance_filter_model\"),\n",
    "        os.path.join(SAVE_DIR, \"stage1_calibration.json\")\n",
    "    ]\n",
    "    for p in required_s1:\n",
    "        if not os.path.exists(p):\n",
    "            raise FileNotFoundError(f\"Missing required S1 artifact before S2: {p}\")\n",
    "    \n",
    "    # Optional: timing checkpoints to avoid silent 10h stalls\n",
    "    from time import perf_counter as _t\n",
    "    _t0_s2 = _t()\n",
    "\n",
    "\n",
    "    # Use the CustomLossTrainer again, passing the targeted loss function.\n",
    "    trainer_s2 = CustomLossTrainer(\n",
    "        model=model_s2,\n",
    "        args=training_args_s2,\n",
    "        train_dataset=train_dataset_s2,\n",
    "        eval_dataset=eval_dataset_s2,\n",
    "        data_collator=data_collator,                 # <-- your collator\n",
    "        compute_metrics=partial(compute_metrics_with_confusion, \n",
    "                                label_names=RELEVANT_CLASSES, \n",
    "                                stage_name=\"Stage2\"),\n",
    "        loss_fct=loss_fct_s2,                        # <-- pass the tweaked loss here\n",
    "        callbacks=[early_stop_callback],\n",
    "    )\n",
    "\n",
    "    # --- canonical single override: AFTER trainer_s2 exists, BEFORE .train() ---\n",
    "    def _custom_train_loader():\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_dataset_s2,\n",
    "            batch_size=training_args_s2.per_device_train_batch_size,\n",
    "            sampler=sampler,                # sampler -> no shuffle\n",
    "            collate_fn=trainer_s2.data_collator,\n",
    "            num_workers=0,                  # spawn-safe in notebooks\n",
    "            pin_memory=False                # CPU path; no need to pin\n",
    "        )\n",
    "    trainer_s2.get_train_dataloader = _custom_train_loader\n",
    "\n",
    "    # --- Single, late-bound sampler override (minimal & safe) --------------------\n",
    "    # Build class counts from the in-memory train split\n",
    "    labels_np     = np.array(train_dataset_s2[\"label\"])\n",
    "    num_classes_s2 = len(label2id_s2)\n",
    "    class_counts  = np.bincount(labels_np, minlength=num_classes_s2)\n",
    "    class_weights = 1.0 / np.clip(class_counts, 1, None)\n",
    "    \n",
    "    # Apply a small extra boost ONLY to the weakest label\n",
    "    weak_id = label2id_s2[WEAKEST_LABEL]\n",
    "    class_weights[weak_id] *= WEAK_BOOST\n",
    "    \n",
    "    sample_weights = class_weights[labels_np]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.as_tensor(sample_weights, dtype=torch.float),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    # --- Train Stage 2 model ---\n",
    "    print(\"üöÄ Starting Stage 2 training...\")\n",
    "    start_time_s2 = time.time() # Record start time\n",
    "    trainer_s2.train()\n",
    "    end_time_s2 = time.time()   # Record end time\n",
    "\n",
    "    _t1_s2 = _t()\n",
    "    print(f\"‚åõ Stage 2 training took: {(_t1_s2 - _t0_s2)/3600:.2f} hours\")\n",
    "\n",
    "    # Calculate and print the duration\n",
    "    duration_s2 = end_time_s2 - start_time_s2\n",
    "    print(f\"‚åõ Stage 2 training took: {time.strftime('%H:%M:%S', time.gmtime(duration_s2))}\")\n",
    "    save_model_and_processor(trainer_s2.model, processor, SAVE_DIR, model_name=\"emotion_classifier_model\")\n",
    "    print(\"\\n‚úÖ Stage 2 Training Complete.\")\n",
    "\n",
    "    # --- Calibrate Stage 2 (scalar temperature on eval) --------------------------\n",
    "    # Uses your existing fit_temperature()/closure machinery from V36\n",
    "    import json\n",
    "    print(\"\\nüß™ Calibrating Stage 2 (scalar T on eval set)...\")\n",
    "    \n",
    "    pred_eval = trainer_s2.predict(eval_dataset_s2)\n",
    "    logits_s2_val = torch.from_numpy(pred_eval.predictions).to(device)   # [N, C]\n",
    "    labels_s2_val = torch.from_numpy(pred_eval.label_ids).long().to(device)\n",
    "    \n",
    "    # If your S2 fitter takes (logits, labels), use it directly:\n",
    "    T_s2 = fit_temperature(logits_s2_val, labels_s2_val)  # LBFGS-based; returns float\n",
    "    \n",
    "    # Save alongside the S2 export so inference can pick it up automatically\n",
    "    s2_calib_path = os.path.join(SAVE_DIR, \"emotion_classifier_model\", \"stage2_calibration.json\")\n",
    "    os.makedirs(os.path.dirname(s2_calib_path), exist_ok=True)\n",
    "    with open(s2_calib_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"T\": float(max(1e-6, T_s2)),\n",
    "            \"val_size\": int(labels_s2_val.numel()),\n",
    "            \"notes\": \"Scalar temperature via NLL on eval; seed=42\"\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ S2 calibration done: T={T_s2:.3f} ‚Üí {s2_calib_path}\")\n",
    "    \n",
    "    # (Optional) if you want the final on-screen eval to reflect calibrated T:\n",
    "    trainer_s2.compute_metrics = partial(\n",
    "        compute_metrics_with_confusion,\n",
    "        label_names=RELEVANT_CLASSES,\n",
    "        stage_name=\"Stage2\",\n",
    "        s2_temperature=float(T_s2),\n",
    "    )\n",
    "    _ = trainer_s2.evaluate(eval_dataset_s2)\n",
    "    \n",
    "    print(\"\\nüéâ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "\n",
    "    \n",
    "    # (Optional) if you want the final on-screen eval to reflect calibrated T:\n",
    "    trainer_s2.compute_metrics = partial(\n",
    "        compute_metrics_with_confusion,\n",
    "        label_names=RELEVANT_CLASSES,\n",
    "        stage_name=\"Stage2\",\n",
    "        s2_temperature=float(T_s2),\n",
    "    )\n",
    "    _ = trainer_s2.evaluate(eval_dataset_s2)\n",
    "\n",
    "    print(\"\\nüéâ Hierarchical Training Pipeline Finished Successfully.\")\n",
    "\n",
    "    \n",
    "    # Final return (works for both ‚Äútrained this run‚Äù and ‚Äúloaded‚Äù flows):\n",
    "    return model_s1, trainer_s2.model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d8e8b7-4491-4629-94b2-e1dc2fc461e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Hierarchical Inference\n",
    "# ----------------------------------\n",
    "# This function defines the two-step prediction pipeline for  images.\n",
    "# It first checks for relevance (Stage 1) and then classifies the emotion (Stage 2).\n",
    "\n",
    "# ======================== OVERWRITE: hierarchical_predict ========================\n",
    "def hierarchical_predict(\n",
    "    image_paths,\n",
    "    model_s1,\n",
    "    model_s2,\n",
    "    processor,\n",
    "    device,\n",
    "    batch_size=4,\n",
    "    s1_calibration_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs S1 ‚Üí S2 with a calibrated S1 gate. Safe under all branches:\n",
    "    - If S1 blocks, returns 'irrelevant' or 'review_lowconf' with None probs/entropy.\n",
    "    - If S1 passes, returns S2 top-1 label/conf, entropy etc.\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # --------- S1 calibration (safe defaults if missing) ---------\n",
    "    T_s1, tau = 1.0, 0.30\n",
    "    if s1_calibration_path and os.path.exists(s1_calibration_path):\n",
    "        try:\n",
    "            with open(s1_calibration_path, \"r\") as f:\n",
    "                _c = json_mod.load(f)\n",
    "            T_s1 = float(_c.get(\"T\", 1.0))\n",
    "            tau  = float(_c.get(\"tau\", 0.30))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è S1 calibration read failed ({e}); using defaults T={T_s1}, œÑ={tau}.\")\n",
    "\n",
    "    # --------- normalize label maps for S2 ---------\n",
    "    def _normalize_label_maps_from_config(cfg):\n",
    "        id2label = {}\n",
    "        for k, v in getattr(cfg, \"id2label\", {}).items():\n",
    "            ik = int(k) if not isinstance(k, int) else k\n",
    "            id2label[int(ik)] = str(v)\n",
    "        label2id = {}\n",
    "        for k, v in getattr(cfg, \"label2id\", {}).items():\n",
    "            name = str(k)\n",
    "            iv   = int(v) if not isinstance(v, int) else v\n",
    "            label2id[name] = int(iv)\n",
    "        if not id2label and label2id:\n",
    "            id2label = {vi: k for k, vi in label2id.items()}\n",
    "        if not label2id and id2label:\n",
    "            label2id = {v: k for k, v in id2label.items()}\n",
    "        return id2label, label2id\n",
    "\n",
    "    id2label_s2, label2id_s2 = _normalize_label_maps_from_config(model_s2.config)\n",
    "\n",
    "    # --------- thresholds for review routing ---------\n",
    "    from dataclasses import dataclass\n",
    "    @dataclass\n",
    "    class _Thresh:\n",
    "        base_conf: float = 0.65\n",
    "        entropy_max: float = 1.60\n",
    "        minority_classes: tuple = (\"sadness\", \"speech_action\")\n",
    "        minority_conf: float = 0.90\n",
    "\n",
    "    thr_cfg = _Thresh()\n",
    "\n",
    "    results = []\n",
    "    model_s1.eval(); model_s2.eval()\n",
    "\n",
    "    # simple micro-batching over a flat list of paths\n",
    "    def _chunks(seq, n):\n",
    "        for i in range(0, len(seq), n):\n",
    "            yield seq[i:i+n]\n",
    "\n",
    "    for batch_paths in _chunks(list(image_paths), batch_size):\n",
    "        # load images\n",
    "        imgs = []\n",
    "        for p in batch_paths:\n",
    "            img = Image.open(p).convert(\"RGB\") if hasattr(Image, \"open\") else None\n",
    "            if img is None:\n",
    "                raise RuntimeError(f\"Failed to open image: {p}\")\n",
    "            imgs.append(img)\n",
    "\n",
    "        # ---- S1 forward (with temperature) ----\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=imgs, return_tensors=\"pt\").to(device)\n",
    "            logits_s1 = model_s1(**inputs).logits / max(T_s1, 1e-6)\n",
    "            probs_s1 = F.softmax(logits_s1, dim=-1)  # [B,2]\n",
    "\n",
    "        # model_s1.config.label2id may be {'irrelevant':0,'relevant':1} or similar\n",
    "        # find index for 'relevant':\n",
    "        # normalize S1 maps similarly:\n",
    "        def _norm_map(cfg):\n",
    "            m = {}\n",
    "            for k, v in getattr(cfg, \"label2id\", {}).items():\n",
    "                name = str(k); vid = int(v) if not isinstance(v, int) else v\n",
    "                m[name] = int(vid)\n",
    "            return m\n",
    "        s1_label2id = _norm_map(model_s1.config)\n",
    "        rel_id = s1_label2id.get(\"relevant\", 1)  # default to 1\n",
    "\n",
    "        p_rel = probs_s1[:, rel_id]  # [B]\n",
    "\n",
    "        # ---- route each item ----\n",
    "        for idx, (img_path, p_relevant) in enumerate(zip(batch_paths, p_rel)):\n",
    "            p_relevant = float(p_relevant.item())\n",
    "            passed_gate = (p_relevant >= tau)\n",
    "\n",
    "            entry = {\n",
    "                \"image_path\": img_path,\n",
    "                \"stage1_prob_relevant\": p_relevant,\n",
    "                \"stage1_tau\": tau,\n",
    "            }\n",
    "\n",
    "            if not passed_gate:\n",
    "                # Blocked at S1: no S2 call\n",
    "                entry.update({\n",
    "                    \"prediction\": \"irrelevant\",\n",
    "                    \"confidence\": 1.0 - p_relevant,  # confidence in the 'irrelevant' decision\n",
    "                    \"entropy\": None,\n",
    "                    \"topk_probs\": None,\n",
    "                    \"route_reason\": \"s1_gate\",\n",
    "                })\n",
    "                results.append(entry)\n",
    "                continue\n",
    "\n",
    "            # ---- S2 forward (runs only for S1-passed items) ----\n",
    "            with torch.no_grad():\n",
    "                # Reuse the already prepared batch inputs; slice idx to single\n",
    "                single_inputs = {k: v[idx:idx+1] for k, v in inputs.items()}\n",
    "                logits_s2 = model_s2(**single_inputs).logits  # [1,C]\n",
    "                probs_s2 = F.softmax(logits_s2, dim=-1)[0].cpu().numpy()  # [C]\n",
    "\n",
    "            # top-1\n",
    "            top1_id = int(probs_s2.argmax())\n",
    "            label = id2label_s2.get(top1_id, str(top1_id))\n",
    "            conf = float(probs_s2[top1_id])\n",
    "\n",
    "            # entropy (Shannon, natural log)\n",
    "            _eps = 1e-12\n",
    "            entropy = float(-_np.sum(probs_s2 * _np.log(probs_s2 + _eps)))\n",
    "\n",
    "            # per-class decision threshold\n",
    "            thr = thr_cfg.minority_conf if label in thr_cfg.minority_classes else thr_cfg.base_conf\n",
    "            if (conf < thr) or (entropy > thr_cfg.entropy_max):\n",
    "                final_label = \"review_lowconf\"\n",
    "                route_reason = \"thresholds\"\n",
    "            else:\n",
    "                final_label = label\n",
    "                route_reason = \"passed\"\n",
    "\n",
    "            entry.update({\n",
    "                \"prediction\": final_label,\n",
    "                \"confidence\": conf,\n",
    "                \"entropy\": entropy,\n",
    "                \"topk_probs\": probs_s2.tolist(),\n",
    "                \"top1_label\": label,\n",
    "                \"top1_conf\": conf,\n",
    "                \"route_reason\": route_reason,\n",
    "            })\n",
    "            results.append(entry)\n",
    "\n",
    "    return results\n",
    "# ====================== END OVERWRITE: hierarchical_predict ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b89aa4-56bb-4d48-99a9-77b69465fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 6. Post-Training Analysis, Review, and Curation\n",
    "# ==============================================================================\n",
    "\n",
    "def run_post_training_analysis(model_s1, model_s2, processor, device, base_dataset_path, save_dir, version):\n",
    "    \"\"\"\n",
    "    Runs a full inference pass and generates logs for review, curation, and analysis.\n",
    "    Combines logic from old sections 15 and 16.\n",
    "    \"\"\"\n",
    "    import pandas as pd   # ensure pd is local; prevents UnboundLocalError in notebooks\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  RUNNING POST-TRAINING ANALYSIS & CURATION WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- Part A: Run Hierarchical Inference on the Entire Dataset ---\n",
    "    all_image_paths = [str(p) for p in Path(base_dataset_path).rglob(\"*\") if is_valid_image(p.name)]\n",
    "    print(f\"Found {len(all_image_paths)} images to process for inference.\")\n",
    "    \n",
    "    predictions = hierarchical_predict(all_image_paths, model_s1, model_s2, processor, device)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Derive true label from path for analysis\n",
    "    df['true_label'] = df['image_path'].apply(lambda p: Path(p).parent.name)\n",
    "\n",
    "    # Save the full log\n",
    "    full_log_path = os.path.join(save_dir, f\"{version}_full_inference_log.csv\")\n",
    "    df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\n‚úÖ Full inference log saved to: {full_log_path}\")\n",
    "\n",
    "    # at top of the function (after building df)\n",
    "    GENERATE_TRAINING_SHORTLISTS = False   # training script should not rebuild these\n",
    "    GENERATE_MINING_PAIRS       = False    # keep mining in the curation notebook\n",
    "\n",
    "    if GENERATE_TRAINING_SHORTLISTS:\n",
    "        # ... (your existing shortlist + curated_additions code)\n",
    "        pass\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipping shortlist/curated_additions creation here (use curation notebook artifacts).\")\n",
    "    \n",
    "    if GENERATE_MINING_PAIRS:\n",
    "        # ... (your existing hard-negative mining code)\n",
    "        pass\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipping hard-negative mining here (handled in curation notebook).\")\n",
    "\n",
    "    # --- Part B: Identify and Organize Images for Manual Review ---\n",
    "    # Tag images with low confidence as \"REVIEW\"\n",
    "    review_threshold = REVIEW_CONF_THRESHOLD\n",
    "    review_df = df[df['confidence'] < review_threshold]\n",
    "    \n",
    "    review_sort_dir = os.path.join(save_dir, \"review_candidates_by_predicted_class\")\n",
    "    os.makedirs(review_sort_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nFound {len(review_df)} images below {review_threshold} confidence for review.\")\n",
    "    for _, row in tqdm(review_df.iterrows(), total=len(review_df), desc=\"Sorting review images\"):\n",
    "        dest_dir = os.path.join(review_sort_dir, row['prediction'])\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        shutil.copy(row['image_path'], dest_dir)\n",
    "    print(f\"üìÇ Sorted review images into folders at: {review_sort_dir}\")\n",
    "\n",
    "    # --- : Generate shortlist and curated patch CSVs for THIS run ---\n",
    "    #     - Shortlist: low-confidence items in focus classes (for targeted manual review)\n",
    "    #     - Curated patch: template CSV for corrected labels to be fed back into VNext\n",
    "    focus_classes = ['sadness','speech_action','neutral','neutral_speech','happiness']\n",
    "    \n",
    "    # Defensive: ensure the expected columns exist\n",
    "    has_pred = 'prediction' in df.columns or 'predicted_label' in df.columns\n",
    "    pred_col = 'prediction' if 'prediction' in df.columns else ('predicted_label' if 'predicted_label' in df.columns else None)\n",
    "    if pred_col is not None:\n",
    "        # Sort by confidence ascending (uncertain first)\n",
    "        df_focus = df[df[pred_col].isin(focus_classes)].copy()\n",
    "        if 'confidence' in df_focus.columns:\n",
    "            df_focus = df_focus.sort_values('confidence', ascending=True)\n",
    "    \n",
    "        short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "        patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "        # Write shortlist with a stable set of columns\n",
    "        keep_cols = [c for c in ['image_path','filepath','true_label',pred_col,'confidence'] if c in df_focus.columns]\n",
    "        df_focus[keep_cols].to_csv(short_csv, index=False)\n",
    "        print(f\"‚úÖ Shortlist written: {short_csv}\")\n",
    "    \n",
    "        # Create empty curated patch template\n",
    "        src_path_col = 'image_path' if 'image_path' in df_focus.columns else 'filepath'\n",
    "        patch_df = pd.DataFrame({\n",
    "            \"filepath\": df_focus[src_path_col],\n",
    "            \"correct_label\": \"\",\n",
    "            \"notes\": \"\"\n",
    "        })\n",
    "        patch_df.to_csv(patch_csv, index=False)\n",
    "        print(f\"‚úÖ Curated patch template written: {patch_csv}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped shortlist/patch CSVs: no predicted label column found in full log.\")\n",
    "\n",
    "    # --- : Merge this run's shortlist/patch with V32 to create canonical merged artifacts ---\n",
    "    def _merge_csvs(csv_list, key_cols, out_csv):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "    \n",
    "        # Normalize common column name variants so we can dedupe safely\n",
    "        def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            colmap = {}\n",
    "            # path columns\n",
    "            if \"image_path\" not in df.columns:\n",
    "                if \"filepath\" in df.columns:\n",
    "                    colmap[\"filepath\"] = \"image_path\"\n",
    "                elif \"path\" in df.columns:\n",
    "                    colmap[\"path\"] = \"image_path\"\n",
    "            # predicted label columns\n",
    "            if \"predicted_label\" not in df.columns:\n",
    "                if \"prediction\" in df.columns:\n",
    "                    colmap[\"prediction\"] = \"predicted_label\"\n",
    "                elif \"predicted\" in df.columns:\n",
    "                    colmap[\"predicted\"] = \"predicted_label\"\n",
    "            return df.rename(columns=colmap)\n",
    "    \n",
    "        frames = []\n",
    "        for p in csv_list:\n",
    "            if os.path.exists(p):\n",
    "                try:\n",
    "                    df = pd.read_csv(p)\n",
    "                    df = _normalize_cols(df)\n",
    "                    frames.append(df)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    \n",
    "        if not frames:\n",
    "            return\n",
    "    \n",
    "        merged = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "        # Keep only keys that actually exist after normalization\n",
    "        available_keys = [k for k in key_cols if k in merged.columns]\n",
    "        if not available_keys:\n",
    "            print(f\"‚ÑπÔ∏è Skipped merge for {out_csv}: none of the key columns {key_cols} exist in merged data.\")\n",
    "            return\n",
    "    \n",
    "        merged = merged.drop_duplicates(subset=available_keys, keep=\"first\")\n",
    "        merged.to_csv(out_csv, index=False)\n",
    "        print(f\"‚úÖ Merged: {out_csv} ({len(merged)} rows)\")\n",
    "\n",
    "    \n",
    "    # Paths for this run (already defined above)\n",
    "    short_csv = os.path.join(save_dir, f\"curation_shortlist_{version}.csv\")\n",
    "    patch_csv  = os.path.join(save_dir, f\"curated_additions_{version}.csv\")\n",
    "    \n",
    "    # V32 paths (if present)\n",
    "    v32_short = os.path.join(save_dir, \"curation_shortlist_V32.csv\")\n",
    "    v32_patch = os.path.join(save_dir, \"curated_additions_V32.csv\")\n",
    "    \n",
    "    # Canonical merged outputs\n",
    "    short_merged = os.path.join(save_dir, \"curation_shortlist_merged.csv\")\n",
    "    patch_merged = os.path.join(save_dir, \"curated_additions_merged.csv\")\n",
    "    \n",
    "    # Merge (shortlist merges on [filepath, predicted_label]; patch merges on [filepath])\n",
    "    if pred_col is not None:\n",
    "        # Figure out the filepath column available\n",
    "        avail_path_cols = ['image_path','filepath']\n",
    "        path_col = next((c for c in avail_path_cols if c in df.columns), None)\n",
    "    \n",
    "        if path_col is not None:\n",
    "            _merge_csvs([v32_short, short_csv], key_cols=[path_col, pred_col], out_csv=short_merged)\n",
    "            _merge_csvs([v32_patch, patch_csv], key_cols=[path_col], out_csv=patch_merged)\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Skipped merge: no filepath column present in full log.\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Skipped merge: no predicted label column present in full log.\")\n",
    "\n",
    "\n",
    "    # --- Part C: Mine for \"Hard Negative\" Confusion Pairs (toggleable & robust) ---\n",
    "    MINING_HARD_NEGATIVES = True  # ‚Üê set False for deployment runs\n",
    "\n",
    "    if MINING_HARD_NEGATIVES:\n",
    "        import pandas as pd\n",
    "         \n",
    "        # Prefer the freshly generated full log from THIS run; fallback to prior runs only if missing.\n",
    "        inference_log_path = full_log_path\n",
    "        if not os.path.exists(inference_log_path):\n",
    "            v33_log = os.path.join(SAVE_DIR, \"V33_full_inference_log.csv\")\n",
    "            v32_log = os.path.join(SAVE_DIR, \"V32_full_inference_log.csv\")\n",
    "            inference_log_path = v33_log if os.path.exists(v33_log) else (v32_log if os.path.exists(v32_log) else None)\n",
    "\n",
    "    \n",
    "        if not os.path.exists(inference_log_path):\n",
    "            print(\"‚è© Skipping hard-negative mining: no full inference log found.\")\n",
    "        else:\n",
    "            print(\"\\n‚õèÔ∏è  Mining for hard negative confusion pairs...\")\n",
    "            print(f\"   using: {inference_log_path}\")\n",
    "            df = pd.read_csv(inference_log_path)\n",
    "    \n",
    "            # Normalize column names between runs (V32 used 'prediction', V33 uses 'predicted_label')\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            col_true = cols.get(\"true_label\", \"true_label\")\n",
    "            col_pred = cols.get(\"predicted_label\") or cols.get(\"prediction\")\n",
    "            if col_pred is None:\n",
    "                raise RuntimeError(f\"Could not find predicted label column in {df.columns.tolist()}\")\n",
    "    \n",
    "            # (Optional) keep a stable sort by confidence descending if available\n",
    "            col_conf = cols.get(\"confidence\")\n",
    "            if col_conf:\n",
    "                df = df.sort_values(col_conf, ascending=False)\n",
    "    \n",
    "            # Which pairs to mine\n",
    "            confusion_pairs_to_mine = [\n",
    "                ('contempt', 'questioning'),\n",
    "                ('contempt', 'neutral'),\n",
    "                ('fear', 'surprise')\n",
    "            ]\n",
    "    \n",
    "            # Save to the current run folder\n",
    "            save_dir = SAVE_DIR\n",
    "    \n",
    "            for c1, c2 in confusion_pairs_to_mine:\n",
    "                mask = ((df[col_true] == c1) & (df[col_pred] == c2)) | \\\n",
    "                       ((df[col_true] == c2) & (df[col_pred] == c1))\n",
    "                hard_negatives = df.loc[mask]\n",
    "    \n",
    "                if not hard_negatives.empty:\n",
    "                    out_path = os.path.join(save_dir, f\"hard_negatives_{c1}_vs_{c2}.csv\")\n",
    "                    hard_negatives.to_csv(out_path, index=False)\n",
    "                    print(f\"  - Found {len(hard_negatives)} hard negatives for ({c1} ‚Üî {c2}). Saved: {out_path}\")\n",
    "                else:\n",
    "                    print(f\"  - No hard negatives found for ({c1} ‚Üî {c2}).\")\n",
    "    else:\n",
    "        print(\"‚è© Hard-negative mining disabled (set MINING_HARD_NEGATIVES=True to enable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c3da51-9785-4c85-85c0-96fee16f5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 7. Model Calibration\n",
    "# ==============================================================================\n",
    "\n",
    "def apply_temperature_scaling(logits, labels):\n",
    "    \"\"\"Finds the optimal temperature for calibrating model confidence.\"\"\"\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler()\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits_tensor), labels_tensor)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    return model.temperature.item()\n",
    "\n",
    "def plot_reliability_diagram(logits, labels, temperature, save_dir, version, stage_name):\n",
    "    \"\"\"Visualizes model calibration before and after temperature scaling.\"\"\"\n",
    "    logits = torch.from_numpy(logits)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    \n",
    "    # Calculate before\n",
    "    probs_before = F.softmax(logits, dim=1)\n",
    "    confs_before, _ = torch.max(probs_before, 1)\n",
    "    \n",
    "    # Calculate after\n",
    "    probs_after = F.softmax(logits / temperature, dim=1)\n",
    "    confs_after, _ = torch.max(probs_after, 1)\n",
    "\n",
    "    # Plotting logic remains the same...\n",
    "    # (For brevity, the detailed plotting code from your old script goes here)\n",
    "    print(f\"üìä Reliability diagram generation logic would go here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432d778-54aa-4b19-ba97-f223e12cbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 8. Hierarchical Model Ensembling\n",
    "# ==============================================================================\n",
    "\n",
    "def hierarchical_ensemble_predict(image_path, processor, s1_models, s2_models, device):\n",
    "    \"\"\"Performs an ensembled prediction using multiple hierarchical models.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "    # --- Stage 1 Ensemble (Majority Vote) ---\n",
    "    s1_votes = []\n",
    "    with torch.no_grad():\n",
    "        for model in s1_models:\n",
    "            logits = model(**inputs).logits\n",
    "            pred = torch.argmax(logits, dim=-1).item()\n",
    "            s1_votes.append(pred)\n",
    "    \n",
    "    # Decide relevance based on majority vote (1 = relevant)\n",
    "    is_relevant = Counter(s1_votes).most_common(1)[0][0] == label2id_s1['relevant']\n",
    "\n",
    "    if not is_relevant:\n",
    "        return \"irrelevant\", None\n",
    "\n",
    "    # --- Stage 2 Ensemble (Average Probabilities) ---\n",
    "    s2_probs = []\n",
    "    with torch.no_grad():\n",
    "        for model in s2_models:\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            s2_probs.append(probs)\n",
    "            \n",
    "    # Average the probabilities across all models\n",
    "    avg_probs = torch.mean(torch.stack(s2_probs), dim=0)\n",
    "    confidence, pred_idx = torch.max(avg_probs, dim=-1)\n",
    "    \n",
    "    final_prediction = id2label_s2[pred_idx.item()]\n",
    "    final_confidence = confidence.item()\n",
    "    \n",
    "    return final_prediction, final_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca80acbe-cef7-4d5a-92b2-d11c054cc9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ SMOKE TEST MODE ‚Äî loading exports from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V35_20251014_162112\n",
      "ü´ß Running smoke test on 6 images (S1‚ÜíS2, no training)‚Ä¶\n",
      "  frame_560_Person_1_face_0.png: neutral_speech (0.93)\n",
      "  Sean_Penn_0002.jpg: irrelevant (1.00)\n",
      "  Training_82073038.jpg: irrelevant (1.00)\n",
      "  Spencer_Abraham_0009.jpg: neutral_speech (0.93)\n",
      "  frame_1847_Person_1_face_0.png: neutral_speech (0.95)\n",
      "  Michel_Duclos_0001.jpg: irrelevant (1.00)\n",
      "‚úÖ Smoke test finished. No training was performed.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 9. Script Execution Entry Point  (SMOKE TEST AWARE)\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    if SMOKE_TEST_ONLY:\n",
    "        # --- NO TRAINING: load finished exports (e.g., V35) and run a tiny batch ---\n",
    "        PRETRAINED_CHECKPOINT_PATH = SMOKE_CHECKPOINT_PATH  # override just for smoke\n",
    "        if not os.path.isdir(PRETRAINED_CHECKPOINT_PATH):\n",
    "            raise FileNotFoundError(f\"Smoke checkpoint not found: {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "\n",
    "        print(f\"\\nüß™ SMOKE TEST MODE ‚Äî loading exports from: {PRETRAINED_CHECKPOINT_PATH}\")\n",
    "        model_s1, model_s2, processor = _load_exports_for_smoke(PRETRAINED_CHECKPOINT_PATH, device)\n",
    "\n",
    "        # optional: S1 gate defaults if no calibration json in that export\n",
    "        T_s1, tau = 1.0, 0.30\n",
    "\n",
    "        # pick a few images\n",
    "        smoke_dir = Path(BASE_DATASET_PATH).joinpath(\"smoke_samples\")\n",
    "        img_paths = []\n",
    "        if smoke_dir.exists():\n",
    "            for p in smoke_dir.iterdir():\n",
    "                if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\"}:\n",
    "                    img_paths.append(str(p))\n",
    "                if len(img_paths) >= 6:\n",
    "                    break\n",
    "        if not img_paths:\n",
    "            # fallback: take a few from the prepared stage_2 set if present\n",
    "            prep = Path(OUTPUT_ROOT_DIR)/\"prepared_datasets\"/\"stage_2_emotion_dataset\"\n",
    "            for cls in (prep.iterdir() if prep.exists() else []):\n",
    "                if cls.is_dir():\n",
    "                    for p in cls.iterdir():\n",
    "                        if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".tif\",\".tiff\"}:\n",
    "                            img_paths.append(str(p))\n",
    "                        if len(img_paths) >= 6:\n",
    "                            break\n",
    "                if len(img_paths) >= 6:\n",
    "                    break\n",
    "\n",
    "        if not img_paths:\n",
    "            raise RuntimeError(\"No images found for smoke test. Add a few to BASE_DATASET_PATH/smoke_samples/\")\n",
    "\n",
    "        print(f\"ü´ß Running smoke test on {len(img_paths)} images (S1‚ÜíS2, no training)‚Ä¶\")\n",
    "        outs = hierarchical_predict(\n",
    "            image_paths=img_paths,\n",
    "            model_s1=model_s1,\n",
    "            model_s2=model_s2,\n",
    "            processor=processor,\n",
    "            device=device,\n",
    "            batch_size=4\n",
    "        )\n",
    "        for o in outs:\n",
    "            print(f\"  {Path(o['image_path']).name}: {o['prediction']} ({o['confidence']:.2f})\")\n",
    "\n",
    "        print(\"‚úÖ Smoke test finished. No training was performed.\")\n",
    "    else:\n",
    "        # --- FULL TRAINING PATH (unchanged) ---\n",
    "        model_s1, model_s2, processor = main(device)\n",
    "\n",
    "        if RUN_INFERENCE:\n",
    "            run_post_training_analysis(model_s1, model_s2, processor, device, BASE_DATASET_PATH, SAVE_DIR, VERSION)\n",
    "\n",
    "        stage2_metrics_path = os.path.join(SAVE_DIR, \"per_class_metrics_Stage2.csv\")\n",
    "        check_deployment_readiness(stage2_metrics_path, f1_threshold=0.80)\n",
    "\n",
    "        logits_s2_path = os.path.join(SAVE_DIR, f\"logits_eval_Stage2_{VERSION}.npy\")\n",
    "        labels_s2_path = os.path.join(SAVE_DIR, f\"labels_eval_Stage2_{VERSION}.npy\")\n",
    "        if os.path.exists(logits_s2_path) and os.path.exists(labels_s2_path):\n",
    "            print(\"\\n\" + \"=\"*60); print(\"  CALIBRATING STAGE 2 MODEL\"); print(\"=\"*60)\n",
    "            logits_s2 = np.load(logits_s2_path); labels_s2 = np.load(labels_s2_path)\n",
    "            optimal_temp = apply_temperature_scaling(logits_s2, labels_s2)\n",
    "            print(f\"‚úÖ Optimal temperature for Stage 2 model: {optimal_temp:.4f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Skipping calibration, logits/labels files for Stage 2 not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0a32d-7921-4cbe-b7cf-8b6ed445242b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions_v5)",
   "language": "python",
   "name": "ml_expressions_v5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
