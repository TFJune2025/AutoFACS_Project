{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V14 changes:\n",
    "    # organized section #0\n",
    "    # section #1 -> updated LABEL_NAMES to include contempt and unknown\n",
    "    # section #2 -> added save_model_and_processor function\n",
    "    # section #6 -> reworked perceptual clustering to include these labels:\n",
    "        # disgust, sadness, fear, questioning, contempt\n",
    "    # section #7 -> updated targeted augmentation for new labels except unknown\n",
    "    # section #8 -> added contempt to minority_classes_names\n",
    "        #updated minority_classes_names and minority_classes\n",
    "    # sction #11 -> updated to exclude unknown\n",
    "    # section #15 deleted -> renumbered rescue section to #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "# Standard Library Imports\n",
    "import gc\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Third-Party Imports\n",
    "import accelerate\n",
    "import dill\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "# From Imports\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW, LBFGS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import (\n",
    "    GaussianBlur,\n",
    "    RandAugment,\n",
    "    RandomAffine,\n",
    "    RandomApply,\n",
    "    ToPILImage,\n",
    "    ToTensor\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfag_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    'anger', 'disgust', 'fear', 'happiness', 'neutral',\n",
    "    'questioning', 'sadness', 'surprise', 'contempt', 'unknown'\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\", VERSION_TAG)\n",
    "LOGITS_PATH = os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\")\n",
    "LABELS_PATH = os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Utility Functions (Metrics & Calibration)\n",
    "# --------------------------\n",
    "\n",
    "# üîç Compute perceptual hash for image similarity clustering (used in REVIEW and Disgust curation)\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# üîÑ Smoothed Cross Entropy Loss (Œµ = 0.05)\n",
    "class SmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# ‚ö†Ô∏è Confidence Penalty to Reduce Overconfidence\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "# üìä Compute Metrics with Confusion Matrix Logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw values for calibration or further use\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Generate confusion matrix heatmap\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify top confused class pairs (excluding diagonal)\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "    entropy_per_class.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nüîç Class entropies (sorted):\")\n",
    "    for class_name, entropy in entropy_per_class:\n",
    "        print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# üå°Ô∏è Apply Temperature Scaling for Calibration\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"‚ùå Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Loading logits from: {logits_path}\")\n",
    "    print(f\"üìÇ Loading labels from: {labels_path}\")\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()\n",
    "\n",
    "\n",
    "# üìà Plot Reliability Diagram (Calibration Curve)\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {output_path}\")\n",
    "\n",
    "# saving model and processor\n",
    "def save_model_and_processor(model, processor, save_dir, trainer=None):\n",
    "    print(f\"Saving model and processor to: {save_dir}\")\n",
    "    \n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Save processor\n",
    "    processor.save_pretrained(save_dir)\n",
    "    print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "    \n",
    "    # Save full model\n",
    "    model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "    print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "    # Save state dict\n",
    "    final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "    # Save trainer state\n",
    "    if trainer is not None:\n",
    "        try:\n",
    "            trainer.save_model(os.path.join(save_dir, \"backup_trainer_model\"))\n",
    "            print(\"‚úÖ Trainer backup saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "\n",
    "    # Memory cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass  # Not all systems have CUDA\n",
    "    print(\"‚úÖ Memory cleanup complete after save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V13_20250527_161430 were not used when initializing ViTForImageClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V13_20250527_161430 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto-loaded model from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V13_20250527_161430\n",
      "üñ•Ô∏è Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Auto-Load Latest Pretrained Model and Processor\n",
    "# --------------------------\n",
    "\n",
    "# Automatically load latest model path\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training\"\n",
    "# List all version folders in descending order\n",
    "model_dirs = sorted(\n",
    "    [os.path.join(MODEL_ROOT, d) for d in os.listdir(MODEL_ROOT)\n",
    "     if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))],\n",
    "    key=lambda x: os.path.getmtime(x),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Remove the current output version (to avoid loading from empty target)\n",
    "model_dirs = [d for d in model_dirs if VERSION in d or not d.startswith(VERSION)]\n",
    "model_dirs = [d for d in model_dirs if os.path.basename(d).startswith(\"V\") and d != SAVE_DIR]\n",
    "\n",
    "# Pick the most recent complete model (not current output)\n",
    "if len(model_dirs) < 1:\n",
    "    raise FileNotFoundError(\"‚ùå No earlier model folders found.\")\n",
    "model_path = model_dirs[0]\n",
    "print(f\"‚úÖ Auto-loaded model from: {model_path}\")\n",
    "\n",
    "# Load base model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Modify classification head with Dropout for regularization\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(model.classifier.in_features, len(id2label))\n",
    ")\n",
    "\n",
    "# Replace classification head to match current label schema\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefd4a9ed7a74b1a9ae033df554c4f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/17630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total examples after filtering: 17619\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Load and Prepare Dataset\n",
    "# --------------------------\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=\"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalfag_dataset\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/tmp/hf_cache\"\n",
    ")\n",
    "\n",
    "counter = {\"n\": 0}\n",
    "\n",
    "def reconcile_labels(example):\n",
    "    counter[\"n\"] += 1\n",
    "    if counter[\"n\"] % 1000 == 0:\n",
    "        print(f\"Processed {counter['n']} images...\")\n",
    "\n",
    "    label = example.get(\"label\", None)\n",
    "\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = getattr(example[\"image\"], \"filename\", None)\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example\n",
    "\n",
    "# Single-threaded labeling to preserve .filename\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "print(f\"‚úÖ Total examples after filtering: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise', 'unknown'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 2302 examples\n",
      "  disgust: 309 examples\n",
      "  fear: 1432 examples\n",
      "  happiness: 2892 examples\n",
      "  neutral: 3334 examples\n",
      "  questioning: 1943 examples\n",
      "  sadness: 1706 examples\n",
      "  surprise: 2779 examples\n",
      "  contempt: 536 examples\n",
      "  unknown: 386 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes: ['contempt', 'disgust', 'unknown']\n",
      "\n",
      "üìÇ Image count per label folder:\n",
      "  anger: 2302 images\n",
      "  contempt: 536 images\n",
      "  disgust: 309 images\n",
      "  fear: 1432 images\n",
      "  happiness: 2892 images\n",
      "  neutral: 3334 images\n",
      "  questioning: 1943 images\n",
      "  sadness: 1706 images\n",
      "  surprise: 2779 images\n",
      "  unknown: 386 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Disgust hash clusters with more than 1 image:\n",
      "üîç Sadness hash clusters with more than 1 image:\n",
      "  - Cluster 958c52e1: 2 images copied for review\n",
      "  - Cluster ee9a8d33: 2 images copied for review\n",
      "  - Cluster d0890396: 2 images copied for review\n",
      "  - Cluster bb0d06f2: 2 images copied for review\n",
      "  - Cluster d7f00fa2: 2 images copied for review\n",
      "üîç Fear hash clusters with more than 1 image:\n",
      "  - Cluster 9ae56592: 2 images copied for review\n",
      "  - Cluster 91c8ee81: 2 images copied for review\n",
      "  - Cluster dae5a596: 2 images copied for review\n",
      "üîç Questioning hash clusters with more than 1 image:\n",
      "  - Cluster da014886: 2 images copied for review\n",
      "  - Cluster 9db42783: 2 images copied for review\n",
      "üîç Contempt hash clusters with more than 1 image:\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Perceptual Clustering for Ambiguous/Confused Classes\n",
    "# --------------------------\n",
    "\n",
    "CLUSTER_TARGETS = [\"disgust\", \"sadness\", \"fear\", \"questioning\", \"contempt\"]\n",
    "\n",
    "for class_name in CLUSTER_TARGETS:\n",
    "    class_dir = os.path.join(BASE_PATH, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        print(f\"‚ö†Ô∏è Class dir not found: {class_dir} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    class_images = [\n",
    "        os.path.join(class_dir, f) for f in os.listdir(class_dir)\n",
    "        if is_valid_image(f)\n",
    "    ]\n",
    "    hash_map = {}\n",
    "    for path in class_images:\n",
    "        h = compute_hash(path)\n",
    "        if h:\n",
    "            hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "    cluster_dir = os.path.join(SAVE_DIR, f\"{class_name}_clusters\")\n",
    "    os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"üîç {class_name.capitalize()} hash clusters with more than 1 image:\")\n",
    "    for h, paths in hash_map.items():\n",
    "        if len(paths) > 1:\n",
    "            cluster_path = os.path.join(cluster_dir, h)\n",
    "            os.makedirs(cluster_path, exist_ok=True)\n",
    "            for p in paths:\n",
    "                shutil.copy(p, cluster_path)\n",
    "            print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Targeted minority augmentation will apply to: ['contempt', 'disgust', 'questioning']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "label_freqs = Counter(dataset[\"label\"])\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "minority_by_name = [n for n in minority_by_name if n != \"unknown\"]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning', 'contempt']\n",
    "\n",
    "# Merge and deduplicate\n",
    "minority_class_names = list(set(minority_by_name + manual_focus_classes))\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"üéØ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6c65a5616949489240f6ac644e4c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmentation counts: {'anger': 2302, 'contempt': 536, 'disgust': 309, 'fear': 1432, 'happiness': 2892, 'neutral': 3334, 'questioning': 1943, 'sadness': 1706, 'surprise': 2779, 'unknown': 386}\n",
      "‚úÖ Saved augmentation snapshot to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/V14_augmentation_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "])\n",
    "\n",
    "# RandAugment for specific minority classes only\n",
    "minority_classes_names = minority_class_names\n",
    "minority_classes = [label2id[label] for label in minority_classes_names]\n",
    "\n",
    "minority_aug = T.Compose([\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "])\n",
    "\n",
    "# Augmentation counter tracking\n",
    "aug_count = Counter()\n",
    "\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "        aug_count[label] += 1\n",
    "\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "        return inputs\n",
    "    return transform_function\n",
    "\n",
    "# After mapping finishes:\n",
    "dataset = dataset.map(make_transform_function(processor, minority_classes))\n",
    "formatted_counts = {LABEL_NAMES[k]: v for k, v in aug_count.items()}\n",
    "print(f\"‚úÖ Augmentation counts: {formatted_counts}\")\n",
    "\n",
    "# Explicitly log dataset snapshots (class distribution) to a \n",
    "# CSV or JSON after each run for easy future diffing and tracking\n",
    "snapshot_path = os.path.join(SAVE_DIR, f\"{VERSION}_augmentation_snapshot.csv\")\n",
    "aug_snapshot = pd.DataFrame.from_dict(dict(aug_count), orient='index', columns=['count'])\n",
    "aug_snapshot.to_csv(snapshot_path)\n",
    "\n",
    "print(f\"‚úÖ Saved augmentation snapshot to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 9. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e37d1e-a6fb-4e9f-aec5-ee945f11c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Saved label distribution snapshot: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/label_snapshots/V14_label_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Label Distribution Snapshot and Drift Monitor\n",
    "# --------------------------\n",
    "snapshot_dir = os.path.join(SAVE_DIR, \"label_snapshots\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Count current training labels\n",
    "train_label_names = [LABEL_NAMES[i] for i in train_dataset['label']]\n",
    "label_counts = pd.Series(train_label_names).value_counts().sort_index()\n",
    "label_counts.name = VERSION\n",
    "\n",
    "# Save snapshot CSV\n",
    "snapshot_path = os.path.join(snapshot_dir, f\"{VERSION}_label_distribution.csv\")\n",
    "label_counts.to_csv(snapshot_path)\n",
    "print(f\"üìä Saved label distribution snapshot: {snapshot_path}\")\n",
    "\n",
    "# Optionally compare to previous version\n",
    "previous_versions = sorted([\n",
    "    f for f in os.listdir(snapshot_dir) if f.endswith(\".csv\") and not f.startswith(VERSION)\n",
    "])\n",
    "if previous_versions:\n",
    "    latest_prev = previous_versions[-1]\n",
    "    prev_df = pd.read_csv(os.path.join(snapshot_dir, latest_prev), index_col=0)\n",
    "    diff = label_counts.subtract(prev_df.iloc[:, 0], fill_value=0)\n",
    "    print(\"üîç Label count change since last snapshot:\")\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({4: 2670, 3: 2295, 7: 2213, 0: 1848, 5: 1568, 6: 1346, 2: 1161, 8: 435, 9: 313, 1: 246})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c17d566708f4f5582c1c1f7ac2136d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd25a94096f54fb782d05b0de40d328b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd97a817a382457589688928a0f73d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc463b0a8a95471eb9847fb07ea161b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d7aaccd5f840e98cea2544beecd0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d5006ce43e4c8b887dbcb0c4c8ea44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97c7a06d11e4610abd9b65309b6867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d9e9343be846e1b6940dce2003413a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415a6d0d98a64109ac6baffe5fc33802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f218762f825444628d9ecae4ff36035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/14095 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({6: 3000, 7: 3000, 1: 3000, 0: 3000, 3: 3000, 8: 3000, 5: 3000, 4: 3000, 2: 3000, 9: 313})\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11. Balance Dataset (with NO oversampling for 'unknown')\n",
    "# --------------------------\n",
    "mp.set_start_method('fork', force=True)\n",
    "\n",
    "label_target = 3000\n",
    "balanced_subsets = []\n",
    "\n",
    "label_counts = Counter(train_dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = train_dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    if LABEL_NAMES[label] == \"unknown\":\n",
    "        # Just append all 'unknown' examples as-is (no upsampling)\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count > label_target:\n",
    "        subset = subset.select(random.sample(range(len(subset)), label_target))\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count < label_target:\n",
    "        multiplier = label_target // len(subset)\n",
    "        remainder = label_target % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        balanced_subsets.append(subset)\n",
    "    else:\n",
    "        balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 12. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # ‚úÖ (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # ‚úÖ Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # ‚úÖ Log once per epoch\n",
    "    save_safetensors=True                  # ‚úÖ Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 13. Define Compute Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17075' max='17075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17075/17075 4:37:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.574100</td>\n",
       "      <td>0.623898</td>\n",
       "      <td>0.906924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>0.693156</td>\n",
       "      <td>0.907491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.335100</td>\n",
       "      <td>0.708004</td>\n",
       "      <td>0.904938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.691474</td>\n",
       "      <td>0.907775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.681942</td>\n",
       "      <td>0.911464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.91      0.95      0.93       454\n",
      "     disgust       0.98      0.71      0.83        63\n",
      "        fear       0.79      0.77      0.78       271\n",
      "   happiness       0.97      0.98      0.98       597\n",
      "     neutral       0.94      0.95      0.94       664\n",
      " questioning       0.88      0.90      0.89       375\n",
      "     sadness       0.93      0.85      0.89       360\n",
      "    surprise       0.93      0.92      0.93       566\n",
      "    contempt       0.50      0.53      0.52       101\n",
      "     unknown       0.97      1.00      0.99        73\n",
      "\n",
      "    accuracy                           0.91      3524\n",
      "   macro avg       0.88      0.86      0.87      3524\n",
      "weighted avg       0.91      0.91      0.91      3524\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - contempt ‚Üí questioning: 35 instances\n",
      "  - fear ‚Üí surprise: 32 instances\n",
      "  - surprise ‚Üí fear: 29 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3464\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - fear: entropy = 0.4747\n",
      "  - contempt: entropy = 0.4551\n",
      "  - sadness: entropy = 0.4492\n",
      "  - disgust: entropy = 0.4031\n",
      "  - surprise: entropy = 0.3415\n",
      "  - neutral: entropy = 0.3239\n",
      "  - questioning: entropy = 0.3117\n",
      "  - anger: entropy = 0.3013\n",
      "  - happiness: entropy = 0.2978\n",
      "  - unknown: entropy = 0.2629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.90      0.96      0.93       454\n",
      "     disgust       0.95      0.63      0.76        63\n",
      "        fear       0.84      0.70      0.76       271\n",
      "   happiness       0.97      0.98      0.98       597\n",
      "     neutral       0.93      0.97      0.95       664\n",
      " questioning       0.87      0.93      0.90       375\n",
      "     sadness       0.91      0.85      0.88       360\n",
      "    surprise       0.91      0.94      0.92       566\n",
      "    contempt       0.52      0.43      0.47       101\n",
      "     unknown       1.00      1.00      1.00        73\n",
      "\n",
      "    accuracy                           0.91      3524\n",
      "   macro avg       0.88      0.84      0.86      3524\n",
      "weighted avg       0.90      0.91      0.90      3524\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí surprise: 43 instances\n",
      "  - contempt ‚Üí questioning: 40 instances\n",
      "  - surprise ‚Üí fear: 21 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2888\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.3812\n",
      "  - disgust: entropy = 0.3519\n",
      "  - fear: entropy = 0.3340\n",
      "  - sadness: entropy = 0.3166\n",
      "  - anger: entropy = 0.3055\n",
      "  - surprise: entropy = 0.2787\n",
      "  - questioning: entropy = 0.2771\n",
      "  - happiness: entropy = 0.2674\n",
      "  - unknown: entropy = 0.2642\n",
      "  - neutral: entropy = 0.2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.90      0.94      0.92       454\n",
      "     disgust       0.93      0.62      0.74        63\n",
      "        fear       0.85      0.71      0.77       271\n",
      "   happiness       0.97      0.98      0.98       597\n",
      "     neutral       0.89      0.98      0.93       664\n",
      " questioning       0.90      0.91      0.91       375\n",
      "     sadness       0.91      0.83      0.87       360\n",
      "    surprise       0.91      0.95      0.93       566\n",
      "    contempt       0.60      0.38      0.46       101\n",
      "     unknown       1.00      1.00      1.00        73\n",
      "\n",
      "    accuracy                           0.90      3524\n",
      "   macro avg       0.89      0.83      0.85      3524\n",
      "weighted avg       0.90      0.90      0.90      3524\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí surprise: 45 instances\n",
      "  - contempt ‚Üí questioning: 36 instances\n",
      "  - sadness ‚Üí neutral: 25 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2810\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.4064\n",
      "  - contempt: entropy = 0.3590\n",
      "  - sadness: entropy = 0.3162\n",
      "  - fear: entropy = 0.3076\n",
      "  - questioning: entropy = 0.2855\n",
      "  - anger: entropy = 0.2767\n",
      "  - surprise: entropy = 0.2748\n",
      "  - neutral: entropy = 0.2590\n",
      "  - happiness: entropy = 0.2554\n",
      "  - unknown: entropy = 0.2537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.94      0.92      0.93       454\n",
      "     disgust       0.96      0.68      0.80        63\n",
      "        fear       0.81      0.76      0.78       271\n",
      "   happiness       0.95      0.99      0.97       597\n",
      "     neutral       0.91      0.97      0.94       664\n",
      " questioning       0.92      0.91      0.92       375\n",
      "     sadness       0.90      0.83      0.87       360\n",
      "    surprise       0.92      0.93      0.93       566\n",
      "    contempt       0.55      0.56      0.56       101\n",
      "     unknown       1.00      1.00      1.00        73\n",
      "\n",
      "    accuracy                           0.91      3524\n",
      "   macro avg       0.89      0.86      0.87      3524\n",
      "weighted avg       0.91      0.91      0.91      3524\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí surprise: 36 instances\n",
      "  - contempt ‚Üí questioning: 25 instances\n",
      "  - sadness ‚Üí neutral: 24 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2828\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.3577\n",
      "  - fear: entropy = 0.3315\n",
      "  - disgust: entropy = 0.2974\n",
      "  - sadness: entropy = 0.2921\n",
      "  - questioning: entropy = 0.2908\n",
      "  - anger: entropy = 0.2859\n",
      "  - surprise: entropy = 0.2791\n",
      "  - neutral: entropy = 0.2641\n",
      "  - unknown: entropy = 0.2607\n",
      "  - happiness: entropy = 0.2604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalyagrokh/miniconda3/envs/ml_expressions/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.91      0.95      0.93       454\n",
      "     disgust       0.92      0.70      0.79        63\n",
      "        fear       0.84      0.71      0.77       271\n",
      "   happiness       0.97      0.98      0.98       597\n",
      "     neutral       0.92      0.97      0.95       664\n",
      " questioning       0.91      0.92      0.92       375\n",
      "     sadness       0.91      0.84      0.87       360\n",
      "    surprise       0.91      0.95      0.93       566\n",
      "    contempt       0.58      0.54      0.56       101\n",
      "     unknown       1.00      1.00      1.00        73\n",
      "\n",
      "    accuracy                           0.91      3524\n",
      "   macro avg       0.89      0.86      0.87      3524\n",
      "weighted avg       0.91      0.91      0.91      3524\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - fear ‚Üí surprise: 45 instances\n",
      "  - contempt ‚Üí questioning: 27 instances\n",
      "  - sadness ‚Üí neutral: 22 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.2812\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - contempt: entropy = 0.3385\n",
      "  - fear: entropy = 0.3296\n",
      "  - disgust: entropy = 0.3051\n",
      "  - sadness: entropy = 0.2993\n",
      "  - questioning: entropy = 0.2828\n",
      "  - surprise: entropy = 0.2802\n",
      "  - anger: entropy = 0.2736\n",
      "  - neutral: entropy = 0.2666\n",
      "  - unknown: entropy = 0.2613\n",
      "  - happiness: entropy = 0.2603\n",
      "Saving model and processor to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959\n",
      "‚úÖ Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959\n",
      "‚úÖ Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959\n",
      "‚úÖ State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/final_model.pth\n",
      "‚úÖ Memory cleanup complete after save.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 14. Trainer with Class-Weighted Loss\n",
    "# --------------------------\n",
    "\n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Use smoothed CE + confidence penalty\n",
    "        smooth_ce_loss = SmoothedCrossEntropyLoss(smoothing=0.05)\n",
    "        loss = smooth_ce_loss(logits, labels) + confidence_penalty(logits, beta=0.05)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Modify training args for learning rate scheduling and early stopping\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.metric_for_best_model = \"eval_loss\"\n",
    "training_args.evaluation_strategy = \"epoch\"\n",
    "training_args.save_strategy = \"epoch\"\n",
    "\n",
    "# Add EarlyStoppingCallback\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "# Initialize WeightedTrainer with focal loss, confidence penalty, and label smoothing\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    "    optimizers=(\n",
    "        AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=0.01),\n",
    "        None\n",
    "    ),\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "# T_0 = epochs before first restart, T_mult = restart multiplier\n",
    "scheduler = CosineAnnealingWarmRestarts(trainer.optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "# Add scheduler step logic inside the training loop:\n",
    "original_train = trainer.train\n",
    "\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)  # instead of eval_loss\n",
    "    return result\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "save_model_and_processor(model, processor, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # 15. Rescue & Save from Last Checkpoint (after training)\n",
    "# # --------------------------\n",
    "# #in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"‚úÖ Processor manually re-saved.\")\n",
    "\n",
    "# # Use parent directory of SAVE_DIR to locate latest V* folder\n",
    "# parent_dir = os.path.dirname(SAVE_DIR)\n",
    "# v_folders = [\n",
    "#     d for d in os.listdir(parent_dir)\n",
    "#     if os.path.isdir(os.path.join(parent_dir, d)) and d.startswith(\"V\")\n",
    "# ]\n",
    "\n",
    "# def extract_timestamp(name):\n",
    "#     try:\n",
    "#         _, date_str, time_str = name.split(\"_\")\n",
    "#         return datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "#     except Exception:\n",
    "#         return datetime.min\n",
    "\n",
    "# latest_version_folder = max(v_folders, key=extract_timestamp)\n",
    "# latest_version_path = os.path.join(parent_dir, latest_version_folder)\n",
    "# print(f\"üóÇÔ∏è Using latest version folder: {latest_version_path}\")\n",
    "\n",
    "# # Locate latest checkpoint within that version folder\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(latest_version_path, d)\n",
    "#     for d in os.listdir(latest_version_path)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(latest_version_path, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"‚ùå No checkpoint found in latest version folder.\")\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # Load model and processor from latest checkpoint and save them\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(latest_version_path)\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959 were not used when initializing ViTForImageClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 16. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"‚úÖ Model reloaded for inference.\")\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 276/276 [08:00<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "üìù Saved REVIEW file paths to V14_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/V14_distribution_plot_20250614_190959.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"üìù Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959\n",
      "üìÇ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/logits_eval_V14.npy\n",
      "üìÇ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/labels_eval_V14.npy\n",
      "‚úÖ Optimal temperature: 1.3030\n",
      "‚úÖ Calibrated Log Loss: 0.4345\n",
      "üìä Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/V14_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"üìÅ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaeec19-d4c7-43a6-80cb-86aafd573989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed tagging + copying REVIEW predictions to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/review_predictions_by_class\n",
      "üìÑ CSV log saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/V14_review_predictions_with_preds.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 19. Review & Relabel 'REVIEW' Predictions\n",
    "# --------------------------\n",
    "# Section #18 and #19 (REVIEW handling paths)\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "REVIEW_DIR = os.path.join(SAVE_DIR, \"review_predictions_clustered\")\n",
    "\n",
    "# ensure all dirs exist\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "os.makedirs(REVIEW_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "REVIEW_THRESHOLD = 0.85\n",
    "REVIEW_BY_CLASS_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "REVIEW_CSV_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "os.makedirs(REVIEW_BY_CLASS_DIR, exist_ok=True)\n",
    "\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "image_paths = [\n",
    "    p for p in Path(IMAGE_DIR).rglob(\"*\")\n",
    "    if p.is_file() and p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]\n",
    "]\n",
    "\n",
    "review_log = []\n",
    "\n",
    "for img_path in image_paths:\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            conf, pred_idx = torch.max(probs, dim=-1)\n",
    "\n",
    "        conf_val = conf.item()\n",
    "        pred_label = id2label[pred_idx.item()]\n",
    "        tag = \"REVIEW\" if conf_val < REVIEW_THRESHOLD else pred_label\n",
    "\n",
    "        review_log.append({\n",
    "            \"image_path\": str(img_path),\n",
    "            \"predicted_label\": pred_label,\n",
    "            \"confidence\": round(conf_val, 4),\n",
    "            \"tag\": tag\n",
    "        })\n",
    "\n",
    "        if tag == \"REVIEW\":\n",
    "            target_dir = os.path.join(REVIEW_BY_CLASS_DIR, pred_label)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            shutil.copy(str(img_path), target_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error with image: {img_path} | {e}\")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame(review_log).to_csv(REVIEW_CSV_LOG, index=False)\n",
    "print(f\"‚úÖ Completed tagging + copying REVIEW predictions to: {REVIEW_BY_CLASS_DIR}\")\n",
    "print(f\"üìÑ CSV log saved to: {REVIEW_CSV_LOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 17479 total predictions (CSV) and 17620 REVIEW-tagged paths.\n",
      "üìÇ Grouped 17479 REVIEW images into folders by predicted label in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V14_20250614_190959/review_predictions_by_class\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 20. Organize REVIEW-tagged Images by Predicted Class\n",
    "# --------------------------\n",
    "\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "\n",
    "# Section #19 explicitly depends on CSV log generated in Section #16\n",
    "# Make sure Section #16 ran successfully before proceeding\n",
    "review_txt_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "\n",
    "if os.path.exists(review_txt_path) and os.path.exists(csv_path):\n",
    "    with open(review_txt_path, \"r\") as f:\n",
    "        review_paths = {line.strip() for line in f.readlines()}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    count = 0\n",
    "\n",
    "    print(f\"üîç Found {len(df)} total predictions (CSV) and {len(review_paths)} REVIEW-tagged paths.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = row[\"image_path\"]\n",
    "        label = row[\"predicted_label\"]\n",
    "        conf = row[\"confidence\"]\n",
    "\n",
    "        # ‚úÖ Only process if path was REVIEW-tagged *and* confidence was below threshold\n",
    "        if path in review_paths and label != \"REVIEW\":\n",
    "            dest_dir = os.path.join(REVIEW_SORT_DIR, label)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy(path, dest_dir)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"üìÇ Grouped {count} REVIEW images into folders by predicted label in: {REVIEW_SORT_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing review candidates file or prediction CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55576ae4-91ae-4d68-b538-2b57a0b46089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
