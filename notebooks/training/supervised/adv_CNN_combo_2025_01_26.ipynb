{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f00424-4f75-4c80-9350-c7b3b9e6bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # Enable mixed precision for faster training and reduced memory usage\n",
    "# from tensorflow.keras.mixed_precision import set_global_policy\n",
    "# set_global_policy('mixed_float16')\n",
    "\n",
    "# # GPU memory growth to prevent fragmentation\n",
    "# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57d5255-df0b-495f-90b6-1654e2086a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 20:34:33.793366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-28 20:34:34.835713: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-12.6/lib64\n",
      "2025-01-28 20:34:34.835851: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-12.6/lib64\n",
      "2025-01-28 20:34:34.835868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense, DepthwiseConv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b9a774-f5e7-4595-845c-459a7c2974aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 1: Constants\n",
    "IMG_SIZE = (48, 48)  # Match original dataset resolution\n",
    "NUM_CLASSES = 8      # Update based on your dataset\n",
    "BATCH_SIZE = 32       # Reduced batch size to fit GPU memory\n",
    "EPOCHS = 50\n",
    "DATASET_DIR = \"/home/natalyagrokh/img_datasets/combo_ferckja_dataset_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd94fb62-65d5-4631-bef5-e7045304f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 2: Define Early Stopping & Learning Rate Reduction\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0583fe8c-ca84-4393-9f6a-3f5994773a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 3: Augment Underrepresented Classes (Before Dataset Loading)\n",
    "def augment_and_save(class_name, multiplier):\n",
    "    \"\"\"Augments images in the given class folder to increase dataset size.\"\"\"\n",
    "    class_path = os.path.join(DATASET_DIR, class_name)\n",
    "    images = [os.path.join(class_path, img) for img in os.listdir(class_path) if img.endswith(('.jpg', '.png'))]\n",
    "    current_count = len(images)\n",
    "    target_count = current_count * multiplier\n",
    "    \n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    for i in range(target_count - current_count):\n",
    "        img_path = images[i % current_count]\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = img_array.reshape((1,) + img_array.shape)  # Reshape for ImageDataGenerator\n",
    "        \n",
    "        for batch in datagen.flow(img_array, batch_size=1, save_to_dir=class_path, save_prefix=\"aug\", save_format=\"jpg\"):\n",
    "            break  # Only generate one image per iteration\n",
    "\n",
    "# ✅ Apply augmentation to rare classes BEFORE loading dataset\n",
    "augment_and_save(\"contempt\", 30)  # 30x more images for Contempt\n",
    "augment_and_save(\"disgust\", 5)    # 5x more images for Disgust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3865d99e-6581-4215-8393-6f70286ffee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 20:35:10.506182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.541616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.544997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.548425: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-28 20:35:10.548902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.552170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.555213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.860941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.863220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.865336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-01-28 20:35:10.867318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1179776   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,200,008\n",
      "Trainable params: 1,199,816\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 4: Define Optimized Model (Reduce Overfitting)\n",
    "# Step 4: Define Simplified Model\n",
    "model = Sequential([\n",
    "    # First Convolution Block\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(48, 48, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    # Second Convolution Block\n",
    "    Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),  # Reduced Dense layer size\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e127b2-a42b-42e8-9747-da78056458b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 5: Compile Model with Optimized Learning Rate\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,  # Higher learning rate for better updates\n",
    "    decay_steps=10000,          # Larger decay steps\n",
    "    decay_rate=0.9              # Slower decay rate\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6293932d-e195-49a5-b1ba-ec38360a2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 6: Configure Model Checkpoint\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=\"best_emotion_model_weights.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,  # Save only weights to prevent optimizer errors\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb6ec3b-0bca-4b13-a51e-76a35c5c0a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51057 files belonging to 8 classes.\n",
      "Using 40846 files for training.\n",
      "Found 51057 files belonging to 8 classes.\n",
      "Using 10211 files for validation.\n",
      "WARNING:tensorflow:From /opt/conda/envs/ml_expressions/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 7: Load Training and Validation Datasets (After Augmentation)\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode=\"grayscale\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# ✅ Apply Data Augmentation Pipeline\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Apply Augmentation During Training\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x, y: (data_augmentation(x) / 255.0, y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Normalize Validation Data\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x, y: (x / 255.0, y),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Prefetch for Performance\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef76530-6dd0-4e2d-ae27-bbda365ec1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 20:35:55.084237: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-01-28 20:35:55.482768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8907\n",
      "2025-01-28 20:35:55.860024: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f9bd44c2560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-01-28 20:35:55.860064: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-01-28 20:35:55.866389: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-28 20:35:56.006725: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1277/1277 [==============================] - ETA: 0s - loss: 2.1040 - accuracy: 0.1106     \n",
      "Epoch 1: val_accuracy improved from -inf to 0.10538, saving model to best_emotion_model_weights.h5\n",
      "1277/1277 [==============================] - 42s 30ms/step - loss: 2.1040 - accuracy: 0.1106 - val_loss: 2.0798 - val_accuracy: 0.1054 - lr: 9.8665e-04\n",
      "Epoch 2/50\n",
      "1273/1277 [============================>.] - ETA: 0s - loss: 2.0800 - accuracy: 0.1068  \n",
      "Epoch 2: val_accuracy improved from 0.10538 to 0.10910, saving model to best_emotion_model_weights.h5\n",
      "1277/1277 [==============================] - 38s 30ms/step - loss: 2.0801 - accuracy: 0.1067 - val_loss: 2.0784 - val_accuracy: 0.1091 - lr: 9.7346e-04\n",
      "Epoch 3/50\n",
      "1275/1277 [============================>.] - ETA: 0s - loss: 2.0796 - accuracy: 0.1047  \n",
      "Epoch 3: val_accuracy did not improve from 0.10910\n",
      "1277/1277 [==============================] - 36s 28ms/step - loss: 2.0796 - accuracy: 0.1047 - val_loss: 2.0803 - val_accuracy: 0.1031 - lr: 9.6045e-04\n",
      "Epoch 4/50\n",
      "1275/1277 [============================>.] - ETA: 0s - loss: 2.0797 - accuracy: 0.1088  \n",
      "Epoch 4: val_accuracy did not improve from 0.10910\n",
      "1277/1277 [==============================] - 38s 30ms/step - loss: 2.0796 - accuracy: 0.1088 - val_loss: 2.0804 - val_accuracy: 0.1033 - lr: 9.4761e-04\n",
      "Epoch 5/50\n",
      "1276/1277 [============================>.] - ETA: 0s - loss: 2.0802 - accuracy: 0.1059  \n",
      "Epoch 5: val_accuracy did not improve from 0.10910\n",
      "1277/1277 [==============================] - 38s 29ms/step - loss: 2.0802 - accuracy: 0.1058 - val_loss: 2.0802 - val_accuracy: 0.1048 - lr: 9.3495e-04\n",
      "Epoch 6/50\n",
      "1276/1277 [============================>.] - ETA: 0s - loss: 2.0802 - accuracy: 0.1074  \n",
      "Epoch 6: val_accuracy improved from 0.10910 to 0.11644, saving model to best_emotion_model_weights.h5\n",
      "1277/1277 [==============================] - 36s 28ms/step - loss: 2.0802 - accuracy: 0.1075 - val_loss: 2.0801 - val_accuracy: 0.1164 - lr: 9.2245e-04\n",
      "Epoch 7/50\n",
      "1277/1277 [==============================] - ETA: 0s - loss: 2.0796 - accuracy: 0.1084  \n",
      "Epoch 7: val_accuracy did not improve from 0.11644\n",
      "1277/1277 [==============================] - 39s 31ms/step - loss: 2.0796 - accuracy: 0.1084 - val_loss: 2.0802 - val_accuracy: 0.1048 - lr: 9.1013e-04\n"
     ]
    }
   ],
   "source": [
    "# ✅ Step 8: Compute Class Weights and Train Model\n",
    "labels = np.concatenate([y.numpy().argmax(axis=1) for _, y in train_dataset])\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# Train Model with Class Weights\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab522b8-282a-4533-b1f5-7445d8e3f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save the Final Model\n",
    "weights_path = \"best_emotion_model_weights.h5\"\n",
    "\n",
    "# ✅ Check if weights file exists before loading\n",
    "if os.path.exists(weights_path):\n",
    "    print(\"✅ Loading best model weights...\")\n",
    "    model.load_weights(weights_path)\n",
    "else:\n",
    "    print(\"⚠️ Warning: No weights file found. Skipping weight loading.\")\n",
    "\n",
    "# ✅ Save the full model\n",
    "model.save(\"final_emotion_model.keras\")  # TensorFlow 2.11+ prefers `.keras`\n",
    "print(\"✅ Training complete and final model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf35112-01a9-455a-9538-3895c5e1a4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_expressions) (Local)",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
