{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf43a58f-16a3-4914-905a-6d7718d51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V14 changes:\n",
    "    # section #2 - modified computer_metrics_with_confusion \n",
    "        # to log confusion w/ unknown\n",
    "    # section #8 - changed data_augment to improve robustness of image type\n",
    "        # added logic to avoid augmenting unknown label\n",
    "    # section #9 - inserted code to collect hard examples\n",
    "    # section #11 - replaced original logic to avoid oversampling unknown label\n",
    "    # section #21 - added to collect hard example mining/oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2039b54e-2fdc-4268-b812-8af2286901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0. Imports\n",
    "# --------------------------\n",
    "import accelerate\n",
    "import dill\n",
    "import gc\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import torchvision.transforms as T\n",
    "import transformers\n",
    "\n",
    "from collections import Counter\n",
    "from datasets import ClassLabel, Dataset, Features, Image as DatasetsImage, concatenate_datasets, load_dataset\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from imagehash import phash\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ExifTags, UnidentifiedImageError\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, log_loss\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import LBFGS, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torchvision.transforms import ToPILImage, RandAugment, RandomApply, GaussianBlur, RandomAffine\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0071173a-74de-4aee-8a54-e38c48ee6971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Output directory created: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Global Configurations\n",
    "# --------------------------\n",
    "RUN_INFERENCE = True  # Toggle this off to disable running inference\n",
    "IMAGE_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalf_dataset\"\n",
    "BASE_PATH = IMAGE_DIR\n",
    "\n",
    "LABEL_NAMES = [\n",
    "    \"anger\", \"disgust\", \"fear\", \"happiness\",\n",
    "    \"sadness\", \"surprise\", \"neutral\", \"questioning\", \"unknown\"\n",
    "]\n",
    "id2label = dict(enumerate(LABEL_NAMES))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "VALID_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith(VALID_EXTENSIONS) and not filename.startswith(\"._\")\n",
    "\n",
    "label_mapping = {name.lower(): name for name in LABEL_NAMES}\n",
    "\n",
    "# üî¢ Dynamically determine the next version\n",
    "def get_next_version(base_dir):\n",
    "\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Automatically create a versioned output folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION = get_next_version(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions\")\n",
    "VERSION_TAG = VERSION + \"_\" + timestamp\n",
    "SAVE_DIR = os.path.join(\"/Users/natalyagrokh/AI/ml_expressions/img_expressions\", VERSION_TAG)\n",
    "LOGITS_PATH = os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\")\n",
    "LABELS_PATH = os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Output directory created: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "342291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Utility Functions (Metrics & Calibration)\n",
    "# --------------------------\n",
    "\n",
    "# üîç Compute perceptual hash for image similarity clustering (used in REVIEW and Disgust curation)\n",
    "def compute_hash(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert(\"L\").resize((64, 64))\n",
    "        return str(phash(img))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# üîÑ Smoothed Cross Entropy Loss (Œµ = 0.05)\n",
    "class SmoothedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, weight=None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.register_buffer(\"weight\", weight if weight is not None else None)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        num_classes = logits.size(1)\n",
    "        with torch.no_grad():\n",
    "            smooth_labels = torch.full_like(logits, self.smoothing / (num_classes - 1))\n",
    "            smooth_labels.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        loss_per_sample = -(smooth_labels * log_probs).sum(dim=1)\n",
    "        if self.weight is not None:\n",
    "            weight_per_sample = self.weight[target]\n",
    "            loss_per_sample = loss_per_sample * weight_per_sample\n",
    "        return loss_per_sample.mean()\n",
    "\n",
    "# ‚ö†Ô∏è Confidence Penalty to Reduce Overconfidence\n",
    "def confidence_penalty(logits, beta=0.05):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "    return beta * entropy.mean()\n",
    "\n",
    "# üìä Compute Metrics with Confusion Matrix Logging\n",
    "def compute_metrics_with_confusion(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, preds, target_names=LABEL_NAMES))\n",
    "\n",
    "    # Save raw values for calibration or further use\n",
    "    np.save(os.path.join(SAVE_DIR, f\"logits_eval_{VERSION}.npy\"), logits)\n",
    "    np.save(os.path.join(SAVE_DIR, f\"labels_eval_{VERSION}.npy\"), labels)\n",
    "\n",
    "    # Generate confusion matrix heatmap\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=LABEL_NAMES,\n",
    "        yticklabels=LABEL_NAMES\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"confusion_matrix_epoch_{VERSION}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify top confused class pairs (excluding diagonal)\n",
    "    confusion_pairs = [\n",
    "        ((LABEL_NAMES[i], LABEL_NAMES[j]), cm[i][j])\n",
    "        for i in range(len(LABEL_NAMES))\n",
    "        for j in range(len(LABEL_NAMES)) if i != j\n",
    "    ]\n",
    "    top_confusions = sorted(confusion_pairs, key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"\\nTop 3 confused class pairs:\")\n",
    "    for (true_label, pred_label), count in top_confusions:\n",
    "        print(f\"  - {true_label} ‚Üí {pred_label}: {count} instances\")\n",
    "\n",
    "    # Compute average prediction entropy\n",
    "    softmax_probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    entropies = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-12), dim=-1)\n",
    "    avg_entropy = entropies.mean().item()\n",
    "    print(f\"\\nüß† Avg prediction entropy: {avg_entropy:.4f}\")\n",
    "\n",
    "    # Entropy per class (sorted by entropy)\n",
    "    entropy_per_class = []\n",
    "    for idx, class_name in enumerate(LABEL_NAMES):\n",
    "        mask = (np.array(labels) == idx)\n",
    "        if mask.any():\n",
    "            class_entropy = entropies[mask].mean().item()\n",
    "            entropy_per_class.append((class_name, class_entropy))\n",
    "    entropy_per_class.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nüîç Class entropies (sorted):\")\n",
    "    for class_name, entropy in entropy_per_class:\n",
    "        print(f\"  - {class_name}: entropy = {entropy:.4f}\")\n",
    "\n",
    "    # --- OOD/unknown class confusion logging ---\n",
    "    if \"unknown\" in LABEL_NAMES:\n",
    "        idx_unknown = LABEL_NAMES.index(\"unknown\")\n",
    "        print(f\"\\nüîç 'unknown' row (actual unknown, predicted as):\")\n",
    "        for idx, pred_name in enumerate(LABEL_NAMES):\n",
    "            print(f\"  predicted {pred_name}: {cm[idx_unknown, idx]}\")\n",
    "        print(f\"\\nüîç 'unknown' col (predicted unknown, actual label):\")\n",
    "        for idx, true_name in enumerate(LABEL_NAMES):\n",
    "            print(f\"  true {true_name}: {cm[idx, idx_unknown]}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# üå°Ô∏è Apply Temperature Scaling for Calibration\n",
    "def apply_temperature_scaling(logits_path, labels_path):\n",
    "    if not (os.path.exists(logits_path) and os.path.exists(labels_path)):\n",
    "        print(f\"‚ùå Missing files:\\n  - {logits_path if not os.path.exists(logits_path) else ''}\\n - {labels_path if not os.path.exists(labels_path) else ''}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"üìÇ Loading logits from: {logits_path}\")\n",
    "    print(f\"üìÇ Loading labels from: {labels_path}\")\n",
    "\n",
    "    logits = torch.tensor(np.load(logits_path), dtype=torch.float32).to(device)\n",
    "    labels = torch.tensor(np.load(labels_path), dtype=torch.long).to(device)\n",
    "\n",
    "    class TemperatureScaler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "        def forward(self, logits):\n",
    "            return logits / self.temperature\n",
    "\n",
    "    model = TemperatureScaler().to(device)\n",
    "    optimizer = LBFGS([model.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "    def eval_fn():\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(eval_fn)\n",
    "    calibrated_logits = model(logits)\n",
    "    probs = F.softmax(calibrated_logits, dim=1).detach().cpu().numpy()\n",
    "    logloss = log_loss(labels.cpu().numpy(), probs)\n",
    "\n",
    "    # Save optimal temperature\n",
    "    temperature_value = model.temperature.item()\n",
    "    torch.save(\n",
    "        torch.tensor([temperature_value]),\n",
    "        os.path.join(SAVE_DIR, f\"{VERSION}_calibrated_temperature.pt\")\n",
    "    )\n",
    "    # Save as .txt for downstream scripts (e.g. CelebA sorter)\n",
    "    txt_path = os.path.join(SAVE_DIR, f\"temperature_{VERSION}.txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        f.write(f\"{temperature_value:.6f}\")\n",
    "    print(f\"‚úÖ Temperature also saved to {txt_path}\")\n",
    "\n",
    "    print(f\"‚úÖ Optimal temperature: {temperature_value:.4f}\")\n",
    "    print(f\"‚úÖ Calibrated Log Loss: {logloss:.4f}\")\n",
    "    return temperature_value, logits.cpu(), labels.cpu()\n",
    "\n",
    "\n",
    "# üìà Plot Reliability Diagram (Calibration Curve)\n",
    "def plot_reliability_diagram(logits, labels, temperature, n_bins=15):\n",
    "    probs = F.softmax(logits / temperature, dim=1)\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    accuracies = predictions.eq(labels)\n",
    "\n",
    "    bins = torch.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers, bin_uppers = bins[:-1], bins[1:]\n",
    "\n",
    "    bin_accuracies, bin_confidences = [], []\n",
    "    for lower, upper in zip(bin_lowers, bin_uppers):\n",
    "        mask = (confidences > lower) & (confidences <= upper)\n",
    "        if mask.any():\n",
    "            bin_accuracies.append(accuracies[mask].float().mean())\n",
    "            bin_confidences.append(confidences[mask].mean())\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(bin_confidences, bin_accuracies, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Diagram (After Temperature Scaling)\")\n",
    "    plt.xlabel(\"Confidence\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(SAVE_DIR, f\"{VERSION}_reliability_diagram_calibrated.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"üìä Saved reliability diagram to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629d4736-d643-4b4e-a107-9c2707c3eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/V13_20250527_161430 were not used when initializing ViTForImageClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/V13_20250527_161430 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auto-loaded model from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V13_20250527_161430\n",
      "üñ•Ô∏è Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=9, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Auto-Load Latest Pretrained Model and Processor\n",
    "# --------------------------\n",
    "\n",
    "# Automatically load latest model path\n",
    "MODEL_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions\"\n",
    "# List all version folders in descending order\n",
    "model_dirs = sorted(\n",
    "    [os.path.join(MODEL_ROOT, d) for d in os.listdir(MODEL_ROOT)\n",
    "     if d.startswith(\"V\") and os.path.isdir(os.path.join(MODEL_ROOT, d))],\n",
    "    key=lambda x: os.path.getmtime(x),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Remove the current output version (to avoid loading from empty target)\n",
    "model_dirs = [d for d in model_dirs if VERSION in d or not d.startswith(VERSION)]\n",
    "model_dirs = [d for d in model_dirs if os.path.basename(d).startswith(\"V\") and d != SAVE_DIR]\n",
    "\n",
    "# Pick the most recent complete model (not current output)\n",
    "if len(model_dirs) < 1:\n",
    "    raise FileNotFoundError(\"‚ùå No earlier model folders found.\")\n",
    "model_path = model_dirs[0]\n",
    "print(f\"‚úÖ Auto-loaded model from: {model_path}\")\n",
    "\n",
    "# Load base model and processor\n",
    "model = AutoModelForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Modify classification head with Dropout for regularization\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(model.classifier.in_features, len(id2label))\n",
    ")\n",
    "\n",
    "# Replace classification head to match current label schema\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.num_labels = len(LABEL_NAMES)\n",
    "\n",
    "# Define device and push model to device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è Using device:\", device)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081d864d-be53-4102-ae7e-ce83ba342de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8139448b4ca48b287a770b5784ebbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/15623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5a22702be54d0eb511398b8e7d5a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/15620 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fd32792a5b49c1bfb26951a92817ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de455fd80ca45d289eb1c825a1e2941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Re-labeling dataset:   0%|          | 0/15620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 images...\n",
      "Processed 2000 images...\n",
      "Processed 3000 images...\n",
      "Processed 4000 images...\n",
      "Processed 5000 images...\n",
      "Processed 6000 images...\n",
      "Processed 7000 images...\n",
      "Processed 8000 images...\n",
      "Processed 9000 images...\n",
      "Processed 10000 images...\n",
      "Processed 11000 images...\n",
      "Processed 12000 images...\n",
      "Processed 13000 images...\n",
      "Processed 14000 images...\n",
      "Processed 15000 images...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fb6075c78b4a7799dde602e7afc6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/15620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total examples after filtering: 15620\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Load and Prepare Dataset\n",
    "# --------------------------\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=\"/Users/natalyagrokh/AI/ml_expressions/img_datasets/ferckjalf_dataset\",\n",
    "    split=\"train\",\n",
    "    cache_dir=\"/tmp/hf_cache\"\n",
    ")\n",
    "\n",
    "counter = {\"n\": 0}\n",
    "\n",
    "def reconcile_labels(example):\n",
    "    counter[\"n\"] += 1\n",
    "    if counter[\"n\"] % 1000 == 0:\n",
    "        print(f\"Processed {counter['n']} images...\")\n",
    "\n",
    "    label = example.get(\"label\", None)\n",
    "\n",
    "    if isinstance(label, int):\n",
    "        original_label = dataset.features[\"label\"].int2str(label).strip().lower()\n",
    "    elif isinstance(label, str):\n",
    "        original_label = label.strip().lower()\n",
    "    else:\n",
    "        file_path = getattr(example[\"image\"], \"filename\", None)\n",
    "        original_label = os.path.basename(os.path.dirname(file_path)).lower() if file_path else None\n",
    "\n",
    "    pretrain_label = label_mapping.get(original_label)\n",
    "    example[\"label\"] = label2id[pretrain_label] if pretrain_label is not None else -1\n",
    "    return example\n",
    "\n",
    "# Single-threaded labeling to preserve .filename\n",
    "dataset = dataset.map(reconcile_labels, desc=\"Re-labeling dataset\")\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "print(f\"‚úÖ Total examples after filtering: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c46f6094-8498-494a-9891-11311592850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label schema (from dataset): ClassLabel(names=['anger', 'disgust', 'fear', 'happiness', 'neutral', 'questioning', 'sadness', 'surprise', 'unknown'], id=None)\n",
      "\n",
      "üìä Full dataset label distribution (from Dataset object):\n",
      "  anger: 2196 examples\n",
      "  disgust: 251 examples\n",
      "  fear: 1271 examples\n",
      "  happiness: 2621 examples\n",
      "  sadness: 1461 examples\n",
      "  surprise: 2572 examples\n",
      "  neutral: 3153 examples\n",
      "  questioning: 1709 examples\n",
      "  unknown: 386 examples\n",
      "\n",
      "‚ö†Ô∏è  Dynamically identified minority classes: ['unknown', 'disgust', 'fear']\n",
      "\n",
      "üìÇ Image count per label folder:\n",
      "  anger: 2196 images\n",
      "  disgust: 251 images\n",
      "  fear: 1271 images\n",
      "  happiness: 2621 images\n",
      "  neutral: 3153 images\n",
      "  questioning: 1709 images\n",
      "  sadness: 1461 images\n",
      "  surprise: 2572 images\n",
      "  unknown: 386 images\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Dataset Label Overview and Folder Stats\n",
    "# --------------------------\n",
    "def analyze_dataset_structure(dataset, id2label, base_path):\n",
    "    # Print label schema from the dataset\n",
    "    print(\"Label schema (from dataset):\", dataset.features[\"label\"])\n",
    "\n",
    "    # Label distribution from the dataset object\n",
    "    label_counts = Counter(dataset[\"label\"])\n",
    "    print(\"\\nüìä Full dataset label distribution (from Dataset object):\")\n",
    "    for label_id, count in sorted(label_counts.items()):\n",
    "        print(f\"  {id2label[label_id]}: {count} examples\")\n",
    "\n",
    "    # Dynamically detect minority classes (lowest 3 frequencies)\n",
    "    N = 3\n",
    "    minority_classes = set(\n",
    "        label for label, _ in sorted(label_counts.items(), key=lambda x: x[1])[:N]\n",
    "    )\n",
    "    print(f\"\\n‚ö†Ô∏è  Dynamically identified minority classes: {[id2label[i] for i in minority_classes]}\")\n",
    "\n",
    "    # Count images per directory, and store for later validation\n",
    "    folder_image_counts = {}\n",
    "    print(\"\\nüìÇ Image count per label folder:\")\n",
    "    for label in sorted(os.listdir(base_path)):\n",
    "        label_path = os.path.join(base_path, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            valid_images = [img for img in os.listdir(label_path) if is_valid_image(img)]\n",
    "            folder_image_counts[label] = len(valid_images)\n",
    "            print(f\"  {label}: {len(valid_images)} images\")\n",
    "\n",
    "    return minority_classes, folder_image_counts\n",
    "\n",
    "# Example usage right after dataset loading\n",
    "minority_classes, folder_image_counts = analyze_dataset_structure(dataset, id2label, BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1117fa-a523-4dbc-b59d-c2117f1e8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Disgust hash clusters with more than 1 image:\n",
      "üîç Sadness hash clusters with more than 1 image:\n",
      "  - Cluster 958c52e1: 2 images copied for review\n",
      "  - Cluster ee9a8d33: 2 images copied for review\n",
      "  - Cluster d0890396: 2 images copied for review\n",
      "  - Cluster bb0d06f2: 2 images copied for review\n",
      "  - Cluster d7f00fa2: 2 images copied for review\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 6. Disgust & Sadness Curation: Perceptual Clustering \n",
    "# --------------------------\n",
    "\n",
    "# Directory to inspect for disgust images\n",
    "disgust_dir = os.path.join(BASE_PATH, \"disgust\")\n",
    "disgust_images = [\n",
    "    os.path.join(disgust_dir, f) for f in os.listdir(disgust_dir)\n",
    "    if is_valid_image(f)\n",
    "]\n",
    "\n",
    "# Compute perceptual hashes for clustering\n",
    "hash_map = {}\n",
    "for path in disgust_images:\n",
    "    h = compute_hash(path)\n",
    "    if h:\n",
    "        hash_map.setdefault(h, []).append(path)\n",
    "\n",
    "# Identify clusters with >1 similar image (potential duplicates or mislabels)\n",
    "cluster_dir = os.path.join(SAVE_DIR, \"disgust_clusters\")\n",
    "os.makedirs(cluster_dir, exist_ok=True)\n",
    "\n",
    "print(\"üîç Disgust hash clusters with more than 1 image:\")\n",
    "for h, paths in hash_map.items():\n",
    "    if len(paths) > 1:\n",
    "        cluster_path = os.path.join(cluster_dir, h)\n",
    "        os.makedirs(cluster_path, exist_ok=True)\n",
    "        for p in paths:\n",
    "            shutil.copy(p, cluster_path)\n",
    "        print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")\n",
    "\n",
    "# Curate Sadness (new minority)\n",
    "sadness_dir = os.path.join(BASE_PATH, \"sadness\")\n",
    "sadness_images = [\n",
    "    os.path.join(sadness_dir, f) for f in os.listdir(sadness_dir)\n",
    "    if is_valid_image(f)\n",
    "]\n",
    "\n",
    "hash_map_s = {}\n",
    "for path in sadness_images:\n",
    "    h = compute_hash(path)\n",
    "    if h:\n",
    "        hash_map_s.setdefault(h, []).append(path)\n",
    "\n",
    "cluster_dir_s = os.path.join(SAVE_DIR, \"sadness_clusters\")\n",
    "os.makedirs(cluster_dir_s, exist_ok=True)\n",
    "\n",
    "print(\"üîç Sadness hash clusters with more than 1 image:\")\n",
    "for h, paths in hash_map_s.items():\n",
    "    if len(paths) > 1:\n",
    "        cluster_path = os.path.join(cluster_dir_s, h)\n",
    "        os.makedirs(cluster_path, exist_ok=True)\n",
    "        for p in paths:\n",
    "            shutil.copy(p, cluster_path)\n",
    "        print(f\"  - Cluster {h[:8]}: {len(paths)} images copied for review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "794327e8-da09-4435-b47e-54f5add8b7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Targeted minority augmentation will apply to: ['disgust', 'questioning', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 7. Class Frequency-Aware Augmentation Targeting\n",
    "# --------------------------\n",
    "\n",
    "# Compute label frequencies from train split (post filtering)\n",
    "# ‚öñÔ∏è Compute dynamic class weights from training labels/set\n",
    "label_freqs = Counter(train_dataset['label'])\n",
    "total = sum(label_freqs.values())\n",
    "class_weights = torch.tensor([total / label_freqs[i] for i in range(len(label_freqs))], dtype=torch.float).to(device)\n",
    "\n",
    "label_id2name = {v: k for k, v in label2id.items()}\n",
    "label_name2id = {v: k for k, v in label_id2name.items()}\n",
    "\n",
    "# Get lowest-count classes dynamically\n",
    "minority_by_count = sorted(label_freqs, key=label_freqs.get)[:3]\n",
    "minority_by_name = [label_id2name[i] for i in minority_by_count]\n",
    "\n",
    "# Manually include known confused or underperforming classes\n",
    "manual_focus_classes = ['disgust', 'questioning']\n",
    "\n",
    "# Merge, deduplicate, and EXCLUDE 'unknown'\n",
    "all_minority_names = set(minority_by_name + manual_focus_classes)\n",
    "if \"unknown\" in all_minority_names:\n",
    "    all_minority_names.remove(\"unknown\")\n",
    "minority_class_names = list(all_minority_names)\n",
    "\n",
    "# Final list as label indices\n",
    "minority_classes = [label_name2id[name] for name in minority_class_names]\n",
    "\n",
    "print(f\"üéØ Targeted minority augmentation will apply to: {minority_class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c881b20a-ded8-464d-bf03-9c9f6ab2eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e205ed4328f246bf861aeb3f9c252eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Augmentation counts: {'anger': 2196, 'disgust': 251, 'fear': 1271, 'happiness': 2621, 'neutral': 3153, 'questioning': 1709, 'sadness': 1461, 'surprise': 2572, 'unknown': 386}\n",
      "‚úÖ Saved augmentation snapshot to /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/V14_augmentation_snapshot.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Define Data Augmentation and Preprocessing Transformation\n",
    "# --------------------------\n",
    "\n",
    "# Baseline augmentation\n",
    "data_augment = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomApply([T.ColorJitter(0.4, 0.4, 0.4)], p=0.8),\n",
    "    T.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.GaussianBlur(3),\n",
    "])\n",
    "\n",
    "# RandAugment for specific minority classes only\n",
    "minority_classes_names = ['disgust', 'questioning', 'fear', 'sadness']\n",
    "minority_classes = [label2id[label] for label in minority_classes_names]\n",
    "\n",
    "minority_aug = T.Compose([\n",
    "    RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    T.ColorJitter(0.3, 0.3, 0.3, 0.1),\n",
    "])\n",
    "\n",
    "# Augmentation counter tracking\n",
    "aug_count = Counter()\n",
    "\n",
    "def make_transform_function(processor, minority_classes):\n",
    "    def transform_function(example):\n",
    "        label = example[\"label\"]\n",
    "        # Do NOT augment 'unknown'\n",
    "        if LABEL_NAMES[label] == \"unknown\":\n",
    "            aug_pipeline = T.Compose([])  # Identity/no-op\n",
    "        else:\n",
    "            aug_pipeline = minority_aug if label in minority_classes else data_augment\n",
    "        aug_count[label] += 1\n",
    "\n",
    "        if example[\"image\"].mode != \"RGB\":\n",
    "            example[\"image\"] = example[\"image\"].convert(\"RGB\")\n",
    "\n",
    "        augmented_image = aug_pipeline(example[\"image\"])\n",
    "        # Convert to tensor inside processor\n",
    "        inputs = processor(augmented_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"labels\"] = example[\"label\"]\n",
    "\n",
    "        # Optional: If you want to use RandomErasing, apply here:\n",
    "        # if label != label2id[\"unknown\"]:\n",
    "        #     inputs[\"pixel_values\"] = T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random')(inputs[\"pixel_values\"])\n",
    "\n",
    "        return inputs\n",
    "    return transform_function\n",
    "\n",
    "# After mapping finishes:\n",
    "dataset = dataset.map(make_transform_function(processor, minority_classes))\n",
    "formatted_counts = {LABEL_NAMES[k]: v for k, v in aug_count.items()}\n",
    "print(f\"‚úÖ Augmentation counts: {formatted_counts}\")\n",
    "\n",
    "# Explicitly log dataset snapshots (class distribution) to a \n",
    "# CSV or JSON after each run for easy future diffing and tracking\n",
    "snapshot_path = os.path.join(SAVE_DIR, f\"{VERSION}_augmentation_snapshot.csv\")\n",
    "aug_snapshot = pd.DataFrame.from_dict(dict(aug_count), orient='index', columns=['count'])\n",
    "aug_snapshot.to_csv(snapshot_path)\n",
    "\n",
    "print(f\"‚úÖ Saved augmentation snapshot to {snapshot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b8d986b-284c-4946-b481-da3088e95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 9. Train-Validation Split\n",
    "# --------------------------\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e37d1e-a6fb-4e9f-aec5-ee945f11c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Saved label distribution snapshot: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/label_snapshots/V14_label_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 10. Label Distribution Snapshot and Drift Monitor\n",
    "# --------------------------\n",
    "snapshot_dir = os.path.join(SAVE_DIR, \"label_snapshots\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Count current training labels\n",
    "train_label_names = [LABEL_NAMES[i] for i in train_dataset['label']]\n",
    "label_counts = pd.Series(train_label_names).value_counts().sort_index()\n",
    "label_counts.name = VERSION\n",
    "\n",
    "# Save snapshot CSV\n",
    "snapshot_path = os.path.join(snapshot_dir, f\"{VERSION}_label_distribution.csv\")\n",
    "label_counts.to_csv(snapshot_path)\n",
    "print(f\"üìä Saved label distribution snapshot: {snapshot_path}\")\n",
    "\n",
    "# Optionally compare to previous version\n",
    "previous_versions = sorted([\n",
    "    f for f in os.listdir(snapshot_dir) if f.endswith(\".csv\") and not f.startswith(VERSION)\n",
    "])\n",
    "if previous_versions:\n",
    "    latest_prev = previous_versions[-1]\n",
    "    prev_df = pd.read_csv(os.path.join(snapshot_dir, latest_prev), index_col=0)\n",
    "    diff = label_counts.subtract(prev_df.iloc[:, 0], fill_value=0)\n",
    "    print(\"üîç Label count change since last snapshot:\")\n",
    "    print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72643383-d4fc-466e-b91c-654e47a7a94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution: Counter({5: 3000, 3: 3000, 1: 3000, 2: 3000, 6: 3000, 4: 3000, 7: 3000, 0: 3000, 8: 302})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfe30f881ee4d13883aa8325db5f5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c456d8a1156d474ab64afcec25b8af02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e185481f350142dbbc66300b086d08e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c43909f2d346ca9d6bfd4d92250e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be796d2a6a3e45a28fa48a984cfdd111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2029b148d65d46c9826846fb9ef73f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1d3a90fcec47328b360b9645614207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a73e381f924d4795137411f0c1aea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e25f4f53e3f498b86795aefde47abfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After balancing: Counter({5: 3000, 4: 3000, 6: 3000, 2: 3000, 0: 3000, 1: 3000, 7: 3000, 3: 3000, 8: 302})\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 11. Balance Dataset (with NO oversampling for 'unknown')\n",
    "# --------------------------\n",
    "mp.set_start_method('fork', force=True)\n",
    "\n",
    "label_target = 3000\n",
    "balanced_subsets = []\n",
    "\n",
    "label_counts = Counter(train_dataset[\"label\"])\n",
    "print(\"Original label distribution:\", label_counts)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    subset = train_dataset.filter(lambda x: x['label'] == label, num_proc=1)\n",
    "    if LABEL_NAMES[label] == \"unknown\":\n",
    "        # Just append all 'unknown' examples as-is (no upsampling)\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count > label_target:\n",
    "        subset = subset.select(random.sample(range(len(subset)), label_target))\n",
    "        balanced_subsets.append(subset)\n",
    "    elif count < label_target:\n",
    "        multiplier = label_target // len(subset)\n",
    "        remainder = label_target % len(subset)\n",
    "        subset = concatenate_datasets([subset] * multiplier + [subset.select(range(remainder))])\n",
    "        balanced_subsets.append(subset)\n",
    "    else:\n",
    "        balanced_subsets.append(subset)\n",
    "\n",
    "train_dataset = concatenate_datasets(balanced_subsets).shuffle(seed=42)\n",
    "print(\"After balancing:\", Counter(train_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8f601ac-21c3-461b-bf29-633457616f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 12. Define Training Arguments for Robust Fine-Tuning\n",
    "# --------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=SAVE_DIR,                   # Directory to save checkpoints and the final model\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint at each epoch\n",
    "    save_total_limit=2,                    # ‚úÖ (optional) Keep only last 2 checkpoints to save space\n",
    "    learning_rate=4e-5,                    # A conservative learning rate for fine-tuning\n",
    "    per_device_train_batch_size=8,         # Adjust based on your CPU memory limits\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,                    # Fine-tune for a few epochs (adjust as needed)\n",
    "    load_best_model_at_end=True,           # Automatically load the best model when training finishes\n",
    "    metric_for_best_model=\"accuracy\",      # Monitor accuracy for best model selection\n",
    "    logging_dir=os.path.join(SAVE_DIR, \"logs\"),  # ‚úÖ Save logs inside versioned folder\n",
    "    logging_strategy=\"epoch\",                 # ‚úÖ Log once per epoch\n",
    "    save_safetensors=True                  # ‚úÖ Optional: saves model weights in `.safetensors` (safe format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df07f1f-619e-4d8d-a2c5-3a408deafa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 13. Define Compute Metrics\n",
    "# --------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67aa498-c49c-4a70-9eaa-0bef78529605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15190' max='15190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15190/15190 4:09:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.664700</td>\n",
       "      <td>6.867499</td>\n",
       "      <td>0.872279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.583500</td>\n",
       "      <td>7.062072</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.045300</td>\n",
       "      <td>6.680960</td>\n",
       "      <td>0.885723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.839500</td>\n",
       "      <td>6.917661</td>\n",
       "      <td>0.885723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.773100</td>\n",
       "      <td>6.732500</td>\n",
       "      <td>0.889245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.88      0.79      0.83       435\n",
      "     disgust       0.97      0.70      0.81        43\n",
      "        fear       0.82      0.81      0.81       251\n",
      "   happiness       0.91      0.94      0.93       520\n",
      "     sadness       0.89      0.87      0.88       311\n",
      "    surprise       0.90      0.87      0.88       495\n",
      "     neutral       0.85      0.86      0.85       621\n",
      " questioning       0.82      0.93      0.87       364\n",
      "     unknown       0.99      1.00      0.99        84\n",
      "\n",
      "    accuracy                           0.87      3124\n",
      "   macro avg       0.89      0.86      0.87      3124\n",
      "weighted avg       0.87      0.87      0.87      3124\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - anger ‚Üí neutral: 40 instances\n",
      "  - fear ‚Üí questioning: 34 instances\n",
      "  - surprise ‚Üí neutral: 33 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3458\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.4234\n",
      "  - anger: entropy = 0.3982\n",
      "  - fear: entropy = 0.3619\n",
      "  - neutral: entropy = 0.3501\n",
      "  - surprise: entropy = 0.3438\n",
      "  - sadness: entropy = 0.3393\n",
      "  - happiness: entropy = 0.3301\n",
      "  - questioning: entropy = 0.3068\n",
      "  - unknown: entropy = 0.2567\n",
      "\n",
      "üîç 'unknown' row (actual unknown, predicted as):\n",
      "  predicted anger: 0\n",
      "  predicted disgust: 0\n",
      "  predicted fear: 0\n",
      "  predicted happiness: 0\n",
      "  predicted sadness: 0\n",
      "  predicted surprise: 0\n",
      "  predicted neutral: 0\n",
      "  predicted questioning: 0\n",
      "  predicted unknown: 84\n",
      "\n",
      "üîç 'unknown' col (predicted unknown, actual label):\n",
      "  true anger: 0\n",
      "  true disgust: 0\n",
      "  true fear: 0\n",
      "  true happiness: 0\n",
      "  true sadness: 0\n",
      "  true surprise: 0\n",
      "  true neutral: 0\n",
      "  true questioning: 1\n",
      "  true unknown: 84\n",
      "------------------------------------------------------------\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.84      0.83      0.84       435\n",
      "     disgust       0.97      0.72      0.83        43\n",
      "        fear       0.76      0.83      0.79       251\n",
      "   happiness       0.92      0.93      0.93       520\n",
      "     sadness       0.89      0.88      0.89       311\n",
      "    surprise       0.88      0.87      0.88       495\n",
      "     neutral       0.88      0.84      0.86       621\n",
      " questioning       0.85      0.90      0.88       364\n",
      "     unknown       0.97      1.00      0.98        84\n",
      "\n",
      "    accuracy                           0.87      3124\n",
      "   macro avg       0.89      0.87      0.87      3124\n",
      "weighted avg       0.87      0.87      0.87      3124\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - neutral ‚Üí anger: 35 instances\n",
      "  - anger ‚Üí neutral: 34 instances\n",
      "  - neutral ‚Üí surprise: 30 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3278\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.3834\n",
      "  - anger: entropy = 0.3472\n",
      "  - sadness: entropy = 0.3462\n",
      "  - fear: entropy = 0.3425\n",
      "  - surprise: entropy = 0.3290\n",
      "  - neutral: entropy = 0.3245\n",
      "  - questioning: entropy = 0.3216\n",
      "  - happiness: entropy = 0.3048\n",
      "  - unknown: entropy = 0.2729\n",
      "\n",
      "üîç 'unknown' row (actual unknown, predicted as):\n",
      "  predicted anger: 0\n",
      "  predicted disgust: 0\n",
      "  predicted fear: 0\n",
      "  predicted happiness: 0\n",
      "  predicted sadness: 0\n",
      "  predicted surprise: 0\n",
      "  predicted neutral: 0\n",
      "  predicted questioning: 0\n",
      "  predicted unknown: 84\n",
      "\n",
      "üîç 'unknown' col (predicted unknown, actual label):\n",
      "  true anger: 0\n",
      "  true disgust: 0\n",
      "  true fear: 1\n",
      "  true happiness: 0\n",
      "  true sadness: 0\n",
      "  true surprise: 0\n",
      "  true neutral: 1\n",
      "  true questioning: 1\n",
      "  true unknown: 84\n",
      "------------------------------------------------------------\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.87      0.83      0.85       435\n",
      "     disgust       0.92      0.81      0.86        43\n",
      "        fear       0.86      0.82      0.84       251\n",
      "   happiness       0.90      0.95      0.92       520\n",
      "     sadness       0.89      0.92      0.90       311\n",
      "    surprise       0.90      0.88      0.89       495\n",
      "     neutral       0.88      0.86      0.87       621\n",
      " questioning       0.88      0.90      0.89       364\n",
      "     unknown       0.99      1.00      0.99        84\n",
      "\n",
      "    accuracy                           0.89      3124\n",
      "   macro avg       0.90      0.89      0.89      3124\n",
      "weighted avg       0.89      0.89      0.89      3124\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - anger ‚Üí neutral: 37 instances\n",
      "  - neutral ‚Üí anger: 28 instances\n",
      "  - fear ‚Üí questioning: 25 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3260\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.3523\n",
      "  - anger: entropy = 0.3498\n",
      "  - questioning: entropy = 0.3374\n",
      "  - sadness: entropy = 0.3290\n",
      "  - surprise: entropy = 0.3247\n",
      "  - fear: entropy = 0.3232\n",
      "  - neutral: entropy = 0.3227\n",
      "  - unknown: entropy = 0.3096\n",
      "  - happiness: entropy = 0.3031\n",
      "\n",
      "üîç 'unknown' row (actual unknown, predicted as):\n",
      "  predicted anger: 0\n",
      "  predicted disgust: 0\n",
      "  predicted fear: 0\n",
      "  predicted happiness: 0\n",
      "  predicted sadness: 0\n",
      "  predicted surprise: 0\n",
      "  predicted neutral: 0\n",
      "  predicted questioning: 0\n",
      "  predicted unknown: 84\n",
      "\n",
      "üîç 'unknown' col (predicted unknown, actual label):\n",
      "  true anger: 0\n",
      "  true disgust: 0\n",
      "  true fear: 0\n",
      "  true happiness: 0\n",
      "  true sadness: 0\n",
      "  true surprise: 0\n",
      "  true neutral: 0\n",
      "  true questioning: 1\n",
      "  true unknown: 84\n",
      "------------------------------------------------------------\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.86      0.85      0.86       435\n",
      "     disgust       0.89      0.74      0.81        43\n",
      "        fear       0.90      0.78      0.84       251\n",
      "   happiness       0.93      0.93      0.93       520\n",
      "     sadness       0.89      0.90      0.89       311\n",
      "    surprise       0.90      0.87      0.89       495\n",
      "     neutral       0.84      0.92      0.88       621\n",
      " questioning       0.88      0.87      0.88       364\n",
      "     unknown       0.99      1.00      0.99        84\n",
      "\n",
      "    accuracy                           0.89      3124\n",
      "   macro avg       0.90      0.87      0.88      3124\n",
      "weighted avg       0.89      0.89      0.89      3124\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - anger ‚Üí neutral: 42 instances\n",
      "  - surprise ‚Üí neutral: 36 instances\n",
      "  - fear ‚Üí questioning: 26 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3182\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.3464\n",
      "  - fear: entropy = 0.3351\n",
      "  - anger: entropy = 0.3238\n",
      "  - surprise: entropy = 0.3221\n",
      "  - questioning: entropy = 0.3160\n",
      "  - happiness: entropy = 0.3156\n",
      "  - sadness: entropy = 0.3150\n",
      "  - unknown: entropy = 0.3093\n",
      "  - neutral: entropy = 0.3089\n",
      "\n",
      "üîç 'unknown' row (actual unknown, predicted as):\n",
      "  predicted anger: 0\n",
      "  predicted disgust: 0\n",
      "  predicted fear: 0\n",
      "  predicted happiness: 0\n",
      "  predicted sadness: 0\n",
      "  predicted surprise: 0\n",
      "  predicted neutral: 0\n",
      "  predicted questioning: 0\n",
      "  predicted unknown: 84\n",
      "\n",
      "üîç 'unknown' col (predicted unknown, actual label):\n",
      "  true anger: 0\n",
      "  true disgust: 0\n",
      "  true fear: 0\n",
      "  true happiness: 0\n",
      "  true sadness: 0\n",
      "  true surprise: 0\n",
      "  true neutral: 0\n",
      "  true questioning: 1\n",
      "  true unknown: 84\n",
      "------------------------------------------------------------\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.90      0.83      0.86       435\n",
      "     disgust       0.91      0.70      0.79        43\n",
      "        fear       0.88      0.80      0.84       251\n",
      "   happiness       0.93      0.94      0.93       520\n",
      "     sadness       0.91      0.89      0.90       311\n",
      "    surprise       0.89      0.89      0.89       495\n",
      "     neutral       0.84      0.92      0.88       621\n",
      " questioning       0.87      0.88      0.88       364\n",
      "     unknown       0.99      1.00      0.99        84\n",
      "\n",
      "    accuracy                           0.89      3124\n",
      "   macro avg       0.90      0.87      0.89      3124\n",
      "weighted avg       0.89      0.89      0.89      3124\n",
      "\n",
      "\n",
      "Top 3 confused class pairs:\n",
      "  - anger ‚Üí neutral: 47 instances\n",
      "  - surprise ‚Üí neutral: 32 instances\n",
      "  - fear ‚Üí questioning: 30 instances\n",
      "\n",
      "üß† Avg prediction entropy: 0.3182\n",
      "\n",
      "üîç Class entropies (sorted):\n",
      "  - disgust: entropy = 0.3676\n",
      "  - fear: entropy = 0.3290\n",
      "  - anger: entropy = 0.3268\n",
      "  - questioning: entropy = 0.3262\n",
      "  - surprise: entropy = 0.3189\n",
      "  - sadness: entropy = 0.3136\n",
      "  - neutral: entropy = 0.3125\n",
      "  - happiness: entropy = 0.3075\n",
      "  - unknown: entropy = 0.3027\n",
      "\n",
      "üîç 'unknown' row (actual unknown, predicted as):\n",
      "  predicted anger: 0\n",
      "  predicted disgust: 0\n",
      "  predicted fear: 0\n",
      "  predicted happiness: 0\n",
      "  predicted sadness: 0\n",
      "  predicted surprise: 0\n",
      "  predicted neutral: 0\n",
      "  predicted questioning: 0\n",
      "  predicted unknown: 84\n",
      "\n",
      "üîç 'unknown' col (predicted unknown, actual label):\n",
      "  true anger: 0\n",
      "  true disgust: 0\n",
      "  true fear: 0\n",
      "  true happiness: 0\n",
      "  true sadness: 0\n",
      "  true surprise: 0\n",
      "  true neutral: 0\n",
      "  true questioning: 1\n",
      "  true unknown: 84\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15190, training_loss=3.581230455892034, metrics={'train_runtime': 14967.6058, 'train_samples_per_second': 8.118, 'train_steps_per_second': 1.015, 'total_flos': 9.416642409717903e+18, 'train_loss': 3.581230455892034, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 14. Trainer with Class-Weighted Loss\n",
    "# --------------------------\n",
    "\n",
    "# Define custom Trainer to inject class weights\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Use smoothed CE + confidence penalty\n",
    "        smooth_ce_loss = SmoothedCrossEntropyLoss(smoothing=0.05, weight=class_weights)\n",
    "        loss = smooth_ce_loss(logits, labels) + confidence_penalty(logits, beta=0.05)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Modify training args for learning rate scheduling and early stopping\n",
    "training_args.load_best_model_at_end = True\n",
    "training_args.metric_for_best_model = \"eval_loss\"\n",
    "training_args.evaluation_strategy = \"epoch\"\n",
    "training_args.save_strategy = \"epoch\"\n",
    "\n",
    "# Add EarlyStoppingCallback\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "# Initialize WeightedTrainer with focal loss, confidence penalty, and label smoothing\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics_with_confusion,\n",
    "    optimizers=(\n",
    "        AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=0.01),\n",
    "        None\n",
    "    ),\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "# T_0 = epochs before first restart, T_mult = restart multiplier\n",
    "scheduler = CosineAnnealingWarmRestarts(trainer.optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "# Add scheduler step logic inside the training loop:\n",
    "original_train = trainer.train\n",
    "\n",
    "def modified_train(*args, **kwargs):\n",
    "    result = original_train(*args, **kwargs)\n",
    "    scheduler.step(trainer.state.epoch)  # instead of eval_loss\n",
    "    return result\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9356fc7-0f49-43e3-8679-d1a4d8507ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------\n",
    "# # Rescue & Save from Last Checkpoint (after training)\n",
    "# # --------------------------\n",
    "# #in case model save fails, resume from latest checkpoint\n",
    "# processor.save_pretrained(SAVE_DIR)\n",
    "# print(\"‚úÖ Processor manually re-saved.\")\n",
    "\n",
    "# # Use parent directory of SAVE_DIR to locate latest V* folder\n",
    "# parent_dir = os.path.dirname(SAVE_DIR)\n",
    "# v_folders = [\n",
    "#     d for d in os.listdir(parent_dir)\n",
    "#     if os.path.isdir(os.path.join(parent_dir, d)) and d.startswith(\"V\")\n",
    "# ]\n",
    "\n",
    "# def extract_timestamp(name):\n",
    "#     try:\n",
    "#         _, date_str, time_str = name.split(\"_\")\n",
    "#         return datetime.strptime(f\"{date_str}_{time_str}\", \"%Y%m%d_%H%M%S\")\n",
    "#     except Exception:\n",
    "#         return datetime.min\n",
    "\n",
    "# latest_version_folder = max(v_folders, key=extract_timestamp)\n",
    "# latest_version_path = os.path.join(parent_dir, latest_version_folder)\n",
    "# print(f\"üóÇÔ∏è Using latest version folder: {latest_version_path}\")\n",
    "\n",
    "# # Locate latest checkpoint within that version folder\n",
    "# checkpoint_dirs = [\n",
    "#     os.path.join(latest_version_path, d)\n",
    "#     for d in os.listdir(latest_version_path)\n",
    "#     if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(latest_version_path, d))\n",
    "# ]\n",
    "# if not checkpoint_dirs:\n",
    "#     raise ValueError(\"‚ùå No checkpoint found in latest version folder.\")\n",
    "\n",
    "# latest_checkpoint = max(checkpoint_dirs, key=os.path.getmtime)\n",
    "# print(f\"‚úÖ Found latest checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# # Load model and processor from latest checkpoint and save them\n",
    "# model = AutoModelForImageClassification.from_pretrained(latest_checkpoint)\n",
    "# processor = AutoImageProcessor.from_pretrained(latest_version_path)\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12eaaf8a-eb3d-43b5-a99c-b513d20621c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processor saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421\n",
      "‚úÖ State dict saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/final_model.pth\n",
      "‚úÖ Full model saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421\n",
      "‚úÖ Trainer backup saved.\n",
      "‚úÖ Memory cleanup complete after save.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 15. Save Final Independent Model (Safe Save Mode)\n",
    "# --------------------------\n",
    "\n",
    "model = model.to(\"cpu\")  # move to CPU first\n",
    "\n",
    "# Save processor\n",
    "processor.save_pretrained(SAVE_DIR)\n",
    "print(f\"‚úÖ Processor saved to: {SAVE_DIR}\")\n",
    "\n",
    "# Save state dict\n",
    "final_model_path = os.path.join(SAVE_DIR, 'final_model.pth')\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ State dict saved to: {final_model_path}\")\n",
    "\n",
    "# Save full model\n",
    "model.save_pretrained(SAVE_DIR, safe_serialization=True)\n",
    "print(f\"‚úÖ Full model saved to: {SAVE_DIR}\")\n",
    "\n",
    "# Save trainer state (if defined)\n",
    "if 'trainer' in globals():\n",
    "    try:\n",
    "        trainer.save_model(os.path.join(SAVE_DIR, \"backup_trainer_model\"))\n",
    "        print(\"‚úÖ Trainer backup saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to save trainer backup: {e}\")\n",
    "    \n",
    "# Free memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úÖ Memory cleanup complete after save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73965871-eced-4b6b-9e56-b98aac0bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421 were not used when initializing ViTForImageClassification: ['classifier.1.bias', 'classifier.1.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model reloaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 16. Inference Utilities\n",
    "# --------------------------\n",
    "\n",
    "# Reload Model for Inference\n",
    "model = AutoModelForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()\n",
    "print(\"‚úÖ Model reloaded for inference.\")\n",
    "\n",
    "# Single image prediction (unbatched)\n",
    "def predict_label(image_path, threshold=0.85):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        conf, pred_idx = torch.max(probs, dim=-1)\n",
    "    return (id2label[pred_idx.item()], conf.item()) if conf.item() >= threshold else (\"REVIEW\", conf.item())\n",
    "\n",
    "# Batched prediction\n",
    "def batch_predict(image_folder, batch_size=64, threshold=0.85):\n",
    "    all_preds = []\n",
    "    error_count = 0\n",
    "    image_paths = [\n",
    "        p for p in Path(image_folder).rglob(\"*\")\n",
    "        if is_valid_image(p.name)\n",
    "    ]\n",
    "\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Running inference in batches\"):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        images, valid_paths = [], []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "                images.append(img)\n",
    "                valid_paths.append(str(path))\n",
    "            except Exception:\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            confs, preds = torch.max(probs, dim=-1)\n",
    "\n",
    "        for pred, conf, path in zip(preds.tolist(), confs.tolist(), valid_paths):\n",
    "            all_preds.append(LABEL_NAMES[pred] if conf >= threshold else \"REVIEW\")\n",
    "\n",
    "    print(f\"‚úÖ Inference complete. Skipped {error_count} invalid image(s).\")\n",
    "    return all_preds\n",
    "\n",
    "# Distribution plot\n",
    "def plot_distribution(predictions, output_path):\n",
    "    label_counts = Counter(predictions)\n",
    "    labels = sorted(label_counts.keys())\n",
    "    counts = [label_counts[label] for label in labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts)\n",
    "    plt.title(\"Predicted Expression Distribution\")\n",
    "    plt.xlabel(\"Expression\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ebf5e58-a801-455b-af5e-3fde26bea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inference in batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 245/245 [05:20<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference complete. Skipped 0 invalid image(s).\n",
      "üìù Saved REVIEW file paths to V14_review_candidates.txt\n",
      "Distribution plot saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/V14_distribution_plot_20250531_160421.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 17. Entry Point for Inference\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\" and RUN_INFERENCE:\n",
    "\n",
    "    # Auto-locate latest model directory\n",
    "    OUTPUT_PATH = os.path.join(SAVE_DIR, f\"{VERSION}_distribution_plot_{timestamp}.png\")\n",
    "\n",
    "    predictions = batch_predict(IMAGE_DIR)\n",
    "    reviewed_paths = []\n",
    "    image_paths = [str(p) for p in Path(IMAGE_DIR).rglob(\"*\") if is_valid_image(p.name)]\n",
    "\n",
    "    for path, label in zip(image_paths, predictions):\n",
    "        if label == \"REVIEW\":\n",
    "            reviewed_paths.append(path)\n",
    "\n",
    "    # Save paths to inspect manually\n",
    "    with open(os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\"), \"w\") as f:\n",
    "        f.write(\"\\n\".join(reviewed_paths))\n",
    "    print(f\"üìù Saved REVIEW file paths to {VERSION}_review_candidates.txt\")\n",
    "\n",
    "    plot_distribution(predictions, OUTPUT_PATH)\n",
    "    print(f\"Distribution plot saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a67976db-f36f-49cf-84c2-04292b75dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Using calibration files from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421\n",
      "üìÇ Loading logits from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/logits_eval_V14.npy\n",
      "üìÇ Loading labels from: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/labels_eval_V14.npy\n",
      "‚úÖ Temperature also saved to /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/temperature_V14.txt\n",
      "‚úÖ Optimal temperature: 1.3240\n",
      "‚úÖ Calibrated Log Loss: 0.5328\n",
      "üìä Saved reliability diagram to /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/V14_reliability_diagram_calibrated.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 18. Temperature Scaling Calibration \n",
    "# --------------------------\n",
    "\n",
    "# Wrapper model for calibrated inference\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, input_ids=None, pixel_values=None, **kwargs):\n",
    "        logits = self.model(pixel_values=pixel_values).logits\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, logits, labels):\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval_fn():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(logits / self.temperature, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval_fn)\n",
    "        print(f\"Optimal temperature (wrapped): {self.temperature.item():.4f}\")\n",
    "        return self\n",
    "\n",
    "# Dynamically locate the most recent V* folder that contains logits/labels\n",
    "base_dir = os.path.dirname(SAVE_DIR)\n",
    "v_folders = sorted([\n",
    "    d for d in os.listdir(base_dir)\n",
    "    if os.path.isdir(os.path.join(base_dir, d)) and d.startswith(\"V\")\n",
    "], key=lambda d: os.path.getmtime(os.path.join(base_dir, d)), reverse=True)\n",
    "\n",
    "logits_path, labels_path = None, None\n",
    "for v in v_folders:\n",
    "    version_tag = v.split('_')[0]\n",
    "    folder_path = os.path.join(base_dir, v)\n",
    "    logits_candidate = os.path.join(folder_path, f\"logits_eval_{version_tag}.npy\")\n",
    "    labels_candidate = os.path.join(folder_path, f\"labels_eval_{version_tag}.npy\")\n",
    "    if os.path.exists(logits_candidate) and os.path.exists(labels_candidate):\n",
    "        INFER_SAVE_DIR = folder_path\n",
    "        INFER_VERSION = version_tag\n",
    "        print(f\"üìÅ Using calibration files from: {SAVE_DIR}\")\n",
    "        logits_path = logits_candidate\n",
    "        labels_path = labels_candidate\n",
    "        break\n",
    "\n",
    "# --------------------------\n",
    "# Run calibration\n",
    "# --------------------------\n",
    "if logits_path and labels_path:\n",
    "    result = apply_temperature_scaling(logits_path, labels_path)\n",
    "    if result is not None:\n",
    "        temperature, logits, labels = result\n",
    "        plot_reliability_diagram(logits, labels, temperature)\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Skipping temperature scaling and diagram (missing logits or labels in {SAVE_DIR})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30b0f4cc-6bc8-4bff-b5e1-f5dfdb685a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 15482 total predictions (CSV) and 15620 REVIEW-tagged paths.\n",
      "üìÇ Grouped 15482 REVIEW images into folders by predicted label in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/V14_20250531_160421/review_predictions_by_class\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 20. Organize REVIEW-tagged Images by Predicted Class\n",
    "# --------------------------\n",
    "\n",
    "REVIEW_SORT_DIR = os.path.join(SAVE_DIR, \"review_predictions_by_class\")\n",
    "os.makedirs(REVIEW_SORT_DIR, exist_ok=True)\n",
    "\n",
    "# Section #19 explicitly depends on CSV log generated in Section #16\n",
    "# Make sure Section #16 ran successfully before proceeding\n",
    "review_txt_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_candidates.txt\")\n",
    "csv_path = os.path.join(SAVE_DIR, f\"{VERSION}_review_predictions_with_preds.csv\")\n",
    "\n",
    "if os.path.exists(review_txt_path) and os.path.exists(csv_path):\n",
    "    with open(review_txt_path, \"r\") as f:\n",
    "        review_paths = {line.strip() for line in f.readlines()}\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    count = 0\n",
    "\n",
    "    print(f\"üîç Found {len(df)} total predictions (CSV) and {len(review_paths)} REVIEW-tagged paths.\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        path = row[\"image_path\"]          \n",
    "        label = row[\"predicted_label\"]    \n",
    "        conf = row[\"confidence\"]\n",
    "        \n",
    "        # ‚úÖ Only process if path was REVIEW-tagged *and* confidence was below threshold\n",
    "        if path in review_paths and label != \"REVIEW\":\n",
    "            dest_dir = os.path.join(REVIEW_SORT_DIR, label)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            shutil.copy(path, dest_dir)\n",
    "            count += 1\n",
    "\n",
    "    print(f\"üìÇ Grouped {count} REVIEW images into folders by predicted label in: {REVIEW_SORT_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing review candidates file or prediction CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aa19d41-af52-42a0-9b9c-c0527b69e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No hard examples identified for mining/oversampling.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 21. Hard Example Mining & Oversampling\n",
    "# --------------------------\n",
    "HARD_EXAMPLES_LOG = os.path.join(SAVE_DIR, f\"{VERSION}_hard_examples.csv\")\n",
    "HARD_EXAMPLES_IMAGE_DIR = os.path.join(SAVE_DIR, \"hard_examples_images\")\n",
    "os.makedirs(HARD_EXAMPLES_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "if len(hard_examples) > 0:\n",
    "    # Create a mini-dataset from hard indices and add it to the mix\n",
    "    hard_subset = eval_dataset.select(hard_examples)\n",
    "    balanced_subsets.append(hard_subset)\n",
    "\n",
    "    # --- Save information about hard examples ---\n",
    "    # If you have eval_dataset with filenames stored, log them.\n",
    "    hard_paths = []\n",
    "    for idx in hard_examples:\n",
    "        # Try all possible keys for filename (depends on how you loaded/processed)\n",
    "        entry = eval_dataset[idx]\n",
    "        if \"image\" in entry and hasattr(entry[\"image\"], \"filename\"):\n",
    "            hard_paths.append(entry[\"image\"].filename)\n",
    "        elif \"image_path\" in entry:\n",
    "            hard_paths.append(entry[\"image_path\"])\n",
    "        else:\n",
    "            hard_paths.append(f\"index_{idx}\")\n",
    "\n",
    "    # Save a CSV log\n",
    "    pd.DataFrame({\n",
    "        \"index\": hard_examples,\n",
    "        \"image_path\": hard_paths,\n",
    "    }).to_csv(HARD_EXAMPLES_LOG, index=False)\n",
    "\n",
    "    print(f\"üìù Hard examples logged to: {HARD_EXAMPLES_LOG}\")\n",
    "\n",
    "    # Optionally copy images to a folder\n",
    "    for p in hard_paths:\n",
    "        try:\n",
    "            if os.path.exists(p):\n",
    "                shutil.copy(p, HARD_EXAMPLES_IMAGE_DIR)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not copy hard example image: {p} | {e}\")\n",
    "\n",
    "    print(f\"üìÇ {len(hard_paths)} hard example images copied to: {HARD_EXAMPLES_IMAGE_DIR}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ No hard examples identified for mining/oversampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c83408-f13e-4a11-9c1d-ce16d3fce2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
