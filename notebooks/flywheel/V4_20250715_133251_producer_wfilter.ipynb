{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693b2bd6-fc39-4e29-ba97-163b5ed57130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import cv2\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d66f7b-0ee9-4e0c-b3fd-d60ea149068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "ANALYSIS_OUTPUT_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel\"\n",
    "MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "CENTROIDS_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/emotion_centroids.pt\"\n",
    "RELEVANCE_THRESHOLD = 0.6 \n",
    "os.makedirs(ANALYSIS_OUTPUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb45407-a427-45e3-a742-b1d5aead16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [int(d[1:].split(\"_\")[0]) for d in existing if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "def get_emotion_predictions(face_image, model, processor, device):\n",
    "    \"\"\"Runs the model and returns a structured dictionary including all probabilities.\"\"\"\n",
    "    inputs = processor(images=face_image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probabilities = F.softmax(logits, dim=1).squeeze()\n",
    "    top_confidence, top_pred_idx = torch.max(probabilities, dim=0)\n",
    "    top_pred_label = model.config.id2label[top_pred_idx.item()]\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-9)).item()\n",
    "    \n",
    "    results = {\n",
    "        \"predicted_label\": top_pred_label,\n",
    "        \"confidence\": top_confidence.item(),\n",
    "        \"entropy\": entropy\n",
    "    }\n",
    "    # This loop ensures all individual probabilities are added to the log\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        label = model.config.id2label[i]\n",
    "        results[f\"prob_{label}\"] = prob.item()\n",
    "    return results\n",
    "\n",
    "# Processes a video with a two-stage approach, ensuring all file paths\n",
    "    # are correctly created and logged.\n",
    "def analyze_video_faces(video_path, save_dir, model, embedding_model, processor, device, centroids, relevance_threshold, process_every_n_frames=1):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"âŒ Error: Video file not found at {video_path}\")\n",
    "        return []\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"âœ… Opened video: {os.path.basename(video_path)} ({total_frames} frames at {fps:.2f} fps)\")\n",
    "\n",
    "    all_results_log = []\n",
    "    \n",
    "    # Create the directory for all face crops at the start\n",
    "    face_crop_dir = os.path.join(save_dir, \"face_crops\")\n",
    "    os.makedirs(face_crop_dir, exist_ok=True)\n",
    "\n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing video frames\")\n",
    "\n",
    "    for frame_count in range(total_frames):\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % process_every_n_frames == 0:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            \n",
    "            if face_locations:\n",
    "                for i, (top, right, bottom, left) in enumerate(face_locations):\n",
    "                    face_image_pil = Image.fromarray(rgb_frame[top:bottom, left:right])\n",
    "                    inputs = processor(images=face_image_pil, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        embedding = embedding_model(**inputs).logits.squeeze()\n",
    "                    \n",
    "                    distances = {\n",
    "                        label_id: torch.nn.functional.cosine_similarity(embedding, centroid, dim=0).item()\n",
    "                        for label_id, centroid in centroids.items()\n",
    "                    }\n",
    "                    max_similarity = max(distances.values())\n",
    "                    \n",
    "                    if max_similarity >= relevance_threshold:\n",
    "                        with torch.no_grad():\n",
    "                            logits = model(**inputs).logits\n",
    "                        \n",
    "                        emotion_results = get_emotion_predictions(face_image_pil, model, processor, device)\n",
    "                        \n",
    "                        # THIS IS THE FIX:\n",
    "                        # 1. Create the full, absolute path for the cropped face.\n",
    "                        face_filename = os.path.join(face_crop_dir, f\"frame_{frame_count}_face_{i}.png\")\n",
    "                        \n",
    "                        # 2. Save the image to that path.\n",
    "                        face_image_pil.save(face_filename)\n",
    "\n",
    "                        # 3. Log this exact, verified path to the results.\n",
    "                        log_entry = {\n",
    "                            \"timestamp_seconds\": frame_count / fps,\n",
    "                            \"frame_number\": frame_count,\n",
    "                            \"face_index\": i,\n",
    "                            \"face_crop_path\": face_filename, # <-- Log the correct, full path\n",
    "                            \"is_relevant\": True,\n",
    "                            \"max_similarity\": max_similarity,\n",
    "                            **emotion_results\n",
    "                        }\n",
    "                        all_results_log.append(log_entry)\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    video_capture.release()\n",
    "    \n",
    "    print(f\"âœ… Video processing complete. Logged {len(all_results_log)} relevant emotional events.\")\n",
    "    return all_results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab199de2-1684-4b38-9592-c178baaf8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Created analysis output directory: /Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/V4_20250714_122050\n",
      "\n",
      "--- Loading assets ---\n",
      "âœ… Models, processor, and centroids loaded onto mps.\n",
      "âœ… Opened video: StreetQs.mp4 (5657 frames at 30.00 fps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5657/5657 [01:54<00:00, 49.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Video processing complete. Logged 354 relevant emotional events.\n",
      "\n",
      "âœ… Successfully saved full analysis to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/V4_20250714_122050/filtered_emotion_log.csv\n",
      "\n",
      "--- DATA DIAGNOSTIC REPORT ---\n",
      "\n",
      "Confidence Score Distribution:\n",
      "count    354.000000\n",
      "mean       0.895038\n",
      "std        0.078503\n",
      "min        0.484652\n",
      "25%        0.881003\n",
      "50%        0.925737\n",
      "75%        0.944186\n",
      "max        0.983957\n",
      "Name: confidence, dtype: float64\n",
      "\n",
      "Entropy Score Distribution:\n",
      "count    354.000000\n",
      "mean       0.513642\n",
      "std        0.258741\n",
      "min        0.116967\n",
      "25%        0.337323\n",
      "50%        0.411385\n",
      "75%        0.558090\n",
      "max        1.622529\n",
      "Name: entropy, dtype: float64\n",
      "\n",
      "Max Emotion Probability Distribution:\n",
      "count    354.000000\n",
      "mean       0.895038\n",
      "std        0.078503\n",
      "min        0.484652\n",
      "25%        0.881003\n",
      "50%        0.925737\n",
      "75%        0.944186\n",
      "max        0.983957\n",
      "Name: max_probability, dtype: float64\n",
      "\n",
      "--- END OF REPORT ---\n",
      "\n",
      "Use the statistical distribution values above to set informed thresholds for the next filtering step.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Setup ---\n",
    "VERSION = get_next_version(ANALYSIS_OUTPUT_ROOT)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION_TAG = f\"{VERSION}_{timestamp}\"\n",
    "SAVE_DIR = os.path.join(ANALYSIS_OUTPUT_ROOT, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ðŸ“ Created analysis output directory: {SAVE_DIR}\")\n",
    "\n",
    "# --- Load Assets ---\n",
    "print(f\"\\n--- Loading assets ---\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "classification_model = AutoModelForImageClassification.from_pretrained(MODEL_PATH).to(device).eval()\n",
    "embedding_model = AutoModelForImageClassification.from_pretrained(MODEL_PATH)\n",
    "embedding_model.classifier = nn.Identity()\n",
    "embedding_model.to(device).eval()\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "emotion_centroids = torch.load(CENTROIDS_PATH, map_location=device)\n",
    "print(f\"âœ… Models, processor, and centroids loaded onto {device}.\")\n",
    "\n",
    "# --- Run Video Analysis ---\n",
    "video_to_process = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/sample_vids/StreetQs.mp4\" \n",
    "analysis_log = analyze_video_faces(\n",
    "    video_path=video_to_process, \n",
    "    save_dir=SAVE_DIR,\n",
    "    model=classification_model,\n",
    "    embedding_model=embedding_model,\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    centroids=emotion_centroids,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "    process_every_n_frames=15 # Using a larger step to speed up this diagnostic run\n",
    ")\n",
    "\n",
    "# --- Save and Analyze Results ---\n",
    "if analysis_log:\n",
    "    log_df = pd.DataFrame(analysis_log)\n",
    "    csv_path = os.path.join(SAVE_DIR, \"filtered_emotion_log.csv\")\n",
    "    log_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nâœ… Successfully saved full analysis to: {csv_path}\")\n",
    "\n",
    "    # --- Perform Diagnostic Analysis on the NEW data ---\n",
    "    print(\"\\n--- DATA DIAGNOSTIC REPORT ---\")\n",
    "    \n",
    "    # 1. Analyze 'confidence'\n",
    "    print(\"\\nConfidence Score Distribution:\")\n",
    "    print(log_df['confidence'].describe())\n",
    "    \n",
    "    # 2. Analyze 'entropy'\n",
    "    print(\"\\nEntropy Score Distribution:\")\n",
    "    print(log_df['entropy'].describe())\n",
    "    \n",
    "    # 3. Analyze the maximum probability for each prediction\n",
    "    prob_columns = [col for col in log_df.columns if col.startswith('prob_')]\n",
    "    if prob_columns:\n",
    "        log_df['max_probability'] = log_df[prob_columns].max(axis=1)\n",
    "        print(\"\\nMax Emotion Probability Distribution:\")\n",
    "        print(log_df['max_probability'].describe())\n",
    "    else:\n",
    "        print(\"\\nError: No 'prob_' columns found in the log DataFrame.\")\n",
    "    \n",
    "    print(\"\\n--- END OF REPORT ---\")\n",
    "    print(\"\\nUse the statistical distribution values above to set informed thresholds for the next filtering step.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No relevant emotional events were detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ed6d0-f1c3-48db-a068-16b9974bf680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
