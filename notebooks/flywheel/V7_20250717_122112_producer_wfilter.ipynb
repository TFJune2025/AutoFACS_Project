{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e134c396-fa33-49e1-a1b4-8ead552a0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using V5 for V7 run with new vid data\n",
    "# V5 changes - adding new filtering logic for static images; separating lower\n",
    "    # and upper face crops to filter out mid speech images\n",
    "    # section #1 - added static and stability vars\n",
    "    # section #2 - added filter_by_emotion_stability\n",
    "    # section #3 - added static image filtering logic\n",
    "    # section #4 - simplified analytics and print statements\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CORE PROCESSING FUNCTION (with Padded Face Cropping)\n",
    "# ==============================================================================\n",
    "def analyze_video_with_filters(video_path, save_dir, model, embedding_model, processor, device, centroids, relevance_threshold, static_threshold, process_every_n_frames=1):\n",
    "    \"\"\"\n",
    "    Processes video with all filters, now with added padding to face crops\n",
    "    to ensure the entire forehead is captured.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ùå Error: Video file not found at {video_path}\")\n",
    "        return []\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS) if video_capture.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "    \n",
    "    # Get frame dimensions for boundary checks\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"‚ùå Error: Could not read the first frame of the video.\")\n",
    "        return []\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    video_capture.set(cv2.CAP_PROP_POS_FRAMES, 0) # Reset video to the beginning\n",
    "\n",
    "    print(f\"‚úÖ Opened video: {os.path.basename(video_path)} ({total_frames} frames at {fps:.2f} fps)\")\n",
    "\n",
    "    face_crop_dir = os.path.join(save_dir, \"face_crops\")\n",
    "    os.makedirs(face_crop_dir, exist_ok=True)\n",
    "\n",
    "    static_object_tracker, ignored_locations = {}, set()\n",
    "    known_face_encodings, known_face_ids = [], []\n",
    "    next_person_id = 1\n",
    "    all_results_log = []\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Analyzing Video\")\n",
    "    \n",
    "    for frame_count in range(total_frames):\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret: break\n",
    "\n",
    "        if frame_count % process_every_n_frames == 0:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            current_face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            current_frame_locations = set()\n",
    "            if current_face_encodings:\n",
    "                for i, face_encoding in enumerate(current_face_encodings):\n",
    "                    top, right, bottom, left = face_locations[i]\n",
    "                    loc_key = (top, right, bottom, left)\n",
    "                    current_frame_locations.add(loc_key)\n",
    "                    \n",
    "                    if loc_key in ignored_locations: continue\n",
    "                    \n",
    "                    # Static Object Filter logic...\n",
    "                    if loc_key not in static_object_tracker:\n",
    "                        static_object_tracker[loc_key] = {\"count\": 1, \"last_frame\": frame_count}\n",
    "                    else:\n",
    "                        if frame_count == static_object_tracker[loc_key][\"last_frame\"] + process_every_n_frames:\n",
    "                            static_object_tracker[loc_key][\"count\"] += 1\n",
    "                        else:\n",
    "                            static_object_tracker[loc_key][\"count\"] = 1\n",
    "                        static_object_tracker[loc_key][\"last_frame\"] = frame_count\n",
    "\n",
    "                    if static_object_tracker[loc_key][\"count\"] > static_threshold:\n",
    "                        if loc_key not in ignored_locations:\n",
    "                            ignored_locations.add(loc_key)\n",
    "                        continue\n",
    "\n",
    "                    # --- THIS IS THE NEW, INTEGRATED PADDING LOGIC ---\n",
    "                    # Calculate padding as a percentage of the detected face size\n",
    "                    face_height = bottom - top\n",
    "                    face_width = right - left\n",
    "                    vertical_padding = int(face_height * 0.40) # Add 40% padding above to capture forehead\n",
    "                    horizontal_padding = int(face_width * 0.15) # Add 15% padding to the sides\n",
    "\n",
    "                    # Apply padding, ensuring coordinates stay within the frame boundaries\n",
    "                    top_pad = max(0, top - vertical_padding)\n",
    "                    bottom_pad = min(frame_height, bottom + int(vertical_padding * 0.1)) # Small padding below\n",
    "                    left_pad = max(0, left - horizontal_padding)\n",
    "                    right_pad = min(frame_width, right + horizontal_padding)\n",
    "                    \n",
    "                    # Use the new padded coordinates to crop the face for analysis\n",
    "                    face_image = Image.fromarray(rgb_frame[top_pad:bottom_pad, left_pad:right_pad])\n",
    "                    # --- END OF PADDING LOGIC ---\n",
    "                    \n",
    "                    # Face Identification, Relevance Gate, and Prediction...\n",
    "                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                    person_id = \"Unknown\"\n",
    "                    if True in matches:\n",
    "                        first_match_index = matches.index(True)\n",
    "                        person_id = known_face_ids[first_match_index]\n",
    "                    else:\n",
    "                        person_id = f\"Person_{next_person_id}\"\n",
    "                        known_face_encodings.append(face_encoding)\n",
    "                        known_face_ids.append(person_id)\n",
    "                        next_person_id += 1\n",
    "                        \n",
    "                    inputs = processor(images=face_image, return_tensors=\"pt\").to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        embedding = embedding_model(**inputs).logits.squeeze()\n",
    "                    \n",
    "                    distances = {label_id: F.cosine_similarity(embedding, centroid, dim=0).item() for label_id, centroid in centroids.items()}\n",
    "                    max_similarity = max(distances.values())\n",
    "                    \n",
    "                    if max_similarity >= relevance_threshold:\n",
    "                        with torch.no_grad():\n",
    "                            logits = model(**inputs).logits\n",
    "                        emotion_results = get_emotion_predictions(face_image, model, processor, device)\n",
    "                        \n",
    "                        face_filename = os.path.join(face_crop_dir, f\"frame_{frame_count}_{person_id}_face_{i}.png\")\n",
    "                        face_image.save(face_filename)\n",
    "\n",
    "                        log_entry = {\n",
    "                            \"timestamp_seconds\": frame_count / fps,\n",
    "                            \"frame_number\": frame_count,\n",
    "                            \"person_id\": person_id,\n",
    "                            \"face_crop_path\": face_filename,\n",
    "                            \"is_relevant\": True,\n",
    "                            \"max_similarity\": max_similarity,\n",
    "                            **emotion_results\n",
    "                        }\n",
    "                        all_results_log.append(log_entry)\n",
    "\n",
    "            # Clean up tracker for objects that are no longer detected\n",
    "            stale_keys = [k for k in static_object_tracker if k not in current_frame_locations]\n",
    "            for k in stale_keys:\n",
    "                del static_object_tracker[k]\n",
    "                \n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    video_capture.release()\n",
    "    \n",
    "    print(f\"\\n--- Video Processing Summary ---\")\n",
    "    print(f\"‚úÖ Discovered {len(known_face_ids)} unique person(s).\")\n",
    "    print(f\"‚ö†Ô∏è Detected and ignored {len(ignored_locations)} static object(s).\")\n",
    "    print(f\"‚úÖ Logged {len(all_results_log)} relevant emotional events.\")\n",
    "    \n",
    "    return all_results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693b2bd6-fc39-4e29-ba97-163b5ed57130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import cv2\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d66f7b-0ee9-4e0c-b3fd-d60ea149068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "ANALYSIS_OUTPUT_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel\"\n",
    "MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "CENTROIDS_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/emotion_centroids.pt\"\n",
    "RELEVANCE_THRESHOLD = 0.6 \n",
    "STATIC_FRAME_THRESHOLD = 60 # Number of consecutive frames to identify a static object (e.g., 2 seconds at 30fps)\n",
    "STABILITY_WINDOW = 5 # Rolling window size for the stability filter\n",
    "STABILITY_THRESHOLD = 3 # How many times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb45407-a427-45e3-a742-b1d5aead16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Dynamically determines the next version number.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [int(d[1:].split(\"_\")[0]) for d in existing if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()]\n",
    "    return f\"V{max(versions, default=0) + 1}\"\n",
    "\n",
    "\n",
    "# Runs the emotion recognition model on a single face image and returns a\n",
    "# structured dictionary of probabilities.\n",
    "def get_emotion_predictions(face_image, model, processor, device):\n",
    "    # Use the processor to prepare the image for the model\n",
    "    inputs = processor(images=face_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "    # Get the top prediction\n",
    "    top_confidence, top_pred_idx = torch.max(probabilities, dim=0)\n",
    "    top_pred_label = model.config.id2label[top_pred_idx.item()]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-9)).item()\n",
    "    \n",
    "    # Create a dictionary with all results\n",
    "    results = {\n",
    "        \"predicted_label\": top_pred_label,\n",
    "        \"confidence\": top_confidence.item(),\n",
    "        \"entropy\": entropy\n",
    "    }\n",
    "    \n",
    "    # This loop ensures all individual probabilities are added to the log for detailed analysis.\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        label = model.config.id2label[i]\n",
    "        results[f\"prob_{label}\"] = prob.item()\n",
    "        \n",
    "    return results\n",
    "    \n",
    "\n",
    "# Post-processes the log to filter for stable emotional states.\n",
    "def filter_by_emotion_stability(df, window, threshold):\n",
    "    if df.empty:\n",
    "        return df\n",
    "        \n",
    "    print(f\"\\n--- Applying Emotion Stability Filter (Window={window}, Threshold={threshold}) ---\")\n",
    "    \n",
    "    # This new list will store the indices of the rows we want to keep.\n",
    "    stable_indices = []\n",
    "    \n",
    "    # Ensure the dataframe is sorted correctly before processing\n",
    "    df = df.sort_values(by=['person_id', 'timestamp_seconds']).reset_index(drop=True)\n",
    "    \n",
    "    # Group by each unique person and iterate through their data\n",
    "    for person_id, group in df.groupby('person_id'):\n",
    "        labels = group['predicted_label']\n",
    "        \n",
    "        # Manually iterate through each prediction for this person\n",
    "        for i in range(len(labels)):\n",
    "            # Define the window of labels to check for stability\n",
    "            current_window = labels.iloc[max(0, i - window + 1) : i + 1]\n",
    "            \n",
    "            # The emotion we are checking for stability is the most recent one\n",
    "            current_label_to_check = labels.iloc[i]\n",
    "            \n",
    "            # Count how many times this emotion appears in the window\n",
    "            if (current_window == current_label_to_check).sum() >= threshold:\n",
    "                # If it's stable enough, keep it\n",
    "                stable_indices.append(group.index[i])\n",
    "\n",
    "    stable_df = df.loc[stable_indices].copy()\n",
    "    \n",
    "    print(f\"-> Filtered {len(df) - len(stable_df)} unstable/transitional frames.\")\n",
    "    print(f\"‚úÖ {len(stable_df)} stable emotional events remain.\")\n",
    "    return stable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f613ff-c673-4607-bd01-94efbf12b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CORE PROCESSING FUNCTION (with Face Identification & All Filters)\n",
    "# ==============================================================================\n",
    "\n",
    "# Processes video with all filters, ensuring file paths and person IDs are correctly logged.\n",
    "def analyze_video_with_filters(video_path, save_dir, model, embedding_model, processor, device, centroids, relevance_threshold, static_threshold, process_every_n_frames=1):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ùå Error: Video file not found at {video_path}\")\n",
    "        return []\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS) if video_capture.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "    print(f\"‚úÖ Opened video: {os.path.basename(video_path)} ({total_frames} frames at {fps:.2f} fps)\")\n",
    "\n",
    "    # Create the directory for all face crops at the start\n",
    "    face_crop_dir = os.path.join(save_dir, \"face_crops\")\n",
    "    os.makedirs(face_crop_dir, exist_ok=True)\n",
    "\n",
    "    # Data structures for tracking\n",
    "    static_object_tracker = {}\n",
    "    ignored_locations = set()\n",
    "    known_face_encodings = []\n",
    "    known_face_ids = []\n",
    "    next_person_id = 1\n",
    "    \n",
    "    all_results_log = []\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Analyzing Video\")\n",
    "    \n",
    "    for frame_count in range(total_frames):\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret: break\n",
    "\n",
    "        if frame_count % process_every_n_frames == 0:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            current_face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "            current_frame_locations = set()\n",
    "            if current_face_encodings:\n",
    "                for i, face_encoding in enumerate(current_face_encodings):\n",
    "                    loc_key = face_locations[i]\n",
    "                    current_frame_locations.add(loc_key)\n",
    "                    \n",
    "                    if loc_key in ignored_locations:\n",
    "                        continue \n",
    "\n",
    "                    # Static Object Filter\n",
    "                    if loc_key not in static_object_tracker:\n",
    "                        static_object_tracker[loc_key] = {\"count\": 1, \"last_frame\": frame_count}\n",
    "                    else:\n",
    "                        if frame_count == static_object_tracker[loc_key][\"last_frame\"] + process_every_n_frames:\n",
    "                            static_object_tracker[loc_key][\"count\"] += 1\n",
    "                        else:\n",
    "                            static_object_tracker[loc_key][\"count\"] = 1\n",
    "                        static_object_tracker[loc_key][\"last_frame\"] = frame_count\n",
    "\n",
    "                    if static_object_tracker[loc_key][\"count\"] > static_threshold:\n",
    "                        if loc_key not in ignored_locations:\n",
    "                            print(f\"\\n‚ö†Ô∏è Detected and ignoring static object at {loc_key}\")\n",
    "                            ignored_locations.add(loc_key)\n",
    "                        continue\n",
    "\n",
    "                    # Face Identification Logic\n",
    "                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                    person_id = \"Unknown\"\n",
    "                    if True in matches:\n",
    "                        first_match_index = matches.index(True)\n",
    "                        person_id = known_face_ids[first_match_index]\n",
    "                    else:\n",
    "                        person_id = f\"Person_{next_person_id}\"\n",
    "                        known_face_encodings.append(face_encoding)\n",
    "                        known_face_ids.append(person_id)\n",
    "                        next_person_id += 1\n",
    "                        print(f\"\\n‚ú® Discovered new face: {person_id}\")\n",
    "\n",
    "                    # Relevance Gate and Emotion Prediction\n",
    "                    face_image = Image.fromarray(rgb_frame[loc_key[0]:loc_key[2], loc_key[3]:loc_key[1]])\n",
    "                    inputs = processor(images=face_image, return_tensors=\"pt\").to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        embedding = embedding_model(**inputs).logits.squeeze()\n",
    "                    \n",
    "                    distances = {label_id: F.cosine_similarity(embedding, centroid, dim=0).item() for label_id, centroid in centroids.items()}\n",
    "                    max_similarity = max(distances.values())\n",
    "                    \n",
    "                    if max_similarity >= relevance_threshold:\n",
    "                        with torch.no_grad():\n",
    "                            logits = model(**inputs).logits\n",
    "                        emotion_results = get_emotion_predictions(face_image, model, processor, device)\n",
    "                        \n",
    "                        # 1. Create the full, absolute path for the cropped face.\n",
    "                        face_filename = os.path.join(face_crop_dir, f\"frame_{frame_count}_{person_id}_face_{i}.png\")\n",
    "                        \n",
    "                        # 2. Save the image to that path.\n",
    "                        face_image.save(face_filename)\n",
    "\n",
    "                        # 3. Log this exact, verified path to the results.\n",
    "                        log_entry = {\n",
    "                            \"timestamp_seconds\": frame_count / fps,\n",
    "                            \"frame_number\": frame_count,\n",
    "                            \"person_id\": person_id,\n",
    "                            \"face_index\": i,\n",
    "                            \"face_crop_path\": face_filename,\n",
    "                            \"is_relevant\": True,\n",
    "                            \"max_similarity\": max_similarity,\n",
    "                            **emotion_results\n",
    "                        }\n",
    "                        all_results_log.append(log_entry)\n",
    "\n",
    "            # Clean up tracker for objects that are no longer detected\n",
    "            stale_keys = [k for k, v in static_object_tracker.items() if k not in current_frame_locations]\n",
    "            for k in stale_keys:\n",
    "                del static_object_tracker[k]\n",
    "                \n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    video_capture.release()\n",
    "    \n",
    "    print(f\"‚úÖ Video processing complete. Logged {len(all_results_log)} relevant emotional events from {len(known_face_ids)} unique person(s).\")\n",
    "    return all_results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab199de2-1684-4b38-9592-c178baaf8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created analysis output directory: /Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V7_20250717_122112\n",
      "\n",
      "--- Loading assets ---\n",
      "‚úÖ Assets loaded onto mps.\n",
      "‚úÖ Opened video: trevor_noah_interview_2.mp4 (7554 frames at 29.97 fps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:   0%|                                 | 0/7554 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:   1%|‚ñè                     | 85/7554 [00:47<1:08:48,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:   1%|‚ñé                     | 94/7554 [00:52<1:11:52,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:   3%|‚ñã                    | 244/7554 [02:13<1:07:23,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:   7%|‚ñà‚ñç                   | 509/7554 [04:38<1:08:23,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:   8%|‚ñà‚ñå                   | 569/7554 [05:16<1:12:12,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (201, 655, 387, 469)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  10%|‚ñà‚ñà‚ñè                  | 783/7554 [07:17<1:03:01,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (297, 712, 426, 583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  12%|‚ñà‚ñà‚ñç                  | 872/7554 [08:05<1:08:30,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  12%|‚ñà‚ñà‚ñå                  | 908/7554 [08:27<1:05:32,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (449, 820, 634, 634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  17%|‚ñà‚ñà‚ñà‚ñã                  | 1252/7554 [11:08<48:23,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  18%|‚ñà‚ñà‚ñà‚ñâ                  | 1365/7554 [11:57<49:28,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  18%|‚ñà‚ñà‚ñà‚ñã                | 1389/7554 [12:11<1:01:08,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  19%|‚ñà‚ñà‚ñà‚ñà‚ñè                 | 1425/7554 [12:31<53:08,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (290, 1745, 675, 1359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  21%|‚ñà‚ñà‚ñà‚ñà‚ñå                 | 1572/7554 [13:43<51:51,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (333, 1745, 718, 1359)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  38%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 2856/7554 [25:28<41:40,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (246, 1016, 708, 553)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã            | 3311/7554 [29:29<36:54,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è Detected and ignoring static object at (192, 786, 415, 563)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà         | 4489/7554 [40:05<27:02,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 4626/7554 [41:11<23:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 4900/7554 [43:20<20:54,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè      | 5203/7554 [45:39<21:01,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä      | 5443/7554 [47:50<18:34,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 5781/7554 [1:00:20<15:47,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 5807/7554 [1:00:34<14:33,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 5829/7554 [1:00:45<15:11,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 5952/7554 [1:01:52<14:10,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ú® Discovered new face: Person_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 7552/7554 [1:16:19<00:01,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video processing complete. Logged 5400 relevant emotional events from 18 unique person(s).\n",
      "\n",
      "‚úÖ Full analysis log saved. Found 5400 relevant emotional events.\n",
      "\n",
      "--- Applying Emotion Stability Filter (Window=5, Threshold=3) ---\n",
      "-> Filtered 1105 unstable/transitional frames.\n",
      "‚úÖ 4295 stable emotional events remain.\n",
      "‚úÖ Saved final stable emotion log to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V7_20250717_122112/final_stable_emotion_log.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Setup Dynamic Save Directory ---\n",
    "VERSION = get_next_version(ANALYSIS_OUTPUT_ROOT)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION_TAG = f\"{VERSION}_{timestamp}\"\n",
    "SAVE_DIR = os.path.join(ANALYSIS_OUTPUT_ROOT, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Created analysis output directory: {SAVE_DIR}\")\n",
    "\n",
    "# --- Load Models, Processor, and Centroids ---\n",
    "print(f\"\\n--- Loading assets ---\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the main model for classification\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_PATH).to(device).eval()\n",
    "\n",
    "# Create a second model instance for generating embeddings\n",
    "embedding_model = AutoModelForImageClassification.from_pretrained(MODEL_PATH)\n",
    "embedding_model.classifier = nn.Identity()\n",
    "embedding_model.to(device).eval()\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load the pre-calculated emotion centroids\n",
    "centroids = torch.load(CENTROIDS_PATH, map_location=device)\n",
    "print(f\"‚úÖ Assets loaded onto {device}.\")\n",
    "\n",
    "\n",
    "# --- Run the Analysis ---\n",
    "video_to_process = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/sample_vids/trevor_noah_interview_2.mp4\" \n",
    "\n",
    "# Call the function with all required keyword arguments for clarity and safety\n",
    "analysis_log = analyze_video_with_filters(\n",
    "    video_path=video_to_process, \n",
    "    save_dir=SAVE_DIR, # <-- This was the missing argument\n",
    "    model=model,\n",
    "    embedding_model=embedding_model,\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    centroids=centroids,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "    static_threshold=STATIC_FRAME_THRESHOLD,\n",
    "    process_every_n_frames=1\n",
    ")\n",
    "\n",
    "# --- Save Logs and Apply Stability Filter ---\n",
    "if analysis_log:\n",
    "    log_df = pd.DataFrame(analysis_log)\n",
    "    full_log_path = os.path.join(SAVE_DIR, \"emotion_log_before_stability_filter.csv\")\n",
    "    log_df.to_csv(full_log_path, index=False)\n",
    "    print(f\"\\n‚úÖ Full analysis log saved. Found {len(log_df)} relevant emotional events.\")\n",
    "    \n",
    "    # Apply the final stability filter\n",
    "    stable_log_df = filter_by_emotion_stability(log_df, window=STABILITY_WINDOW, threshold=STABILITY_THRESHOLD)\n",
    "    \n",
    "    # Save the final, clean log\n",
    "    stable_log_path = os.path.join(SAVE_DIR, \"final_stable_emotion_log.csv\")\n",
    "    stable_log_df.to_csv(stable_log_path, index=False)\n",
    "    print(f\"‚úÖ Saved final stable emotion log to: {stable_log_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No relevant emotional events were detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ed6d0-f1c3-48db-a068-16b9974bf680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
