{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e134c396-fa33-49e1-a1b4-8ead552a0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6 changes - adding improved filtering logic for static images\n",
    "    # section #1 - removed static and stability vars\n",
    "    # section #2 - reworked get_emotion_predictions\n",
    "        # removed filter_by_emotion_stability\n",
    "    # section #3 - updated analyze_video_with_filters\n",
    "    # section #4 - updated execution block\n",
    "        # deleted embedding_model\n",
    "        # simplified analysis_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693b2bd6-fc39-4e29-ba97-163b5ed57130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import cv2\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d66f7b-0ee9-4e0c-b3fd-d60ea149068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "ANALYSIS_OUTPUT_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel\"\n",
    "MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "os.makedirs(ANALYSIS_OUTPUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb45407-a427-45e3-a742-b1d5aead16e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Dynamically determines the next version number.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [int(d[1:].split(\"_\")[0]) for d in existing if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()]\n",
    "    return f\"V{max(versions, default=0) + 1}\"\n",
    "\n",
    "\n",
    "# Runs the emotion recognition model on a single face image and returns a\n",
    "# structured dictionary of probabilities.\n",
    "def get_emotion_predictions(face_image, model, processor, device):\n",
    "    \"\"\"Runs the emotion model on a face image and returns a structured dictionary.\"\"\"\n",
    "    if face_image.size[0] < 10 or face_image.size[1] < 10: # Skip tiny, invalid crops\n",
    "        return None\n",
    "        \n",
    "    # Use the processor to prepare the image for the model    \n",
    "    inputs = processor(images=face_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "    # Get the top prediction\n",
    "    top_confidence, top_pred_idx = torch.max(probabilities, dim=0)\n",
    "    top_pred_label = model.config.id2label[top_pred_idx.item()]\n",
    "    \n",
    "    return {\"predicted_label\": top_pred_label, \"confidence\": top_confidence.item()}\n",
    "    \n",
    "\n",
    "# Post-processes the log to filter for stable emotional states.\n",
    "def filter_by_emotion_stability(df, window, threshold):\n",
    "    if df.empty:\n",
    "        return df\n",
    "        \n",
    "    print(f\"\\n--- Applying Emotion Stability Filter (Window={window}, Threshold={threshold}) ---\")\n",
    "    \n",
    "    # This new list will store the indices of the rows we want to keep.\n",
    "    stable_indices = []\n",
    "    \n",
    "    # Ensure the dataframe is sorted correctly before processing\n",
    "    df = df.sort_values(by=['person_id', 'timestamp_seconds']).reset_index(drop=True)\n",
    "    \n",
    "    # Group by each unique person and iterate through their data\n",
    "    for person_id, group in df.groupby('person_id'):\n",
    "        labels = group['predicted_label']\n",
    "        \n",
    "        # Manually iterate through each prediction for this person\n",
    "        for i in range(len(labels)):\n",
    "            # Define the window of labels to check for stability\n",
    "            current_window = labels.iloc[max(0, i - window + 1) : i + 1]\n",
    "            \n",
    "            # The emotion we are checking for stability is the most recent one\n",
    "            current_label_to_check = labels.iloc[i]\n",
    "            \n",
    "            # Count how many times this emotion appears in the window\n",
    "            if (current_window == current_label_to_check).sum() >= threshold:\n",
    "                # If it's stable enough, keep it\n",
    "                stable_indices.append(group.index[i])\n",
    "\n",
    "    stable_df = df.loc[stable_indices].copy()\n",
    "    \n",
    "    print(f\"-> Filtered {len(df) - len(stable_df)} unstable/transitional frames.\")\n",
    "    print(f\"‚úÖ {len(stable_df)} stable emotional events remain.\")\n",
    "    return stable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f613ff-c673-4607-bd01-94efbf12b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CORE PROCESSING FUNCTION (with Multi-Region Filter)\n",
    "# ==============================================================================\n",
    "def analyze_video_with_multi_region_filter(video_path, save_dir, model, processor, device, process_every_n_frames=1):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ùå Error: Video file not found at {video_path}\")\n",
    "        return []\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS) if video_capture.get(cv2.CAP_PROP_FPS) > 0 else 30\n",
    "    print(f\"‚úÖ Opened video: {os.path.basename(video_path)} ({total_frames} frames at {fps:.2f} fps)\")\n",
    "\n",
    "    all_results_log = []\n",
    "\n",
    "    # Initialize dictionary to track static objects before loop begins\n",
    "    static_object_tracker = {}\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Analyzing Video\")\n",
    "    \n",
    "    for frame_count in range(total_frames):\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret: break\n",
    "\n",
    "        if frame_count % process_every_n_frames == 0:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Get both face locations and the facial landmarks\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            face_landmarks_list = face_recognition.face_landmarks(rgb_frame, face_locations)\n",
    "\n",
    "            if face_locations:\n",
    "                for i, face_landmarks in enumerate(face_landmarks_list):\n",
    "                    top, right, bottom, left = face_locations[i]\n",
    "                    full_face_image = Image.fromarray(rgb_frame[top:bottom, left:right])\n",
    "\n",
    "                    # --- Multi-Region Cropping ---\n",
    "                    # Create upper face crop (eyes and brows)\n",
    "                    top_of_eyes = min([p[1] for p in face_landmarks['left_eyebrow'] + face_landmarks['right_eyebrow']])\n",
    "                    bottom_of_eyes = max([p[1] for p in face_landmarks['left_eye'] + face_landmarks['right_eye']])\n",
    "                    upper_face_image = Image.fromarray(rgb_frame[top_of_eyes:bottom_of_eyes, left:right])\n",
    "                    \n",
    "                    # Create lower face crop (mouth and jaw)\n",
    "                    top_of_mouth = min([p[1] for p in face_landmarks['top_lip']])\n",
    "                    bottom_of_mouth = max([p[1] for p in face_landmarks['bottom_lip']])\n",
    "                    lower_face_image = Image.fromarray(rgb_frame[top_of_mouth:bottom_of_mouth, left:right])\n",
    "                    \n",
    "                    # --- Parallel Analysis ---\n",
    "                    upper_face_results = get_emotion_predictions(upper_face_image, model, processor, device)\n",
    "                    lower_face_results = get_emotion_predictions(lower_face_image, model, processor, device)\n",
    "\n",
    "                    # --- The New Filter Logic ---\n",
    "                    # If the upper face is neutral but the lower face is not, it's likely speech.\n",
    "                    # We only proceed if an emotion is detected in the upper face.\n",
    "                    if upper_face_results and upper_face_results['predicted_label'] != 'neutral':\n",
    "                        \n",
    "                        # Since it's a real emotion, get the prediction for the FULL face\n",
    "                        full_face_results = get_emotion_predictions(full_face_image, model, processor, device)\n",
    "                        \n",
    "                        if full_face_results:\n",
    "                            log_entry = {\n",
    "                                \"timestamp_seconds\": frame_count / fps,\n",
    "                                \"frame_number\": frame_count,\n",
    "                                \"face_index\": i,\n",
    "                                **full_face_results\n",
    "                            }\n",
    "                            all_results_log.append(log_entry)\n",
    "\n",
    "            # Clean up tracker for objects that are no longer detected\n",
    "            stale_keys = [k for k, v in static_object_tracker.items() if k not in current_frame_locations]\n",
    "            for k in stale_keys:\n",
    "                del static_object_tracker[k]\n",
    "                \n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    video_capture.release()\n",
    "    print(f\"‚úÖ Video processing complete. Logged {len(all_results_log)} stable emotional events.\")\n",
    "    return all_results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab199de2-1684-4b38-9592-c178baaf8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created analysis directory: /Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V6_20250716_112248\n",
      "‚úÖ Assets loaded onto mps.\n",
      "‚úÖ Opened video: StreetQs.mp4 (5657 frames at 30.00 fps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5657/5657 [01:46<00:00, 53.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video processing complete. Logged 361 stable emotional events.\n",
      "\n",
      "‚úÖ Successfully saved final analysis log to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V6_20250716_112248/multi_region_filtered_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    # --- Setup ---\n",
    "    VERSION = get_next_version(ANALYSIS_OUTPUT_ROOT)\n",
    "    SAVE_DIR = os.path.join(ANALYSIS_OUTPUT_ROOT, f\"{VERSION}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f\"üìÅ Created analysis directory: {SAVE_DIR}\")\n",
    "\n",
    "    # --- Load Assets ---\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = AutoModelForImageClassification.from_pretrained(MODEL_PATH).to(device).eval()\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "    print(f\"‚úÖ Assets loaded onto {device}.\")\n",
    "\n",
    "    # --- Process Video ---\n",
    "    video_to_process = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/sample_vids/StreetQs.mp4\"\n",
    "    analysis_log = analyze_video_with_multi_region_filter(\n",
    "        video_path=video_to_process, \n",
    "        save_dir=SAVE_DIR,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        process_every_n_frames=15 # Start with a larger step for faster testing\n",
    "    )\n",
    "\n",
    "    # --- Save Log ---\n",
    "    if analysis_log:\n",
    "        log_df = pd.DataFrame(analysis_log)\n",
    "        csv_path = os.path.join(SAVE_DIR, \"multi_region_filtered_log.csv\")\n",
    "        log_df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n‚úÖ Successfully saved final analysis log to: {csv_path}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No stable emotional events were detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ed6d0-f1c3-48db-a068-16b9974bf680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
