{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee21a26e-d06e-4bfd-b67d-7fb855156031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import cv2\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a639b3-a0d0-4b7b-850d-fe9828f3bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "ANALYSIS_OUTPUT_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel\"\n",
    "MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "CENTROIDS_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/emotion_centroids.pt\" # The file created by the last script\n",
    "\n",
    "# This is a new hyperparameter. It's the distance threshold for the \"gatekeeper\".\n",
    "# A smaller value makes the filter stricter. You can tune this value later.\n",
    "RELEVANCE_THRESHOLD = 0.6 \n",
    "\n",
    "os.makedirs(ANALYSIS_OUTPUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccee248-78ef-4b54-90c4-5c5e88e24d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Dynamically determines the next version number by scanning a directory.\n",
    "def get_next_version(base_dir):\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    existing = [os.path.basename(d) for d in all_entries if os.path.isdir(d)]\n",
    "    versions = [int(d[1:].split(\"_\")[0]) for d in existing if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()]\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9fbe85-8c9b-40bb-bad9-6023c7f404b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CORE PROCESSING FUNCTION (with Two-Stage Analysis)\n",
    "# ==============================================================================\n",
    "\n",
    "# Function processes a video with a two-stage approach:\n",
    "    # 1. Relevance Check: Determines if a face is showing a clear emotion.\n",
    "    # 2. Emotion Classification: If relevant, classifies the emotion.\n",
    "def analyze_video_with_relevance_gate(\n",
    "    video_path,\n",
    "    save_dir,\n",
    "    model,\n",
    "    embedding_model,\n",
    "    processor,\n",
    "    device,\n",
    "    centroids,\n",
    "    relevance_threshold,\n",
    "    process_every_n_frames=1\n",
    "):\n",
    "    \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ùå Error: Video file not found at {video_path}\")\n",
    "        return []\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"‚úÖ Opened video: {os.path.basename(video_path)} ({total_frames} frames at {fps:.2f} fps)\")\n",
    "\n",
    "    all_results_log = []\n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing video frames\")\n",
    "\n",
    "    for frame_count in range(total_frames):\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % process_every_n_frames == 0:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            \n",
    "            if face_locations:\n",
    "                for i, (top, right, bottom, left) in enumerate(face_locations):\n",
    "                    face_image_pil = Image.fromarray(rgb_frame[top:bottom, left:right])\n",
    "                    inputs = processor(images=face_image_pil, return_tensors=\"pt\").to(device)\n",
    "\n",
    "                    # --- Stage 1: Relevance Detector ---\n",
    "                    with torch.no_grad():\n",
    "                        # Get the feature embedding for the current face\n",
    "                        embedding = embedding_model(**inputs).logits.squeeze()\n",
    "                    \n",
    "                    # Calculate the distance to all known emotion centroids\n",
    "                    distances = {\n",
    "                        label_id: torch.nn.functional.cosine_similarity(embedding, centroid, dim=0).item()\n",
    "                        for label_id, centroid in centroids.items()\n",
    "                    }\n",
    "                    \n",
    "                    # Find the highest similarity score (closest distance)\n",
    "                    max_similarity = max(distances.values())\n",
    "                    \n",
    "                    # --- The Gatekeeper ---\n",
    "                    # Only proceed if the face is similar enough to a known emotion\n",
    "                    if max_similarity >= relevance_threshold:\n",
    "                        # --- Stage 2: Emotion Classifier ---\n",
    "                        with torch.no_grad():\n",
    "                            logits = model(**inputs).logits\n",
    "                        \n",
    "                        probabilities = F.softmax(logits, dim=1).squeeze()\n",
    "                        top_confidence, top_pred_idx = torch.max(probabilities, dim=0)\n",
    "                        top_pred_label = model.config.id2label[top_pred_idx.item()]\n",
    "                        entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-9)).item()\n",
    "\n",
    "                        log_entry = {\n",
    "                            \"timestamp_seconds\": frame_count / fps,\n",
    "                            \"frame_number\": frame_count,\n",
    "                            \"face_index\": i,\n",
    "                            \"is_relevant\": True,\n",
    "                            \"max_similarity\": max_similarity,\n",
    "                            \"predicted_label\": top_pred_label,\n",
    "                            \"confidence\": top_confidence.item(),\n",
    "                            \"entropy\": entropy\n",
    "                        }\n",
    "                        all_results_log.append(log_entry)\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    video_capture.release()\n",
    "    \n",
    "    print(f\"‚úÖ Video processing complete. Logged {len(all_results_log)} relevant emotional events.\")\n",
    "    return all_results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4444df8-f9aa-4dfd-9dbb-e68e73e41e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created analysis output directory: /Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/V3_20250714_093845\n",
      "\n",
      "--- Loading assets ---\n",
      "‚úÖ Models, processor, and 10 centroids loaded onto mps.\n",
      "‚úÖ Opened video: StreetQs.mp4 (5657 frames at 30.00 fps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5657/5657 [24:45<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video processing complete. Logged 5152 relevant emotional events.\n",
      "\n",
      "‚úÖ Successfully saved filtered analysis to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/V3_20250714_093845/filtered_emotion_log.csv\n",
      "\n",
      "--- Summary ---\n",
      "Total relevant faces analyzed: 5152\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Setup Dynamic Save Directory ---\n",
    "VERSION = get_next_version(ANALYSIS_OUTPUT_ROOT)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VERSION_TAG = f\"{VERSION}_{timestamp}\"\n",
    "SAVE_DIR = os.path.join(ANALYSIS_OUTPUT_ROOT, VERSION_TAG)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Created analysis output directory: {SAVE_DIR}\")\n",
    "\n",
    "# --- Load Models, Processor, and Centroids ---\n",
    "print(f\"\\n--- Loading assets ---\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Load the original model for the final classification\n",
    "classification_model = AutoModelForImageClassification.from_pretrained(MODEL_PATH).to(device).eval()\n",
    "\n",
    "# 2. Create a second model instance specifically for generating embeddings\n",
    "embedding_model = AutoModelForImageClassification.from_pretrained(MODEL_PATH)\n",
    "embedding_model.classifier = nn.Identity()\n",
    "embedding_model.to(device).eval()\n",
    "\n",
    "# 3. Load the processor\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 4. Load the pre-calculated emotion centroids from their path\n",
    "emotion_centroids = torch.load(CENTROIDS_PATH, map_location=device)\n",
    "print(f\"‚úÖ Models, processor, and {len(emotion_centroids)} centroids loaded onto {device}.\")\n",
    "\n",
    "\n",
    "# --- Run the Analysis ---\n",
    "# Define the path to your video file directly here.\n",
    "video_to_process = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/flywheel/sample_vids/StreetQs.mp4\" \n",
    "\n",
    "# Call the function with all required arguments\n",
    "analysis_log = analyze_video_with_relevance_gate(\n",
    "    video_path=video_to_process, \n",
    "    save_dir=SAVE_DIR,\n",
    "    model=classification_model,\n",
    "    embedding_model=embedding_model,\n",
    "    processor=processor,\n",
    "    device=device,\n",
    "    centroids=emotion_centroids,\n",
    "    relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "    process_every_n_frames=1\n",
    ")\n",
    "\n",
    "# --- Save Results to CSV ---\n",
    "if analysis_log:\n",
    "    log_df = pd.DataFrame(analysis_log)\n",
    "    csv_path = os.path.join(SAVE_DIR, \"filtered_emotion_log.csv\")\n",
    "    log_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n‚úÖ Successfully saved filtered analysis to: {csv_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No relevant emotional events were detected, so no log file was created.\")\n",
    "\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"Total relevant faces analyzed: {len(analysis_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdc2db-5f58-4404-a53f-2a5c7f34554f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
