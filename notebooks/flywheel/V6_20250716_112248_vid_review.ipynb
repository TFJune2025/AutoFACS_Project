{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05179f1-de89-4876-9484-4fae0bd0823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this pipeline was impractical since 5 secs of vid give too many images\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f7264e-4287-4310-a43e-4d1253344a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "# --- IMPORTANT: Update these paths before running ---\n",
    "\n",
    "# This should be the full path to original source video you analyzed.\n",
    "SOURCE_VIDEO_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/sample_vids/StreetQs.mp4\"\n",
    "\n",
    "# This should be the path to the versioned folder from your last analysis run.\n",
    "RUN_DIRECTORY = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V6_20250716_112248\"\n",
    "\n",
    "# --- Filtering Options ---\n",
    "# Defines emotions for generating clips; to review all events, set to None.\n",
    "EMOTIONS_TO_REVIEW = ['contempt', 'disgust']\n",
    "\n",
    "# Define the duration of the video clips to generate (in seconds).\n",
    "CLIP_DURATION = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f442082-3494-4e85-9433-c4cd0fb32d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. SCRIPT LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "# Loads a log, filters for events, creates video clips, and generates a\n",
    "    # starter spreadsheet for manual curation.\n",
    "def generate_review_clips(run_dir, source_video, emotions_to_review, clip_duration):\n",
    "    \n",
    "    log_path = os.path.join(run_dir, \"multi_region_filtered_log.csv\")\n",
    "    if not os.path.exists(log_path):\n",
    "        print(f\"❌ Error: '{log_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(log_path)\n",
    "    print(f\"✅ Loaded '{log_path}' with {len(df)} relevant emotional events.\")\n",
    "\n",
    "    # --- Filter for the emotions we want to review ---\n",
    "    review_df = df[df['predicted_label'].isin(emotions_to_review)] if emotions_to_review else df\n",
    "    \n",
    "    if review_df.empty:\n",
    "        print(\"✅ No events found for the specified emotions. Nothing to do.\")\n",
    "        return\n",
    "\n",
    "    # --- Create a directory to save the new clips ---\n",
    "    clips_output_dir = os.path.join(run_dir, \"review_clips\")\n",
    "    os.makedirs(clips_output_dir, exist_ok=True)\n",
    "    curation_log = [] # To store data for the new CSV\n",
    "\n",
    "    print(f\"\\n--- Generating {len(review_df)} Video Clips & Curation Sheet ---\")\n",
    "    \n",
    "    # --- Create Clips and Log Data for Spreadsheet ---\n",
    "    for index, row in tqdm(review_df.iterrows(), total=review_df.shape[0], desc=\"Creating Artifacts\"):\n",
    "        start_time = max(0, row['timestamp_seconds'] - (clip_duration / 2))\n",
    "        emotion_label = row['predicted_label']\n",
    "        frame_num = int(row['frame_number'])\n",
    "        \n",
    "        output_clip_name = f\"{emotion_label}_frame_{frame_num}.mp4\"\n",
    "        output_path = os.path.join(clips_output_dir, output_clip_name)\n",
    "        \n",
    "        # Add data to our log for the spreadsheet\n",
    "        curation_log.append({\n",
    "            \"clip_filename\": output_clip_name,\n",
    "            \"predicted_label\": emotion_label,\n",
    "            \"actual_label\": \"\", # Leave blank for manual entry\n",
    "            \"notes\": \"\"          # Leave blank for manual entry\n",
    "        })\n",
    "        \n",
    "        # Builds/executes command automatically for each row\n",
    "            # creates the command as a list of strings\n",
    "        ffmpeg_command = [\n",
    "            'ffmpeg',\n",
    "            '-ss', str(start_time),\n",
    "            '-i', source_video,\n",
    "            '-t', str(clip_duration),\n",
    "            '-c', 'copy',\n",
    "            '-y',\n",
    "            '-loglevel', 'error',\n",
    "            output_path\n",
    "        ]\n",
    "\n",
    "        subprocess.run(ffmpeg_command)\n",
    "    \n",
    "    # --- Save the Starter Spreadsheet ---\n",
    "    curation_df = pd.DataFrame(curation_log)\n",
    "    spreadsheet_path = os.path.join(clips_output_dir, \"manual_curation_starter_sheet.csv\")\n",
    "    curation_df.to_csv(spreadsheet_path, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Success! All clips created in: {clips_output_dir}\")\n",
    "    print(f\"✅ Starter spreadsheet for curation saved to: {spreadsheet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d975395e-3da5-47ba-8386-a1fb081de25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded '/Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V6_20250716_112248/multi_region_filtered_log.csv' with 361 relevant emotional events.\n",
      "\n",
      "--- Generating 91 Video Clips & Curation Sheet ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Artifacts: 100%|███████████████████████| 91/91 [00:04<00:00, 21.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Success! All clips created in: /Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V6_20250716_112248/review_clips\n",
      "✅ Starter spreadsheet for curation saved to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/data_flywheel/V6_20250716_112248/review_clips/manual_curation_starter_sheet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    if os.path.isfile(SOURCE_VIDEO_PATH) and os.path.isdir(RUN_DIRECTORY):\n",
    "        generate_review_clips(\n",
    "            run_dir=RUN_DIRECTORY,\n",
    "            source_video=SOURCE_VIDEO_PATH,\n",
    "            emotions_to_review=EMOTIONS_TO_REVIEW,\n",
    "            clip_duration=CLIP_DURATION\n",
    "        )\n",
    "    else:\n",
    "        print(\"❌ Error: Please ensure SOURCE_VIDEO_PATH and RUN_DIRECTORY are correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf787f-fde5-4651-8809-6d87756dabe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
