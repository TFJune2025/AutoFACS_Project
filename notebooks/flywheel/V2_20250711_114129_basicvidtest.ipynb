{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f1de15-ff69-4f77-a4a4-534e596e75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 - rerunning test to avoid split screen confusing model\n",
    "    # section #2 - added filter_high_conviction_emotions\n",
    "    # section #4 - worked in new filtering function,\n",
    "        # updated vid to StreetQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04e53e5-9b09-4bc7-9fdd-83ee2fc08584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8168d33d-f020-4266-bcbb-8746e1e2a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "# Define the root directory where all analysis outputs will be stored.\n",
    "ANALYSIS_OUTPUT_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/vid_inference\"\n",
    "# Define the path to the final, production-ready model\n",
    "MODEL_PATH = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807\"\n",
    "os.makedirs(ANALYSIS_OUTPUT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ffb214-a5d6-465f-b010-a019aa593833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "# Checks for valid image file extensions.\n",
    "def is_valid_image(filename):\n",
    "    return filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and not filename.startswith(\"._\")\n",
    "\n",
    "# Dynamically determines the next version number by scanning a directory.\n",
    "def get_next_version(base_dir):\n",
    "    # Use glob to find all entries matching the pattern\n",
    "    all_entries = glob.glob(os.path.join(base_dir, \"V*_*\"))\n",
    "    \n",
    "    # Filter to include only directories\n",
    "    existing = [\n",
    "        os.path.basename(d) for d in all_entries if os.path.isdir(d)\n",
    "    ]\n",
    "\n",
    "    # Extract version numbers from the directory names\n",
    "    versions = [\n",
    "        int(d[1:].split(\"_\")[0]) for d in existing\n",
    "        if d.startswith(\"V\") and \"_\" in d and d[1:].split(\"_\")[0].isdigit()\n",
    "    ]\n",
    "    \n",
    "    # Determine the next version number\n",
    "    next_version = max(versions, default=0) + 1\n",
    "    return f\"V{next_version}\"\n",
    "\n",
    "# Runs the emotion recognition model on a single face image and prints the\n",
    "    # formatted probability distribution.\n",
    "def predict_and_display_emotions(face_image, model, processor, device):\n",
    "\n",
    "    # Use the processor to prepare the image for the model\n",
    "    inputs = processor(images=face_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "    # Get the top prediction\n",
    "    top_confidence, top_pred_idx = torch.max(probabilities, dim=0)\n",
    "    top_pred_label = model.config.id2label[top_pred_idx.item()]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-9)).item()\n",
    "    \n",
    "    # Create a dictionary with all results\n",
    "    results = {\n",
    "        \"predicted_label\": top_pred_label,\n",
    "        \"confidence\": top_confidence.item(),\n",
    "        \"entropy\": entropy\n",
    "    }\n",
    "    # Add individual probabilities for each class\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        label = model.config.id2label[i]\n",
    "        results[f\"prob_{label}\"] = prob.item()\n",
    "        \n",
    "    return results\n",
    "\n",
    "# üÜï Post-processing function to filter confidence>0.85;entropy<0.35;prob>0.95\n",
    "def filter_high_conviction_emotions(log_df):\n",
    "    print(\"\\n--- Filtering for High-Conviction Predictions ---\")\n",
    "    \n",
    "    # --- Step 1: Filter by Confidence and Entropy ---\n",
    "    initial_filter_mask = (log_df['confidence'] > 0.85) & (log_df['entropy'] < 0.45)\n",
    "    filtered_df = log_df[initial_filter_mask].copy()\n",
    "    print(f\"-> Found {len(filtered_df)} predictions with confidence > 0.8 and entropy < 0.4.\")\n",
    "\n",
    "    # --- Step 2: Further refine by high probability for any single emotion ---\n",
    "    prob_columns = [col for col in log_df.columns if col.startswith('prob_')]\n",
    "    high_prob_mask = (filtered_df[prob_columns] > 0.95).any(axis=1)\n",
    "    \n",
    "    final_df = filtered_df[high_prob_mask]\n",
    "    print(f\"-> Refined to {len(final_df)} predictions with a single emotion probability > 0.9.\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a0075c-cffa-4c8a-b9ec-a6f19b03c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. CORE PROCESSING FUNCTION - VIDEO FACE EXTRACTOR\n",
    "# ==============================================================================\n",
    "\n",
    "# Reads vid file, detects faces, saves cropped images, runs emotion\n",
    "    # prediction, and logs all results to a list\n",
    "def analyze_video_faces(video_path, save_dir, model, processor, device, process_every_n_frames=1):\n",
    "   \n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ùå Error: Video file not found at {video_path}\")\n",
    "        return []\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"‚úÖ Opened video: {os.path.basename(video_path)} ({total_frames} frames at {fps:.2f} fps)\")\n",
    "\n",
    "    frame_count = 0\n",
    "    all_results_log = []\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing video frames\")\n",
    "\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % process_every_n_frames == 0:\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            face_locations = face_recognition.face_locations(rgb_frame)\n",
    "            \n",
    "            if face_locations:\n",
    "                frame_save_path = os.path.join(save_dir, \"face_crops\", f\"frame_{frame_count}\")\n",
    "                os.makedirs(frame_save_path, exist_ok=True)\n",
    "                \n",
    "                for i, (top, right, bottom, left) in enumerate(face_locations):\n",
    "                    face_image_arr = frame[top:bottom, left:right]\n",
    "                    face_image_pil = Image.fromarray(cv2.cvtColor(face_image_arr, cv2.COLOR_BGR2RGB))\n",
    "                    \n",
    "                    # Get the dictionary of emotion predictions\n",
    "                    emotion_results = predict_and_display_emotions(face_image_pil, model, processor, device)\n",
    "                    \n",
    "                    # Save the cropped face image\n",
    "                    face_filename = os.path.join(frame_save_path, f\"face_{i}.png\")\n",
    "                    face_image_pil.save(face_filename)\n",
    "\n",
    "                    # Add frame-specific info to the log\n",
    "                    log_entry = {\n",
    "                        \"timestamp_seconds\": frame_count / fps,\n",
    "                        \"frame_number\": frame_count,\n",
    "                        \"face_index\": i,\n",
    "                        \"face_crop_path\": face_filename,\n",
    "                        **emotion_results  # Unpack the emotion results into the log\n",
    "                    }\n",
    "                    all_results_log.append(log_entry)\n",
    "\n",
    "        frame_count += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    video_capture.release()\n",
    "    \n",
    "    print(f\"‚úÖ Video processing complete. Found and analyzed {len(all_results_log)} faces.\")\n",
    "    return all_results_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d2eeba-63c1-4358-af19-0da7d786efef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created analysis output directory: /Users/natalyagrokh/AI/ml_expressions/img_expressions/vid_inference/V2_20250711_114129\n",
      "\n",
      "--- Loading model from /Users/natalyagrokh/AI/ml_expressions/img_expressions/sup_training/V29_20250710_082807 ---\n",
      "\n",
      "üñ•Ô∏è Using device: mps\n",
      "‚úÖ Opened video: StreetQs.mp4 (5657 frames at 30.00 fps)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5657/5657 [23:48<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Video processing complete. Found and analyzed 7612 faces.\n",
      "\n",
      "‚úÖ Successfully saved detailed analysis to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/vid_inference/V2_20250711_114129/emotion_log.csv\n",
      "\n",
      "--- Filtering for High-Conviction Predictions ---\n",
      "-> Found 2970 predictions with confidence > 0.8 and entropy < 0.4.\n",
      "-> Refined to 332 predictions with a single emotion probability > 0.9.\n",
      "‚úÖ Successfully saved refined analysis to: /Users/natalyagrokh/AI/ml_expressions/img_expressions/vid_inference/V2_20250711_114129/refined_emotion_log.csv\n",
      "\n",
      "--- Summary ---\n",
      "Total faces analyzed and saved: 7612\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. MAIN EXTRACTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Setup Dynamic Save Directory ---\n",
    "    VERSION = get_next_version(ANALYSIS_OUTPUT_ROOT)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    VERSION_TAG = f\"{VERSION}_{timestamp}\"\n",
    "    SAVE_DIR = os.path.join(ANALYSIS_OUTPUT_ROOT, VERSION_TAG)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f\"üìÅ Created analysis output directory: {SAVE_DIR}\")\n",
    "\n",
    "    # --- Load Model and Set Device ---\n",
    "    print(f\"\\n--- Loading model from {MODEL_PATH} ---\")\n",
    "    model = AutoModelForImageClassification.from_pretrained(MODEL_PATH)\n",
    "    processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(\"\\nüñ•Ô∏è Using device:\", device)\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # --- Run the Analysis ---\n",
    "    video_to_process = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/vid_inference/sample_vids/StreetQs.mp4\" \n",
    "    analysis_log = analyze_video_faces(\n",
    "        video_path=video_to_process, \n",
    "        save_dir=SAVE_DIR,\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        device=device,\n",
    "        process_every_n_frames=1\n",
    "    )\n",
    "    \n",
    "    # --- Save and Filter Results ---\n",
    "    if analysis_log:\n",
    "        log_df = pd.DataFrame(analysis_log)\n",
    "        csv_path = os.path.join(SAVE_DIR, \"emotion_log.csv\")\n",
    "        log_df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n‚úÖ Successfully saved detailed analysis to: {csv_path}\")\n",
    "\n",
    "        # Call filtering function\n",
    "        refined_df = filter_high_conviction_emotions(log_df)\n",
    "\n",
    "        # Save the refined results to a new CSV file\n",
    "        if not refined_df.empty:\n",
    "            refined_csv_path = os.path.join(SAVE_DIR, \"refined_emotion_log.csv\")\n",
    "            refined_df.to_csv(refined_csv_path, index=False)\n",
    "            print(f\"‚úÖ Successfully saved refined analysis to: {refined_csv_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No faces were detected, so no log file was created.\")\n",
    "\n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Total faces analyzed and saved: {len(analysis_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846f07e-80c3-4727-beaf-05884f9eb9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
