{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b061f-647d-4e52-8e59-39111914ce6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce08a795-a759-4309-bae5-c11f1671c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28bfd35-f81a-430d-bff8-2b24a75d7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# --- Path to the cloned Wav2Lip repository ---\n",
    "WAV2LIP_ROOT = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/synthetic_speech_imgs/Wav2Lip\"\n",
    "\n",
    "# --- Path to the pre-trained model checkpoint ---\n",
    "# This is the path to the specific pre-trained model file required by Wav2Lip.\n",
    "# It should be located inside the 'checkpoints' folder within the Wav2Lip directory.\n",
    "WAV2LIP_CHECKPOINT = os.path.join(WAV2LIP_ROOT, \"/Users/natalyagrokh/AI/ml_expressions/img_datasets/synthetic_speech_imgs/Wav2Lip/checkpoints/Wav2Lip-SD-GAN.pt\")\n",
    "\n",
    "# --- MODIFICATION: Define your source DIRECTORIES ---\n",
    "# This directory should contain all your source images with pure emotional expressions (e.g., anger, happiness).\n",
    "# The script will iterate through every valid image file in this folder.\n",
    "SOURCE_IMAGE_DIR = \"/path/to/your/source_emotion_images\" # e.g., a folder with hundreds of .png files\n",
    "\n",
    "SOURCE_AUDIO_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_expressions/neutral_speech\"\n",
    "\n",
    "# --- Path where final output will be saved ---\n",
    "OUTPUT_DIR = \"/Users/natalyagrokh/AI/ml_expressions/img_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66d3c0-b59a-4a3c-855f-129dc43c252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. SCRIPT LOGIC (Helper Functions)\n",
    "# ==============================================================================\n",
    "# This section contains the core functions that handle video creation,\n",
    "# running the AI model, and extracting the final image frames.\n",
    "\n",
    "def create_static_video_from_image(image_path, audio_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a temporary, static video file from a single image.\n",
    "    Wav2Lip requires a video as its face input, not a static image. This function\n",
    "    uses the powerful 'ffmpeg' tool to generate a short video where the input\n",
    "    image is looped for the duration of the selected audio file.\n",
    "    \"\"\"\n",
    "    # This command tells ffmpeg to loop the input image (-loop 1), combine it with the\n",
    "    # input audio, and encode it into a standard .mp4 video file.\n",
    "    command = [\n",
    "        'ffmpeg', '-y', # -y overwrites the output file without asking\n",
    "        '-loop', '1', '-i', image_path, '-i', audio_path,\n",
    "        '-c:v', 'libx264', '-tune', 'stillimage', '-c:a', 'aac', '-b:a', '192k',\n",
    "        '-pix_fmt', 'yuv420p', '-shortest', output_path\n",
    "    ]\n",
    "    try:\n",
    "        # Executes the ffmpeg command. 'check=True' will raise an error if ffmpeg fails.\n",
    "        # 'capture_output=True' prevents ffmpeg's logs from cluttering the console.\n",
    "        subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # If ffmpeg fails, this block catches the error and prints a detailed message.\n",
    "        print(f\"   - ‚ùå ERROR: ffmpeg failed to create static video for {os.path.basename(image_path)}.\")\n",
    "        print(f\"     - Stderr: {e.stderr.strip()}\")\n",
    "        return False\n",
    "\n",
    "def run_wav2lip_inference(face_video_path, audio_path, checkpoint_path, output_path):\n",
    "    \"\"\"\n",
    "    Constructs and runs the main Wav2Lip inference command via a subprocess.\n",
    "    This function calls the core 'inference.py' script from the Wav2Lip tool,\n",
    "    passing all the necessary file paths and parameters.\n",
    "    \"\"\"\n",
    "    # Defines the path to the main script within the Wav2Lip directory.\n",
    "    wav2lip_script = os.path.join(WAV2LIP_ROOT, \"inference.py\")\n",
    "    # Assembles the full command-line instruction to run the Wav2Lip model.\n",
    "    command = [\n",
    "        'python', wav2lip_script, '--checkpoint_path', checkpoint_path,\n",
    "        '--face', face_video_path, '--audio', audio_path, '--outfile', output_path,\n",
    "        '--pads', '0', '10', '0', '0' # Defines padding around the detected face (Top, Bottom, Left, Right).\n",
    "    ]\n",
    "    try:\n",
    "        # This command is executed from within the Wav2Lip directory ('cwd=WAV2LIP_ROOT')\n",
    "        # to ensure that the Wav2Lip script can find all its necessary helper files.\n",
    "        subprocess.run(command, check=True, cwd=WAV2LIP_ROOT, capture_output=True, text=True)\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # If the Wav2Lip script fails, this catches and prints the specific error.\n",
    "        print(f\"   - ‚ùå ERROR: Wav2Lip inference failed for {os.path.basename(face_video_path)}.\")\n",
    "        print(f\"     - Stderr: {e.stderr.strip()}\")\n",
    "        return False\n",
    "\n",
    "def extract_frames_from_video(video_path, output_frame_dir):\n",
    "    \"\"\"\n",
    "    Extracts every frame from the final generated video and saves them as\n",
    "    individual .png image files. These extracted frames are the final output\n",
    "    that you will use as your new 'speech_action' training data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_frame_dir):\n",
    "        os.makedirs(output_frame_dir)\n",
    "\n",
    "    # Uses the OpenCV library to open the video file.\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    # Loops through the video frame-by-frame.\n",
    "    while video_capture.isOpened():\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret: # 'ret' is false when there are no more frames.\n",
    "            break\n",
    "        # Saves each frame as a uniquely named image file.\n",
    "        frame_filename = os.path.join(output_frame_dir, f\"frame_{frame_count:04d}.png\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "    video_capture.release() # Releases the video file handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fe52a2-6407-481e-a776-dc376a6c8687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "# This is the main part of the script that orchestrates the entire process.\n",
    "if __name__ == '__main__':\n",
    "    # --- Step 1: Verify all initial paths and settings are correct ---\n",
    "    if not all([os.path.exists(WAV2LIP_CHECKPOINT), os.path.isdir(SOURCE_IMAGE_DIR), os.path.isdir(SOURCE_AUDIO_DIR)]):\n",
    "        print(\"‚ùå CRITICAL ERROR: A required path is invalid. Please check:\")\n",
    "        if not os.path.exists(WAV2LIP_CHECKPOINT): print(f\"   - Wav2Lip Checkpoint: {WAV2LIP_CHECKPOINT}\")\n",
    "        if not os.path.isdir(SOURCE_IMAGE_DIR): print(f\"   - Source Image Directory: {SOURCE_IMAGE_DIR}\")\n",
    "        if not os.path.isdir(SOURCE_AUDIO_DIR): print(f\"   - Source Audio Directory: {SOURCE_AUDIO_DIR}\")\n",
    "        exit()\n",
    "\n",
    "    # --- Step 2: Discover all valid image and audio files ---\n",
    "    valid_image_extensions = ('.png', '.jpg', '.jpeg', '.tiff', '.bmp')\n",
    "    valid_audio_extensions = ('.wav', '.mp3', '.m4a', '.flac')\n",
    "    \n",
    "    source_images = [f for f in os.listdir(SOURCE_IMAGE_DIR) if f.lower().endswith(valid_image_extensions)]\n",
    "    source_audios = [f for f in os.listdir(SOURCE_AUDIO_DIR) if f.lower().endswith(valid_audio_extensions)]\n",
    "\n",
    "    if not source_images or not source_audios:\n",
    "        print(\"‚ùå CRITICAL ERROR: Source image or audio directory is empty.\")\n",
    "        exit()\n",
    "        \n",
    "    print(f\"‚úÖ Found {len(source_images)} source images and {len(source_audios)} audio files.\")\n",
    "    \n",
    "    # --- Step 3: Main processing loop to iterate through all images ---\n",
    "    # The 'tqdm' library creates a progress bar for the loop.\n",
    "    for image_name in tqdm(source_images, desc=\"Processing Emotion Images\"):\n",
    "        try:\n",
    "            current_image_path = os.path.join(SOURCE_IMAGE_DIR, image_name)\n",
    "            \n",
    "            # --- For each image, randomly select one audio file ---\n",
    "            # This introduces variety and prevents the model from overfitting to one speaker.\n",
    "            selected_audio_name = random.choice(source_audios)\n",
    "            current_audio_path = os.path.join(SOURCE_AUDIO_DIR, selected_audio_name)\n",
    "\n",
    "            # --- Setup unique output paths for the current image ---\n",
    "            # Creates a subfolder named after the image to keep outputs organized.\n",
    "            source_image_basename = os.path.splitext(image_name)[0]\n",
    "            session_output_dir = os.path.join(OUTPUT_DIR, source_image_basename)\n",
    "            os.makedirs(session_output_dir, exist_ok=True)\n",
    "            \n",
    "            # Defines the full paths for the temporary and final video files.\n",
    "            temp_video_path = os.path.join(session_output_dir, \"temp_face_video.mp4\")\n",
    "            generated_video_path = os.path.join(session_output_dir, \"generated_speech.mp4\")\n",
    "            output_frames_dir = os.path.join(session_output_dir, \"synthetic_speech_action_frames\")\n",
    "\n",
    "            # --- Run the 3-step pipeline for the current image ---\n",
    "            if create_static_video_from_image(current_image_path, current_audio_path, temp_video_path):\n",
    "                if run_wav2lip_inference(temp_video_path, current_audio_path, WAV2LIP_CHECKPOINT, generated_video_path):\n",
    "                    extract_frames_from_video(generated_video_path, output_frames_dir)\n",
    "\n",
    "        except Exception as e:\n",
    "            # This 'try...except' block ensures that if one image fails for any reason,\n",
    "            # the script will print an error and continue to the next image,\n",
    "            # preventing the entire batch process from crashing.\n",
    "            print(f\"\\n--- ‚ö†Ô∏è An unexpected error occurred while processing {image_name}. Skipping. ---\")\n",
    "            print(f\"   - Error: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nüéâ Batch processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_expressions",
   "language": "python",
   "name": "ml_expressions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
